<<<<<<< HEAD
stats::factanal
two_factor_ML <- fa(dataFR2, nfactors = 2, rotate = "oblimin", fm = "ml")
diag(two_factor_ML$loadings[,] %*% two_factor_ML$Phi %*% t(two_factor_ML$loadings[,]))
t(two_factor_ML$loadings[,]) %*% two_factor_ML$loadings[,]
t(two_factor_ML$Structure[,]) %*% two_factor_ML$Structure[,]
two_factor_ML$values
two_factor_ML <- fa(dataFR2, nfactors = 6, rotate = "oblimin", fm = "ml")
t(two_factor_ML$loadings[,]) %*% two_factor_ML$loadings[,]
t(two_factor_ML$Structure[,]) %*% two_factor_ML$Structure[,]
two_factor_ML$values
diag(t(two_factor_ML$Structure[,]) %*% two_factor_ML$loadings[,])
two_factor_ML$values
two_factor_ML$e.values
two_factor_ML$values
two_factor_ML$values
diag(t(L)  %*% L)
L <- two_factor_ML$loadings[,]
diag(t(L)  %*% L)
S <- two_factor_ML$Structure[,]
diag(t(S)  %*% L)
two_factor_ML$Vaccounted[1,]
load(url("https://pandar.netlify.app/post/PISA2009.rda"))
load(url("https://pandar.netlify.app/post/PISA2009.rda"))
1e-1
lm(Reading ~ HISEI + MotherEdu + Books, data = PISA2009)
summary(m)
m <- lm(Reading ~ HISEI + MotherEdu + Books, data = PISA2009)
summary(m)
m <- lm(Reading ~ HISEI + MotherEdu + Books*Age, data = PISA2009)
summary(m)
m <- lm(Reading ~ HISEI*Age + MotherEdu + Books, data = PISA2009)
summary(m)
m <- lm(Reading ~ HISEI + MotherEdu + Books*JoyRead, data = PISA2009)
summary(m)
m <- lm(Reading ~ HISEI*JoyRead + MotherEdu + Books, data = PISA2009)
summary(m)
PISA2009$LearnMins
m <- lm(Reading ~ JoyRead, data = PISA2009)
summary(m)
m <- lm(Reading ~ JoyRead*LearnMins, data = PISA2009)
summary(m)
PISA2009
head(PISA2009)
m <- lm(Reading ~ JoyRead*Grade, data = PISA2009)
summary(m)
m <- lm(Reading ~ JoyRead, data = PISA2009)
summary(m)
head(PISA2009)
m <- lm(Reading ~ JoyRead*Female, data = PISA2009)
summary(m)
head(PISA2009)
m <- lm(Reading ~ JoyRead*HISEI, data = PISA2009)
summary(m)
head(PISA2009)
m <- lm(Reading ~ JoyRead*LearnMins, data = PISA2009)
summary(m)
head(PISA2009)
m <- lm(Reading ~ JoyRead*CultPoss, data = PISA2009)
summary(m)
head(PISA2009)
m <- lm(Reading ~ JoyRead*Books, data = PISA2009)
summary(m)
head(PISA2009)
m <- lm(Reading ~ JoyRead*FatherEdu, data = PISA2009)
summary(m)
m <- lm(Reading ~ MotherEdu*FatherEdu, data = PISA2009)
summary(m)
m <- lm(Reading ~ MotherEdu+FatherEdu, data = PISA2009)
summary(m)
load(url("https://pandar.netlify.app/post/Schulleistungen.rda"))
head(Schulleistungen)
m <- lm(reading ~ IQ*math, data = Schulleistungen)
summary(m)
head(Schulleistungen)
m <- lm(reading ~ IQ*math, data = scale(Schulleistungen))
summary(m)
scale(Schulleistungen)
d <- data.frame(scale(Schulleistungen))
m <- lm(reading ~ IQ*math, data = d)
summary(m)
m <- lm.beta(lm(reading ~ IQ*math, data = d))
library(lm.beta)
m <- lm.beta(lm(reading ~ IQ*math, data = d))
summary(m)
install.packages("reghelper")
library(reghelper)
simple_slopes(model = m)
install.packages("pequod")
library(pequod)
simpleSlope(m)
m <- lm(reading ~ IQ*math, data = d)
simpleSlope(m)
library(ggplot2)
interact_plot(reading, pred = IQ, modx = math, data = d)
install.packages("interactions")
library(ggplot2)
interact_plot(m, pred = IQ, modx = math)
library(ggplot2); ibrary(interactions)
library(ggplot2); library(interactions)
interact_plot(m, pred = IQ, modx = math)
blogdown:::serve_site()
blogdown:::serve_site()
---
title: "Regression IV: quadratische und moderierte Regression"
date: '2021-03-30'
slug: quadratische-und-moderierte-regression
categories:
- MSc1
tags:
- quadratisch
- moderiert
- Interaktion
- Moderation
- Regression
subtitle: ''
summary: ''
authors: [irmer, hartig]
lastmod: '2021-01-28T08:32:21+02:00'
featured: no
header:
image: "/header/PsyBSc7_Reg4.jpg"
caption: "[Courtesy of pxhere](https://pxhere.com/en/photo/692189)"
projects: []
---
```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = "center")
```
## Einleitung und Datensatz
In dieser Sitzung werden wir uns weitere nichtlineare Effekte in Regressionsmodellen ansehen. Dazu verwenden wir zunächst den Datensatz aus der Übung zum letzten Themenblock.
Der Beispieldatensatz enthält Daten zu Lesekompetenz aus der deutschen Stichprobe der PISA-Erhebung in Deutschland 2009. Sie können den im Folgenden verwendeten  [`r fontawesome::fa("download")` Datensatz "PISA2009.rda" hier herunterladen](https://pandar.netlify.app/post/PISA2009.rda).
### Daten laden
Wir laden zunächst die Daten: entweder lokal von Ihrem Rechner:
```{r, eval = F}
load("C:/Users/Musterfrau/Desktop/PISA2009.rda")
```
oder wir laden sie direkt über die Website:
```{r, eval = T}
load(url("https://pandar.netlify.app/post/PISA2009.rda"))
```
Außerdem werden wir folgende `R`-Pakete brauchen:
```{r, message=F}
library(car)
library(MASS)
library(lm.beta) # erforderlich für standardiserte Gewichte
library(ggplot2)
library(interactions) # für Interaktionsplots in moderierten Regressionen
```
## Quadratische Verläufe in der Vorhersage von Lesekompetenz mit individuellen Merkmalen der Schüler/innen
In der Übung zur letzten Sitzung hatten wir herausgefunden, dass der Sozialstatus (`HISEI`), der Bildungsabschluss der Mutter (`MotherEdu`) und die Zahl der Bücher zu Hause (`Books`) bedeutsame Prädiktoren für die Lesekompetenz der Schüler/innen sind, allerdings zeigten Analysen, dass nicht alle Voraussetzungen erfüllt waren:
```{r}
# Berechnung des Modells und Ausgabe der Ergebnisse
m1 <- lm(Reading ~ HISEI + MotherEdu + Books, data = PISA2009)
summary(lm.beta(m1))
```
Die Residuenplots sowie die Testung auf quadratische Trends zeigen an, dass für den Bildungsabschluss der Mutter auch eine quadratische Beziehung mit der Lesekompetenz besteht:
```{r, fig.height=6, fig.align="center"}
# Residuenplots
residualPlots(m1, pch = 16)
```
Die Effekte von Sozialstatus und Büchern werden durch das lineare Modell gut wiedergegeben. Für den Bildungsabschluss der Mutter ist ein leicht nicht-linearer Zusammenhang zu erkennen, der quadratische Trend für die Residuen ist signifikant (*signifikantes Ergebnis für den Bildungsabschluss der Mutter*). Der Effekt ist dadurch charakterisiert, dass der Zuwachs der Lesekompetenz im unteren Bereich des mütterlichen Bildungsabschlusses stärker ist und im oberen Bereich abflacht.
Auch dem Histogramm war eine Schiefe zu entnehmen, welche durch nichtlineare Terme entstehen können (im niederen Bereich liegen mehr Werte; eine Linksschiefe/Rechtssteile ist zu erkennen).
```{r, fig.height=6, fig.align="center"}
res <- studres(m1) # Studentisierte Residuen als Objekt speichern
df_res <- data.frame(res) # als Data.Frame für ggplot
# Grafisch: Histogramm mit Normalverteilungskurve
library(ggplot2)
ggplot(data = df_res, aes(x = res)) +
geom_histogram(aes(y =..density..),
bins = 15,                    # Wie viele Balken sollen gezeichnet werden?
colour = "blue",              # Welche Farbe sollen die Linien der Balken haben?
fill = "skyblue") +           # Wie sollen die Balken gefüllt sein?
stat_function(fun = dnorm, args = list(mean = mean(res), sd = sd(res)), col = "darkblue") + # Füge die Normalverteilungsdiche "dnorm" hinzu und nutze den empirischen Mittelwert und die empirische Standardabweichung "args = list(mean = mean(res), sd = sd(res))", wähle dunkelblau als Linienfarbe
labs(title = "Histogramm der Residuen mit Normalverteilungsdichte", x = "Residuen") # Füge eigenen Titel und Achsenbeschriftung hinzu
# Test auf Abweichung von der Normalverteilung mit dem Shpiro Test
shapiro.test(res)
```
Die Frage ist nun, woher die Verstöße gegen die Normalverteilungsannahme kommen. Erste Indizien aus den Partialplots wiesen darauf hin, dass möglicherweise ein quadratischer Effekt des Bildungsabschlusses der Mutter besteht.
## Aufnahme eines quadratischen Effekts
Wird für den Bildungsabschluss der Mutter mit der Funktion `poly` ein linearer und quadratischer Trend in das Regressionsmodell aufgenommen, wird der quadratische Trend signifikant und das Modell erklärt signifikant mehr Varianz als ohne den quadratischen Trend: Um diese Ergebnisse zu sehen müssen wir zunächst ein quadratisches Regressionsmodell schätzen. Wir interessieren uns anschließend für die standardisierten Ergebnisse (`summary` und `lm.beta`). Den quadratischen Verlauf erhalten wir, indem wir innerhalb des linearen Modells `poly` auf den Bildungsabschluss der Mutter anwenden. `poly` nimmt als zweites Argument die Potenz, für welche wir uns interessieren; hier 2:
```{r exercise_lm_quad-solution}
m1.b <- lm(Reading ~ HISEI + poly(MotherEdu, 2) + Books, data = PISA2009)
summary(lm.beta(m1.b))
```
Mit dem folgenden Befehl können wir auf eine simple Weise das Inkrement bestimmen.
```{r}
# Vergleich mit Modell ohne quadratischen Trend
summary(m1.b)$r.squared - summary(m1)$r.squared # Inkrement
```
Wir möchten dieses Inkrement auf Signifikanz prüfen. Dies geht mit dem `anova` Befehl.
```{r}
anova(m1, m1.b)
```
Hier sollte dem anova-Befehl immer das "kleinere" (restriktivere) Modell (mit weniger Prädiktoren und Parametern, die zu schätzen sind) zuerst übergeben werden. Hier: `m1`, da sonst die df negativ sein sind (und auch als solche vom Programm angezeigt werden können, obwohl dieses das oft erkennen kann und dann das Vorzeichen umdreht...) und auch die Änderung in den `Sum of Sq` (Quadratsumme) negativ sind! `R` erkennt dies zwar und testet trotzdem die richtige Differenz auf Signifikanz, aber wir wollen uns besser vollständig korrekt aneignen!
Erzeugt man für das erweiterte Modell Residuenplots, ist der quadratische Trend beim Bildungsabschluss komplett verschwunden - er ist ja schon im Modell enthalten und bildet sich somit nicht mehr in den Residuen ab:
```{r, fig.height=6, fig.align="center"}
residualPlots(m1.b, pch = 16)
```
Was bedeutet nun dieser Effekt inhaltlich? Um dies genauer zu verstehen, stellen wir die um die anderen Variablen bereinigte Beziehung zwischen dem Bildungsabschluss der Mutter und der Leseleistung grafisch dar.
```{r, echo= F, fig.align="center", fig.height=6}
X <- scale(poly(PISA2009$MotherEdu, 2))
std_par_ME <- c(0.1588, -0.1436)
pred_effect_ME <- X %*% std_par_ME
std_ME <- X[,1]
data_ME <- data.frame(std_ME, pred_effect_ME)
ggplot(data = data_ME, aes(x = std_ME,  y = pred_effect_ME)) + geom_point(pch = 16, col = "blue", cex = 4)+
labs(y = "std. Leseleistung | Others", x =  "std. Bildungsabschluss der Mutter | Others",
title = "Standardisierte bedingte Beziehung zwischen\n Bildungsabschluss der Mutter und Leseleistung")
```
Für den Grafik-Code sowie weitere Informationen zu quadaratischen Effekten und Funktionen siehe [Appendix A](#AppendixA). Die Grafik zeigt die vorhergesagte Beziehung zwischen den standardisierten Werten des Bildungsabschlusses der Mutter sowie der Leseleistung. Hierbei steht erneut `|` für "gegeben" (wie etwa beim Partialplot mit `avPlots` aus der vergangenen Sitzung). Wir sehen also den um die anderen Variablen im Modell bereinigten Effekt zwischen Bildungsabschluss und Leseleistung. Hierbei ist ein starker mittlerer Anstieg der Leseleistung (-1 bis ca. 0.1) für einen Anstieg des Bildungsabschlusses von deutlich unterdurchschnittlich bis durchschnittlich (von -2.5 bis 0) zu sehen. Danach ist die Beziehung zwischen Leseleistung und Bildungsabschluss fast horizontal (Veränderung geringer als 0.1), was dafür spricht, dass es für einen durchschnittlichen bis überdurchschnittlichen Bildungsabschluss der Mutter (von 0 bis 1.5) kaum eine Beziehung zwischen den Variablen gibt. Dies bedeutet, dass besonders im unterdurchschnittlichen Bereich der mütterlichen Bildung Unterschiede zwischen Müttern einen starken Zusammenhang mit der Leseleistung ihrer Kinder zeigen. Wenn das Bildungsniveau der Mutter jedoch durchschnittlich oder überdurchschnittlich ist, scheint der Zusammenhang beinahe zu verschwinden.
Die grobe Gestalt der Beziehung hätten wir auch aus dem Koeffizienten ablesen können. Der Koeffizient des quadratischen Teils war negativ, was für eine invers-u-förmige (konkave) Funktion steht. Das Einzeichnen hilft uns jedoch, das genaue Ausmaß zu verstehen (siehe auch [Appendix A](#AppendixA)). Auch hatten wir gesehen, dass der lineare Teil des Bildungsabschlusses der Mutter keinen statistisch signifikanten Beitrag zur Vorhersage geleistet hat. Jedoch gehört zu einer quadratischen Funktion immer auch ihr linearer Anteil dazu. Aus diesem Grund können wir unsere Stichprobe nur angemessen beschreiben, wenn wir den linearen Trend des Bildungsabschlusses der Mutter im Regressionsmodell beibehalten. Um das genaue Ausmaß besser zu verstehen, manipulieren Sie doch einmal die Beziehung, die wir soeben grafisch gesehen haben, indem Sie den Code aus dem folgenden Block kopieren und die Inputvariablen verändern. Hierbei können Sie den linearen und den quadratischen Effekt verändern und sich die Auswirkungen auf die Grafik (die Beziehung zwischen Bildungsabschluss der Mutter und Leseleistung) ansehen. Die Default-Einstellungen sind identisch zu der oberen Grafik. `curve` plottet eine Linie und nimmt `x` automatisch als Argument der Funktion, somit wird $f(x)=\text{linear}*x+\text{quadratisch}*x^2$ geplottet. Probieren Sie doch einmal aus, was passiert, wenn Sie den linearen Teil auf 0 setzen oder das Vorzeichen des quadratischen Anteils ändern!
```{r fig.align="center", fig.height=6}
linear <- .1588
quadratisch <- -.1436
curve(linear * x + quadratisch * x^2,
xlim = c(-2, 2))
```
Mit Hilfe von `poly(X, p)`, lassen sich Polynome bis zum Grad $p$ (als $X, X^2,\dots,X^{p-1},X^p$) in die Regression mit aufnehmen, ohne, dass sich die Parameterschätzungen der anderen Potenzen von $X$ ändern. Wenn Sie noch mehr über die Funktion `poly` und ihre Vorteile erfahren möchten, dann schauen Sie sich doch mal den [Appendix A](#AppendixA) an. Wenn wir `poly` nicht verwenden wollen würden, so sollten wir zumindest die Prädiktoren, für welche wir quadratische Effekte annehmen, zentrieren, also den Mittelwert der Variable von dieser abziehen. Bspw. $X_i-\bar{X}$, was in `R` so aussieht: `X-mean(X)`. Diese Variable würde wir dann an unseren Datensatz anhängen. Bezogen auf den Bildungsstatus der Mutter könnten wir wie folgt vorgehen:
```{r}
PISA2009$MotherEdu_centered <- PISA2009$MotherEdu - mean(PISA2009$MotherEdu)
mean(PISA2009$MotherEdu_centered) # sehr kleine Zahl
```
`e-17` steht hierbei für $10^{-17}$ also eine Verschiebung des Kommas um 17 Stellen nach links, was eine sehr kleine Zahl ausdrückt. Der Mittelwert ist hier nicht exakt Null, da `R` intern immer auf die sogenannte Maschienengenauigkeit rundet (das sind ca 16 Nachkommastellen). `_centered` steht hierbei für zentriert, also Mittelwert = 0.
### Interaktionsterme
Außerdem können auch Interaktionen zwischen Variablen in ein Regressionsmodell aufgenommen werden. Für weiter inhaltliche Details siehe [Eid et al. (2017) Kapitel 19.9](https://hds.hebis.de/ubffm/Record/HEB366849158). Eine Regression mit einem Interaktionsterm wird auch häufig moderierte Regression genannt. Häufig wird dann von einem Moderator gesprochen, der die Beziehung eines Prädiktores mit dem Kriterium "moderiert", allerdings gibt es keinen Beweis dafür, ob der Prädiktor tatsächlich ein Prädiktor oder ein Moderator ist. Dies ist leicht einzusehen, wenn wir uns die Modellgleichungen ansehen. Wir nennen den Prädiktor $X$, den Moderator $Z$ und das Kriterium $Y$. Dann ergibt sich folgende Regressionsgleichung (für eine Person $i$):
$$Y_i=\beta_0 + \beta_1X_i + \beta_2Z_i + \beta_3X_iZ_i + \varepsilon_i.$$
Der Interaktionsterm trägt also den Koeffizienten $\beta_3$ in diesem Beispiel. Um das ganze sich leichter vorstellen zu können, stellen wir diese Gleichung um und stellen die Beziehung zwischen $X$ und $Y$ mit Hilfe von $Z$ dar. Das wird auch manchmal "simple slopes" also einfache Steigungen genannt, da wir im Grunde mehrere Geraden für $X$ in Abhängigkeit von $Z$ annehmen wollen:
$$Y_i=\underbrace{(\beta_0 + \beta_2Z_i)}_{\text{Interzept}(Z_i)} + \underbrace{(\beta_1 + \beta_3Z_i)}_{\text{Slope}(Z_i)}X_i + \varepsilon_i.$$
Hier ist eigentlich gar nichts passiert - wir haben lediglich die Gleichung umgestellt. Allerdings sieht dies nun so aus, als würde von ein Interzept $(\beta_0 + \beta_2Z_i)$ und vor $X_i$ eine Slope (Steigungskoeffizient) $(\beta_1 + \beta_3Z_i)$ stehen - beide abhängig von $Z_i$, deshalb haben wir sie gleich mal $\text{Interzept}(Z_i)$ und $\text{Slope}(Z_i)$ genannt. Genauso könnten wir allerdings auch alles nach $X$ umstellen: $Y_i=(\beta_0 + \beta_1X_i) + (\beta_2 + \beta_3X_i)Z_i + \varepsilon_i.$ Somit ist ersichtlich, dass es keine mathematische Begründung gibt, welcher der beiden Variablen der Prädiktor und welcher der Moderator ist! Manche sagen auch, dass dieses Modell "symmetrisch" in den beiden Variablen ist, man sie also leicht hinsichtlich der inhaltlichen Interpretation austauschen kann. Das ganze in `R` sich anzuschauen geht sehr einfach. Wir wollen dies am Datensatz `Schulleistungen.rda` durchführen, den wir bereits aus vorherigen Sitzungen kennen. Wie genau wir an den Datensatz herankommen, können Sie sich in der entsprechenden Sitzung ansehen. Wir laden den Datensatz wie folgt über die Website:
```{r, eval = T}
load(url("https://pandar.netlify.app/post/Schulleistungen.rda"))
head(Schulleistungen)
```
Auch bei Interaktionen ist es wichtig, dass die Daten zentriert sind, also einen Mittelwert von 0 aufweisen. Das erleichtert die Interpretation und verändert die Korrelation des Interaktionsterms (oben $X_i*Z_i$) mit den Haupteffekten von $X_i$ und $Z_i$. Daher verwenden wir die `scale` Funktion, um den gesamten Datensatz zu standardisieren (also zu zentrieren und gleich noch die Varianz auf 1 zu setzen) und speichern diesen unter dem Namen `Schulleistungen_std`.
```{r}
Schulleistungen_std <- data.frame(scale(Schulleistungen)) # standardisierten Datensatz abspeichern als data.frame
colMeans(Schulleistungen_std)     # Mittelwert pro Spalte ausgeben
apply(Schulleistungen_std, 2, sd) # Standardabweichungen pro Spalte ausgeben
```
Nun führen wir eine moderierte Regression durch, in welcher wir in diesem Datensatz die Leseleistung `reading` durch den `IQ` sowie die Matheleistung `math` vorhersagen, sowie durch deren Interaktion. Die Interaktion können wir durch `:` ausdrücken. Falls wir einfach `*` verwenden, werden auch gleich noch die Haupteffekte, also die Variablen selbst mit aufgenommen. Es gilt also: `math + IQ + math:IQ = math*IQ`. Um auch wirklich die Interaktion zu testen, ist es unbedingt notwendig, die Haupteffekte der Variablen ebenfalls in das Modell mit aufzunehmen, da die Variablen trotzdem mit der Interaktion korreliert sein können, auch wenn die Variablen zentriert sind.
```{r}
mod_reg <- lm(reading ~ math + IQ + math:IQ, data = Schulleistungen_std)
summary(mod_reg)
```
Dem Output entnehmen wir, dass der Haupteffekt des IQ signifikant ist, sowie die Interaktion mit der Matheleistung. Die Matheleistung an sich bringt aber keine signifikante Vorhersagekraft der Leseleistung. Wie genau hier es zu diesen Ergebnissen gekommen ist, ist schwer zu sagen. Wir wollen es so interpretieren, dass die Matheleistung die Beziehung zwischen IQ und Leseleistung moderiert. Somit wäre $X=$ `IQ` (Prädiktor) und $Z=$ `math` (Moderator). Es gibt ein `R`-Paket, dass eine solche Interaktion grafisch darstellt: `interactions`. Nachdem Sie dieses installiert haben, können Sie es laden und die Funktion `interact_plot` verwenden, um diese Interaktion zu veranschaulichen. Dem Argument `model` übergeben wir `mod_reg`, also unser moderiertes Regressionsmodell, als Prädiktor hatten wir den IQ gewählt, also müssen wir dem Argument `pred` den `IQ` übergeben. Der Moderator ist hier die Matheleistung, folglich übergeben wir `math` dem Argument `modx`.
```{r}
library(interactions)
interact_plot(model = mod_reg, pred = IQ, modx = math)
```
Uns wird nun ein Plot mit drei Linien ausgegeben. Dieser wird auch häufig "simple slopes" Plot genannt. Dargestellt sind drei Beziehungen zwischen `IQ` und `reading` für unterschiedliche Ausprägungen von `math`; nämlich einmal für einen durchschnittlichen `math`-Wert, sowie für jeweils Werte, die eine Standardabweichung (SD) oberhalb oder unterhalb des Mittelwertes liegen. Damit bekommen wir ein Gefühl dafür, wie sehr sich die Beziehung (und damit Interzept und Slope) zwischen der Leseleistung und der Intelligenz verändert für unterschiedliche Ausprägungen der Matheleistung --- nämlich für eine durchschnittliche (`Mean`) Ausprägung sowie für eine unter- (`- SD`) und eine überdurchschnittliche (`+ SD`) Ausprägung. Die Signifikanzentscheiden oben zeigte uns, dass diese Unterschiede bedeutsam sind und somit die Matheleistung entscheidend ist, wie genau die Leseleistung mit der Intelligenz zusammenhängt. Die einzelnen Regressiongerade lassen sich ebenfalls auf signifikante Unterschiede prüfen. Es kann auch untersucht werden, welche Ausprägungen des Moderators zu unterschiedlichen "bedingten" Regressionsgewichten führen, also ab wann sich Interzept oder Slope des Prädiktors signifikant verändert, wenn sich der Moderator verändert. Interessierte Leser können sich bei Interesse, die über diesen Kurs hinaus geht, dazu das `R`-Paket `reghelper` mit der Funktion `simple_slopes` ansehen. Nach laden des Pakets kann mit `?simple_slopes` die Hilfe zu dieser Funktion aufgerufen werden, die den Umgang damit etc. erklärt.
Mit Hilfe von `I()` lassen sich innerhalb des `lm` Befehls zu dem noch weitere Funktionale hinzufügen, ohne diese vorher erzeugen zu müssen. Beispielsweise ließe sich durch `lm(Y ~ X + I(sin(X))) + I(exp(sqrt(X))` folgendes Regressionsmodell schätzen: $Y = \beta_0+\beta_1X + \beta_2\sin(X) + \beta_3e^{\sqrt{X}} + \varepsilon$. Allerdings lassen sich so nicht Wachstumsraten modellieren (z.B. exponentielles oder logarithmisches Wachstum) - hierzu müssten die Variablen tatsächlich transformiert werden. Dies wollen wir uns in der [nächsten Sitzung](/post/nichtlineare-Regression) genauer ansehen.
Den gesamten `R`-Code, der in dieser Sitzung genutzt wird, können Sie [`r fontawesome::fa("download")` hier herunterladen](https://raw.githubusercontent.com/jpirmer/MSc1_FEI/master/R-Scripts/4_Log_Reg_RCode.R).
## Appendix A {#AppendixA}
### Exkurs: Was genau macht `poly`?
```{r}
X <- 1:10   # Variable X
X2 <- X^2   # Variable X hoch 2
X_poly <- poly(X, 2)  # erzeuge Variable X und X hoch mit Hilfe der poly Funktion
colnames(X_poly) <- c("poly(X, 2)1", "poly(X, 2)2")
cbind(X, X2, X_poly)
```
Die Funktion `poly` erzeugt sogenannte *orthogonale Polynome*. Das bedeutet, dass zwar die $x$ und $x^2$ berechnet werden, diese Terme anschließend allerdings so transformiert werden, dass sie jeweils einen Mittelwert von *0* und die gleiche Varianz haben und zusätzlich noch unkorreliert sind:
```{r}
round(apply(X = cbind(X, X2, X_poly), MARGIN = 2, FUN = mean), 2) # Mittelwerte über die Spalten hinweg berechnen
round(apply(X = cbind(X, X2, X_poly), MARGIN = 2, FUN = sd), 2) # Standardabweichung über die Spalten hinweg berechnen
round(cor(cbind(X, X2, X_poly)),2) # Korrelationen berechnen
```
Die Funktion `apply` führt an der Matrix, welche dem Argument `X` übergeben wird, entweder über die Zeilen `MARGIN = 1` oder über die Spalten `MARGIN = 2` (hier jeweils gewählt) die Funktion aus, welche im Argument `FUN` angegeben wird. So wird zunächst mit `FUN = mean` der Mittelwert und anschließend mit `FUN = sd` die Standardabweichung von $X, X^2$ sowie `poly(X, 2)` berechnet. Der Korrelationsmatrix ist zu entnehmen, dass $X$ und $X^2$ in diesem Beispiel sehr hoch miteinander korrelieren und somit gleiche lineare Informationen enthalten ($\hat{r}_{X,X^2}$ = `cor(X, X2)` = 0.97), während die linearen und die quadratischen Anteile in `poly(X, 2)` keinerlei lineare Gemeinsamkeiten haben - sie sind unkorreliert (`cor(poly(X,2)1 , poly(X,2)2)` = 0). Ein weiterer Vorteil ist deshalb, dass bei sukzessiver Aufnahme der Anteile von `poly(X, 2)` in ein Regressionmodell, sich die Parameterschätzungen des linearen Terms im Modell nicht (bzw. sehr wenig) ändern. Der Anteil erklärter Varianz bleibt jedoch in allen gleich - die Modelle sind äquivalent, egal auf welche Art und Weise quadratische Terme gebildet werden.
```{r}
m1.b1 <- lm(Reading ~ HISEI + poly(MotherEdu, 1) + Books, data = PISA2009)
summary(lm.beta(m1.b1))
m1.b2 <- lm(Reading ~ HISEI + poly(MotherEdu, 2) + Books, data = PISA2009)
summary(lm.beta(m1.b2))
PISA2009$MotherEdu2 <- PISA2009$MotherEdu^2 # füge dem Datensatz den quadrierten Bildungsabschluss der Mutter hinzu
m1.c1 <- lm(Reading ~ HISEI + MotherEdu + Books, data = PISA2009)
summary(lm.beta(m1.c1))
m1.c2 <- lm(Reading ~ HISEI + MotherEdu + MotherEdu2 + Books, data = PISA2009)
summary(lm.beta(m1.c2))
rbind(coef(m1.b1), coef(m1.c1)) # vgl Koeffizienten
rbind(coef(m1.b2),coef(m1.c2)) # vgl Koeffizienten
rbind(summary(m1.b1)$r.squared, summary(m1.c1)$r.squared) # vgl R^2
rbind(summary(m1.b2)$r.squared,summary(m1.c2)$r.squared) # vgl R^2
```
Wir erkennen, dass die Funktion `poly` keinen Einfluss auf die Güte des Modells hat (dies lässt sich bspw. auch an $R^2$ der jeweiligen Modelle ablesen). Auch die Effekte der anderen Variablen sind identisch über die Modelle hinweg.
### Einordnung quadratischer Verläufe
Wie kommen wir nun auf die Interpretation der quadratischen Beziehung?
Eine allgemeine quadratische Funktion $f$ hat folgende Gestalt
$$f(x):=ax^2 + bx + c,$$
wobei $a\neq 0$, da es sich sonst nicht um eine quadratische Funktion handelt. Wäre $a=0$, würde es sich um eine lineare Funktion mit Achsenabschnitt $c$ und Steigung (Slope) $b$ handeln. Wäre zusätzlich $b=0$, so handelt es sich um eine horizontale Linie bei $y=f(x)=c$.
Für betraglich große $x$ fällt $x^2$ besonders ins Gewicht. Damit entscheidet das Vorzeichen von $a$, ob es sich um eine u-formige (falls $a>0$) oder eine umgekehrt-u-förmige (falls $a<0$) Beziehung handelt. Die betragliche Größe von $a$ entscheidet hierbei, wie gestaucht die u-förmige Beziehung (die Parabel) ist. Die reine quadratische Beziehung $f(x)=x^2$ sieht so aus:
```{r, fig.align="center", fig.height=6}
a <- 1; b <- 0; c <- 0
x <- seq(-2,2,0.01)
f <- a*x^2 + b*x + c
data_X <- data.frame(x, f)
ggplot(data = data_X, aes(x = x,  y = f)) +
geom_line(col = "black")+
labs(x = "x", y =  "f(x)",
title = expression("f(x)="~x^2))
```
Wir werden diese Funktion immer als Referenz mit in die Grafiken einzeichnen.
```{r, fig.align="center", fig.height=6}
a <- 0.5; b <- 0; c <- 0
x <- seq(-2,2,0.01)
f <- a*x^2 + b*x + c
data_X <- data.frame(x, f)
ggplot(data = data_X, aes(x = x,  y = f)) +
geom_line(col = "blue", lwd = 2)+ geom_line(mapping = aes(x, x^2), lty = 3)+
labs(x = "x", y =  "f(x)",
title = expression("f(x)="~0.5*x^2))
```
```{r, fig.align="center", fig.height=6}
a <- 2; b <- 0; c <- 0
x <- seq(-2,2,0.01)
f <- a*x^2 + b*x + c
data_X <- data.frame(x, f)
ggplot(data = data_X, aes(x = x,  y = f)) +
geom_line(col = "blue", lwd = 2)+ geom_line(mapping = aes(x, x^2), lty = 3)+
labs(x = "x", y =  "f(x)",
title = expression("f(x)="~2*x^2))
```
```{r, fig.align="center", fig.height=6}
a <- -1; b <- 0; c <- 0
x <- seq(-2,2,0.01)
f <- a*x^2 + b*x + c
data_X <- data.frame(x, f)
ggplot(data = data_X, aes(x = x,  y = f)) +
geom_line(col = "blue", lwd = 2)+ geom_line(mapping = aes(x, x^2), lty = 3)+
labs(x = "x", y =  "f(x)",
title = expression("f(x)="~-x^2))
```
Diese invers-u-förmige Beziehung ist eine konkave Funktion. Als Eselsbrücke für das Wort *konkav*, welches fast das englische Wort *cave* enthält, können wir uns merken: eine konkave Funktion stellt eine Art *Höhleneingang* dar.
$c$ bewirkt eine vertikale Verschiebung der Parabel:
```{r, fig.align="center", fig.height=6}
a <- 1; b <- 0; c <- 1
x <- seq(-2,2,0.01)
f <- a*x^2 + b*x + c
data_X <- data.frame(x, f)
ggplot(data = data_X, aes(x = x,  y = f)) +
geom_line(col = "blue", lwd = 2)+ geom_line(mapping = aes(x, x^2), lty = 3)+
labs(x = "x", y =  "f(x)",
title = expression("f(x)="~x^2+1))
```
$b$ bewirkt eine horizontale und vertikale Verschiebung, die nicht mehr leicht vorhersehbar ist. Für $f(x)=x^2+x$ lässt sich beispielsweise durch Umformen leicht erkennen: $f(x)=x^2+x=x(x+1)$, dass diese Funktion zwei Nullstellen bei $0$ und $-1$ hat. Somit ist ersichtlich, dass die Funktion nach unten und links verschoben ist:
```{r, fig.align="center", fig.height=6}
a <- 1; b <- 1; c <- 0
x <- seq(-2,2,0.01)
f <- a*x^2 + b*x + c
data_X <- data.frame(x, f)
ggplot(data = data_X, aes(x = x,  y = f)) +
geom_line(col = "blue", lwd = 2)+ geom_line(mapping = aes(x, x^2), lty = 3)+
labs(x = "x", y =  "f(x)",
title = expression("f(x)="~x^2+x))
```
Für die genaue Gestalt einer allgemeinen quadratischen Funktion $ax^2 + bx + c$ würden wir die Nullstellen durch Lösen der Gleichung $ax^2 + bx + c=0$ bestimmen (via *p-q Formel* oder *a-b-c-Formel*). Den Scheitelpunkt würden wir durch Ableiten und Nullsetzen der Gleichung bestimmen. Wir müssten also $2ax+b=0$ lösen und dies in die Gleichung einsetzen. Wir könnten auch die binomischen Formeln nutzen, um die Funktion in die Gestalt $f(x):=a'(x-b')^2+c'$ oder $f(x):=a'(x-b'_1)(x-b_2')+c'$ zu bekommen, falls die Nullstellen reell sind (also das Gleichungssystem *lösbar* ist), da wir so die Nullstellen ablesen können als $b'$ oder $b_1'$ und $b_2'$, falls $c=0$. Für die Interpretation der Ergebnisse reicht es zu wissen, dass $a$ eine Stauchung bewirkt und entscheind dafür ist, ob die Funktion u-förmig oder invers-u-förmig verläuft.
```{r, fig.align="center", fig.height=6}
a <- -0.5; b <- 1; c <- 2
x <- seq(-2,2,0.01)
f <- a*x^2 + b*x + c
data_X <- data.frame(x, f)
ggplot(data = data_X, aes(x = x,  y = f)) +
geom_line(col = "blue", lwd = 2)+ geom_line(mapping = aes(x, x^2), lty = 3)+
labs(x = "x", y =  "f(x)",
title = expression("f(x)="~-0.5*x^2+x+2))
```
$\longrightarrow$ so ähnlich sieht die bedingte Beziehung (kontrolliert für die weiteren Prädiktoren im Modell) zwischen Bildungsabschluss der Mutter und Leseleistung aus.
### Code für quadratische Verlaufsgrafik
Der Code, der die Grafik des standardisierten vorhergesagten bedingten Verlaufs des Bildungsabschlusses der Mutter erzeugt, sieht folgendermaßen aus:
```{r, fig.align="center", fig.height=6}
X <- scale(poly(PISA2009$MotherEdu, 2))
std_par_ME <- c(0.1588, -0.1436)
pred_effect_ME <- X %*% std_par_ME
std_ME <- X[,1]
data_ME <- data.frame(std_ME, pred_effect_ME)
ggplot(data = data_ME, aes(x = std_ME,  y = pred_effect_ME)) + geom_point(pch = 16, col = "blue", cex = 4)+
labs(y = "std. Leseleistung | Others", x =  "std. Bildungsabschluss der Mutter | Others",
title = "Standardisierte bedingte Beziehung zwischen\n Bildungsabschluss der Mutter und Leseleistung")
```
Wir verwenden `scale`, um die linearen und quadratischen Anteile des Bildungsabschlusses der Mutter zu standardisieren und speichern sie in `X`. Anschließend ist das Interzept der quadratischen Funktion 0 ($c=0$, da wir standardisiert haben). Die zugehörigen standardisierten Koeffizienten sind $b=0.1588$ und $a=-0.1436$, die wir aus der standardisierten `summary` abgelesen haben. Somit wissen wir, dass es sich um eine invers-u-förmige Beziehung handelt (ohne die Grafik zu betrachten). Wir speichern die standardisierten Koeffizienten unter `std_par_ME` ab und verwenden anschließend das Matrixprodukt ` X %*% std_par_ME`, um die vorhergesagten Werte via $y_{std,i}=0.1588 ME - 0.1436ME^2$ zu berechnen. Diese vorhergesagten Werte `pred_effect_ME` plotten wir nun gegen die standardisierten Werte des Bildungsabschlusses der  Mutter `std_ME`, welche in der ersten Spalte von `X` stehen: `X[, 1]`.
## Literatur
[Eid, M., Gollwitzer, M., & Schmitt, M. (2017).](https://hds.hebis.de/ubffm/Record/HEB366849158) *Statistik und Forschungsmethoden* (5. Auflage, 1. Auflage: 2010). Weinheim: Beltz.
* <small> *Blau hinterlegte Autorenangaben führen Sie direkt zur universitätsinternen Ressource.*
=======
ylab("Average Stringency Index") +
theme_apa()#+xlim(c(0,630))
ggplot(data = df,aes(x = M_Cases_pP,y = M_Index)) +
# scale_x_continuous(trans = "log10", breaks = c(10, 1000, 100000),
#                   labels = c(10, 1000, "100000"))+
geom_point(mapping = aes(M_Cases_pP[Country=="Germany"], M_Index[Country=="Germany"]), cex = 5, col = "grey")+
geom_errorbar(aes(ymin = M_Index - SE_Index,ymax = M_Index + SE_Index),size = .2, color = "darkgrey") +
geom_errorbarh(aes(xmin = M_Cases_pP - SE_Cases_pP,xmax =  M_Cases_pP + SE_Cases_pP), size = .2, color = "darkgrey")+
geom_point(cex=0.8) +
xlab("Average Cumulative COVID-19 Cases per 100,000 Inhabitants")+
ylab("Average Stringency Index") +
theme_apa()+xlim(c(0,630))
ggplot(data = df,aes(x = M_Cases_pP,y = M_Index)) +
# scale_x_continuous(trans = "log10", breaks = c(10, 1000, 100000),
#                   labels = c(10, 1000, "100000"))+
geom_point(mapping = aes(M_Cases_pP[Country=="Germany"], M_Index[Country=="Germany"]), cex = 5, col = "grey")+
geom_errorbar(aes(ymin = M_Index - SE_Index,ymax = M_Index + SE_Index),size = .2, color = "darkgrey") +
geom_errorbarh(aes(xmin = M_Cases_pP - SE_Cases_pP,xmax =  M_Cases_pP + SE_Cases_pP), size = .2, color = "darkgrey")+
geom_point(cex=0.8) +
xlab("Average Cumulative COVID-19 Cases per 100,000 Inhabitants")+
ylab("Average Stringency Index") +
theme_apa()#+xlim(c(0,630))
ggplot(data = df,aes(x = M_Cases_pP,y = M_Index)) +
# scale_x_continuous(trans = "log10", breaks = c(10, 1000, 100000),
#                   labels = c(10, 1000, "100000"))+
geom_point(mapping = aes(M_Cases_pP[Country=="Germany"], M_Index[Country=="Germany"]), cex = 5, col = "grey")+
geom_errorbar(aes(ymin = M_Index - SE_Index,ymax = M_Index + SE_Index),size = .2, color = "darkgrey") +
geom_errorbarh(aes(xmin = M_Cases_pP - SE_Cases_pP,xmax =  M_Cases_pP + SE_Cases_pP), size = .2, color = "darkgrey")+
geom_point(cex=0.8) +
xlab("Average Cumulative COVID-19 Cases per 100,000 Inhabitants")+
ylab("Average Stringency Index") +
theme_apa()+xlim(c(0,630))
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
2^c(1,2)
2^c(-1,-2)
2^c(0.5,0.6)
982454635
0.0093*10^9
source('startup.R')
data(conspiracy, package = 'PsyBSc7')
load(url("https://pandar.netlify.app/post/conspiracy.rda"))
conspiracy$id <- as.factor(1:nrow(conspiracy))
ezANOVA(data = conspiracy, wid = id, dv = ET, between = urban)
library(ez)
ezANOVA(data = conspiracy, wid = id, dv = ET, between = urban)
aggregate(ET ~ urban, data = conspiracy, mean)
tapply(X = conspiracy$ET, INDEX = list(conspiracy$urban), FUN = mean)
tapply(X = conspiracy$ET, INDEX = conspiracy$urban, FUN = mean)
# Mithilfe des aggregate-Befehls
aggregate(ET ~ urban, data = conspiracy, mean)
aggregate(ET ~ edu, data = conspiracy, mean)
# Mithilfe des aggregate-Befehls mit anderer Schreibweise (wie bei tapply)
aggregate(conspiracy$ET, conspiracy$urban, mean)
# Mithilfe des aggregate-Befehls
aggregate(ET ~ urban, data = conspiracy, mean)
aggregate(ET ~ edu, data = conspiracy, mean)
# Mithilfe des aggregate-Befehls mit anderer Schreibweise (wie bei tapply)
aggregate(conspiracy$ET, list(conspiracy$urban), mean)
aggregate(conspiracy$ET, list(conspiracy$edu), mean)
# Mithilfe des describeBy-Befehls aus dem psych-Paket
library(psych)
describeBy(conspiracy$ET, conspiracy$urban)
describeBy(conspiracy$ET, conspiracy$edu)
load(url("https://pandar.netlify.app/post/fairplayer.rda"))
fairplayer$emt1z <- scale(fairplayer$emt1)
fairplayer
fairplayer$emt1
fairplayer
fairplayer$emt1 <- colMeans(fairplayer[, c("em1t1", "em2t1", "em3t1")], na.rm = T)
fairplayer$emt1
colMeans(fairplayer[, c("em1t1", "em2t1", "em3t1")], na.rm = T)
fairplayer$emt1 <- rowMeans(fairplayer[, c("em1t1", "em2t1", "em3t1")], na.rm = T)
fairplayer$emt1
fairplayer$sit1
fairplayer$sit1 <- rowMeans(fairplayer[, c("si1t1", "si2t1", "si3t1")], na.rm = T)
fairplayer$sit1
fairplayer$emt1 <- rowMeans(fairplayer[, c("em1t1", "em2t1", "em3t1")], na.rm = T)
fairplayer$sit1 <- rowMeans(fairplayer[, c("si1t1", "si2t1", "si3t1")], na.rm = T)
fairplayer$emt1z <- scale(fairplayer$emt1)
fairplayer$sit1z<-scale(fairplayer$sit1)
fairplayer$Int <- fairplayer$emt1z*fairplayer$sit1z
modLz <- '
# Regression
rat1 ~ 1
rat1 ~ sit1z
rat1 ~ emt1z
rat1 ~ Int
# Residuum
rat1 ~~ rat1'
fit_z <- lavaan(modLz, fairplayer)
library(lavaan)
modLz <- '
# Regression
rat1 ~ 1
rat1 ~ sit1z
rat1 ~ emt1z
rat1 ~ Int
# Residuum
rat1 ~~ rat1'
fit_z <- lavaan(modLz, fairplayer)
fairplayer$emt1 <- rowMeans(fairplayer[, c('em1t1', 'em2t1', 'em3t1')],
na.rm = TRUE)
fairplayer$sit1 <- rowMeans(fairplayer[, c('si1t1', 'si2t1', 'si3t1')],
na.rm = TRUE)
fairplayer$rat1 <- rowMeans(fairplayer[, c('ra1t1', 'ra2t1', 'ra3t1')],
na.rm = TRUE)
fairplayer$emt1z <- scale(fairplayer$emt1)
fairplayer$sit1z<-scale(fairplayer$sit1)
fairplayer$Int <- fairplayer$emt1z*fairplayer$sit1z
library(lavaan)
modLz <- '
# Regression
rat1 ~ 1
rat1 ~ sit1z
rat1 ~ emt1z
rat1 ~ Int
# Residuum
rat1 ~~ rat1'
fit_z <- lavaan(modLz, fairplayer)
summary(fit_z)
lm(rat1~sit1z*emt1z)
lm(rat1~sit1z*emt1z, data = fairplayer)
coef(fit_z)
lm(rat1~sit1z*emt1z, data = fairplayer)
load(url("https://pandar.netlify.app/post/fairplayer.rda"))
fairplayer$emt1 <- rowMeans(fairplayer[, c('em1t1', 'em2t1', 'em3t1')],
na.rm = TRUE)
fairplayer$sit1 <- rowMeans(fairplayer[, c('si1t1', 'si2t1', 'si3t1')],
na.rm = TRUE)
fairplayer$rat1 <- rowMeans(fairplayer[, c('ra1t1', 'ra2t1', 'ra3t1')],
na.rm = TRUE)
fairplayer$emt1z <- scale(fairplayer$emt1, scale = F)
fairplayer$sit1z<-scale(fairplayer$sit1, scale = F)
fairplayer$Int <- fairplayer$emt1z*fairplayer$sit1z
library(lavaan)
modLz <- '
# Regression
rat1 ~ 1
rat1 ~ sit1z
rat1 ~ emt1z
rat1 ~ Int
# Residuum
rat1 ~~ rat1'
fit_z <- lavaan(modLz, fairplayer)
summary(fit_z)
coef(fit_z)
lm(rat1~sit1z*emt1z, data = fairplayer)
summary(lm(ET ~ urban*edu, data = conspiracy))
anova(lm(ET ~ urban*edu, data = conspiracy))
summary(aov(ET ~ urban*edu, data = conspiracy))
options(contrasts=c(unordered="contr.sum", ordered="contr.poly"))
anova(lm(ET ~ urban*edu, data = conspiracy))
options(contrasts=c(unordered="contr.sum", ordered="contr.poly"))
anova(lm(ET ~ urban*edu, data = conspiracy))
options(contrasts=c(unordered="contr.treatment", ordered="contr.poly"))
anova(lm(ET ~ urban*edu, data = conspiracy))
??options
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
load(url("https://pandar.netlify.app/post/Schulleistungen.rda"))
Schulleistungen_std <- scale(Schulleistungen)
reg <- lm(math ~ IQ*reading, data = Schulleistungen_std)
Schulleistungen_std
Schulleistungen_std <- data.frame(scale(Schulleistungen))
reg <- lm(math ~ IQ*reading, data = Schulleistungen_std)
reg
summary(reg)
library(interactions)
mod_reg <- lm(math ~ IQ*reading, data = Schulleistungen_std)
mod_reg
summary(mod_reg)
interact_plot(model = mod_reg, pred = IQ, modx = reading)
interact_plot(model = mod_reg, pred = reading, modx = IQ)
X <- mvtnorm::rmvnorm(n = 10^4, mean = c(0,0), sigma = matrix(c(1,.5,.5,1),2,2))
X1 <- X[,1];X2<-X[,2]
Y <- 0.5*X1 + 0.3*X2 + 0.1*X1*X2 + rnorm(dim(X)[1])
summary(lm(Y~X1+X2))
summary(lm(Y~X1+X2+I(X1*X2)))
summary(lm(Y~X1+X2+I(X1^2)))
summary(lm(Y~X1+X2+I(X1^2)+I(X^2)))
summary(lm(Y~X1+X2+I(X1^2)+I(X2^2)))
summary(lm(Y~X1+X2+I(X1^2)+I(X2^2)+I(X1*X2)))
Y <- 0.5*X1 + 0.3*X2 + 0.1*X1^2 + rnorm(dim(X)[1])
summary(lm(Y~X1+X2))
summary(lm(Y~X1+X2+I(X1*X2)))
summary(lm(Y~X1+X2+I(X1^2)+I(X2^2)+I(X1*X2)))
mod_reg <- lm(reading ~ IQ*math, data = Schulleistungen_std)
mod_reg
summary(mod_reg)
mod_reg <- lm(reading ~ IQ*math + I(math^2)+I(IQ^2), data = Schulleistungen_std)
mod_reg
summary(mod_reg)
mod_reg <- lm(reading ~ IQ+math+ I(IQ*math) + I(math^2)+I(IQ^2), data = Schulleistungen_std)
mod_reg
summary(mod_reg)
mod_reg <- lm(reading ~ IQ+math+ I(math^2)+I(IQ*math) +I(IQ^2), data = Schulleistungen_std)
summary(mod_reg)
load(url("https://pandar.netlify.app/post/PISA2009.rda"))
names(PISA2009)
reg1 <- lm(Reading ~ JoyRead*LearnMins, data = PISA2009)
summary(reg1)
reg2 <- lm(Reading ~ JoyRead*LearnMins + I(JoyRead^2)+I(LearnMins^2), data = PISA2009)
summary(reg2)
summary(reg1)
coef(reg1)
coef(reg1)["JoyRead:LearnMins"]
coef(reg2)
coef(reg2)["JoyRead:LearnMins"]/coef(reg1)["JoyRead:LearnMins"]
coef(mod_reg)
coef(mod_reg)["I(IQ * math)"]
PISA2009_std <- data.frame(scale(PISA2009))
reg2 <- lm(Reading ~ JoyRead*LearnMins + I(JoyRead^2)+I(LearnMins^2), data = PISA2009)
summary(reg2)
reg1 <- lm(Reading ~ JoyRead*LearnMins, data = PISA2009_std)
summary(reg1)
PISA2009_std <- data.frame(scale(PISA2009))
reg2 <- lm(Reading ~ JoyRead*LearnMins + I(JoyRead^2)+I(LearnMins^2), data = PISA2009_std)
summary(reg2)
coef(reg2)["JoyRead:LearnMins"]/coef(reg1)["JoyRead:LearnMins"]
mod_reg <- lm(reading ~ IQ+math+ I(IQ*math), data = Schulleistungen_std)
mod_quad_reg <- lm(reading ~ IQ+math+ I(math^2)+I(IQ*math) +I(IQ^2), data = Schulleistungen_std)
coef(mod_reg)["I(IQ * math)"]/coef(mod_quad_reg)["I(IQ * math)"]
coef(mod_quad_reg)["I(IQ * math)"]
coef(mod_reg)["I(IQ * math)"]
mod_reg <- lm(reading ~ IQ+math+ I(IQ*math), data = Schulleistungen_std)
mod_quad_reg <- lm(reading ~ IQ+math+ I(math^2)+I(IQ*math) +I(IQ^2), data = Schulleistungen_std)
summary(mod_reg)
summary(mod_quad_reg)
summary(mod_reg)
coef(summary(mod_reg))
coef(summary(mod_reg))["I(IQ * math)",1:3]
coef(summary(mod_reg))["I(IQ * math)",1:3]/coef(summary(mod_quad_reg))["I(IQ * math)",1:3]
coef(summary(mod_quad_reg))["I(IQ * math)",1:3]/coef(summary(mod_reg))["I(IQ * math)",1:3]
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(car)
library(MASS)
### Datensatz: Corona-Pandemie 2020
confirmed <- read.csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv')
deaths <- read.csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv')
### Long format ---
confirmed_long <- reshape(confirmed,
varying = names(confirmed)[-c(1:4)],
v.names = 'Confirmed',
timevar = 'Day',
idvar = names(confirmed)[1:4],
direction = 'long')
deaths_long <- reshape(deaths,
varying = names(deaths)[-c(1:4)],
v.names = 'Deaths',
timevar = 'Day',
idvar = names(deaths)[1:4],
direction = 'long')
### Merged data ----
long <- merge(confirmed_long, deaths_long,
by = c('Province.State', 'Country.Region', 'Lat', 'Long', 'Day'))
### Full data ----
covid <- aggregate(cbind(Confirmed, Deaths) ~ Country.Region + Day, data = long, FUN = 'sum')
### Only data until Day 79 ----
covid_full <- covid
covid <- covid[covid$Day < 80, ]
### Subsets ----
covid_de <- covid[covid$Country.Region == 'Germany', ]
covid_sel <- covid[covid$Country.Region %in% c('France', 'Germany', 'Italy', 'Spain', 'United Kingdom'), ]
covid$Day
covid
confirmed
covid$Day
names(confirmed_long)
names(confirmed)
names(confirmed)[79+4]
confirmed[confirmed$Country.Region == "Germany"]
confirmed[confirmed$Country.Region == "Germany",]
Ger <- confirmed[confirmed$Country.Region == "Germany",]
Ger
Ger <- confirmed[confirmed$Country.Region == "Germany",-c(1:3)]
Ger
Ger <- confirmed[confirmed$Country.Region == "Germany",-c(1:4)]
Ger
Ger[,1:79]
sum(Ger[,1:79])
covid_de
(Ger[,79])
(Ger[,78:79])
confirmed[,1:4]
confirmed[1,1:4]
confirmed[confirmed$Country.Region == "Germany", (78+4):(79+4)]
covid_de[covid_de$Day == c(78,79),]
m_l <- lm(Confirmed ~ Day, data = covid_de) # linearer Verlauf
summary(m_l)
### Quadratisches Modell
m_q <- lm(Confirmed ~ poly(Day, 2), data = covid_de) # quadratischer Verlauf
summary(m_q)
summary(m_q)$r.squared - summary(m_l)$r.squared  # Inkrement
anova(m_l, m_q)
m_l <- lm(Confirmed ~ poly(Day,1), data = covid_de) # linearer Verlauf
summary(m_l)
### Quadratisches Modell
m_q <- lm(Confirmed ~ poly(Day, 2), data = covid_de) # quadratischer Verlauf
summary(m_q)
summary(m_q)$r.squared - summary(m_l)$r.squared  # Inkrement
anova(m_l, m_q)
covid_nation <- covid_sel[covid_sel$Country.Region == "France", ]
covid_nation$log_Confirmed <- log(covid_nation$Confirmed)               # Logarithmieren der "confirmed cases"
covid_sel
library(ggplot2)
library(car)
library(MASS)
### Datensatz: Corona-Pandemie 2020
confirmed <- read.csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv')
deaths <- read.csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv')
### Long format ---
confirmed_long <- reshape(confirmed,
varying = names(confirmed)[-c(1:4)],
v.names = 'Confirmed',
timevar = 'Day',
idvar = names(confirmed)[1:4],
direction = 'long')
deaths_long <- reshape(deaths,
varying = names(deaths)[-c(1:4)],
v.names = 'Deaths',
timevar = 'Day',
idvar = names(deaths)[1:4],
direction = 'long')
### Merged data ----
long <- merge(confirmed_long, deaths_long,
by = c('Province.State', 'Country.Region', 'Lat', 'Long', 'Day'))
### Full data ----
covid <- aggregate(cbind(Confirmed, Deaths) ~ Country.Region + Day, data = long, FUN = 'sum')
### Only data until Day 79 ----
covid_full <- covid
covid <- covid[covid$Day < 80, ]
### Subsets ----
covid_sel <- covid[covid$Country.Region %in% c('France', 'Germany', 'Italy', 'Spain', 'United Kingdom'), ]
covid_de <- covid_sel[covid_sel$Country.Region == "Germany", ]
covid_de$log_Confirmed <- log(covid_de$Confirmed)               # Logarithmieren der "confirmed cases"
covid_de$log_Confirmed[covid_de$log_Confirmed == -Inf] <- NA    # Ersetzen von -unendlich durch missing (NA)
ggplot(covid_de, aes(x = Day, y = log_Confirmed))+geom_line(lwd=2)
covid_de <- covid_de[!is.na(covid_de$log_Confirmed),] # Löschen aller Fälle, in welchen Confrimed = 0 war
### lineares Modell
m_l <- lm(Confirmed ~ poly(Day,1), data = covid_de) # linearer Verlauf
summary(m_l)
### Quadratisches Modell
m_q <- lm(Confirmed ~ poly(Day, 2), data = covid_de) # quadratischer Verlauf
summary(m_q)
summary(m_q)$r.squared - summary(m_l)$r.squared  # Inkrement
anova(m_l, m_q)
covid_de <- covid_de[!is.na(covid_de$log_Confirmed),] # Löschen aller Fälle, in welchen Confrimed = 0 war
covid_de
### lineares Modell
m_l <- lm(Confirmed ~ Day, data = covid_de) # linearer Verlauf
summary(m_l)
### Quadratisches Modell
m_q <- lm(Confirmed ~ poly(Day, 2), data = covid_de) # quadratischer Verlauf
summary(m_q)
summary(m_q)$r.squared - summary(m_l)$r.squared  # Inkrement
anova(m_l, m_q)
covid_de <- covid_sel[covid_sel$Country.Region == "Germany", ]
### lineares Modell
m_l <- lm(Confirmed ~ Day, data = covid_de) # linearer Verlauf
summary(m_l)
### Quadratisches Modell
m_q <- lm(Confirmed ~ poly(Day, 2), data = covid_de) # quadratischer Verlauf
summary(m_q)
summary(m_q)$r.squared - summary(m_l)$r.squared  # Inkrement
anova(m_l, m_q)
covid_de
>>>>>>> df665f1f26029cb13a2f87c046bd58d80ad28012
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
<<<<<<< HEAD
knitr::opts_chunk$set(echo = TRUE, fig.align = "center")
knitr::include_graphics(![](/post/Comments.png){width = 100%})
knitr::opts_chunk$set(echo = TRUE, fig.align = "center")
knitr::include_graphics(![](/post/Comments.png){width = 100%})
knitr::opts_chunk$set(echo = TRUE, fig.align = "center")
knitr::include_graphics(![](/post/Comments.png){width = 100%})
knitr::opts_chunk$set(echo = TRUE, fig.align = "center")
knitr::include_graphics(![](/post/Comments.png){width = 100%})
![](/post/Comments.png){width = 100%}
<!-- https://i.redd.it/b9e4xbeg40151.jpg Ich habe keine Ahnung, ob das Bild urheberechtlich geschützt ist, und weiß nicht wie ich es herausfinden könnte-->
knitr::opts_chunk$set(echo = TRUE, fig.align = "center")
zahl <- 100
zahl = 100
log(100)
log(zahl)
args(round)
round(1.2859)
round(1.2859, digits = 2)
round(digits = 2, x = 1.2859)
zahlen <- c(8, 3, 4)
zahlen * 3
knitr::include_graphics("Vektoren.png")
str(zahlen)
class(zahlen) #alternativer Befehl
abfrage <- zahlen == 3 #elementenweise logische Abfrage
str(abfrage)
zeichen <- as.character(zahlen)
str(zeichen)
gender <- c(0, 1, 0, 2, 1, 1, 0, 0, 2)
str(gender)
gender_factor <- as.factor(gender)
str(gender_factor)
knitr::include_graphics("Matrizen.png")
mat<- matrix(c(7, 3, 9, 1, 4, 6), ncol = 2)
mat <- matrix(c(7, 3, 9, 1, 4, 6), ncol = 2)
mat[3, 1]
nrow(mat)
ncol(mat)
dim(mat) #alternativer Befehl
mat2 <-  matrix(c(8, 2, 11, 3, 5, 9), ncol = 2)
combined <- cbind(mat, mat2)
combined
knitr::include_graphics("Packages.png")
library(psych)
load(url("https://pandar.netlify.app/post/mach.rda"))
mean(mach$TIPI1)
mean(mach[,1]) #Alle Zeilen, erste Spalte
knitr::include_graphics("Screenshot1.png")
knitr::include_graphics("Screenshot2.png")
knitr::include_graphics("Screenshot3.png")
knitr::include_graphics("Screenshot4.png")
knitr::include_graphics("Screenshot5.png")
blogdown:::serve_site()
knitr::opts_chunk$set(echo = TRUE, fig.align = "center")
zahl <- 100
zahl = 100
log(100)
log(zahl)
args(round)
round(1.2859)
round(1.2859, digits = 2)
round(digits = 2, x = 1.2859)
zahlen <- c(8, 3, 4)
zahlen * 3
str(zahlen)
class(zahlen) #alternativer Befehl
abfrage <- zahlen == 3 #elementenweise logische Abfrage
str(abfrage)
zeichen <- as.character(zahlen)
str(zeichen)
gender <- c(0, 1, 0, 2, 1, 1, 0, 0, 2)
str(gender)
gender_factor <- as.factor(gender)
str(gender_factor)
mat<- matrix(c(7, 3, 9, 1, 4, 6), ncol = 2)
mat <- matrix(c(7, 3, 9, 1, 4, 6), ncol = 2)
mat[3, 1]
nrow(mat)
ncol(mat)
dim(mat) #alternativer Befehl
mat2 <-  matrix(c(8, 2, 11, 3, 5, 9), ncol = 2)
combined <- cbind(mat, mat2)
combined
library(psych)
load(url("https://pandar.netlify.app/post/mach.rda"))
mean(mach$TIPI1)
mean(mach[,1]) #Alle Zeilen, erste Spalte
blogdown:::serve_site()
knitr::include_graphics('/post/Matrizen.png')
![](/pandar/content/post/comments.png){width = 30%}
<!-- https://i.redd.it/b9e4xbeg40151.jpg Ich habe keine Ahnung, ob das Bild urheberechtlich geschützt ist, und weiß nicht wie ich es herausfinden könnte-->
![](/post/Matrizen.png){width=100%}
blogdown:::serve_site()
=======
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
tab = matrix(c(2,6,3,6,2,7,3,6,6,7,6,8,7,7,8,7,5,7,8,9), ncol = 2, byrow = T)
trans.tab = t(tab)
n = length(tab[,1])
mean.x1 = mean(tab[,1])
mean.x2 = mean(tab[,2])
spalte.mean = matrix(c(mean.x1, mean.x2))
spalte.mean
zeile.mean = matrix(c(mean.x1, mean.x2), ncol= 2, byrow = TRUE)
zeile.mean
1/n * trans.tab %*% tab - spalte.mean %*% zeile.mean
cov.mat = function(x, col) {
if (col == 2) {
trans.tab = t(x)
n = length(x[,1])
mean.x1 = mean(x[,1])
mean.x2 = mean(x[,2])
spalte.mean = matrix(c(mean.x1, mean.x2))
zeile.mean = matrix(c(mean.x1, mean.x2), ncol= 2, byrow = TRUE)
cov.mat = 1/n * trans.tab %*% x - spalte.mean %*% zeile.mean
} else if  (col == 3) {
trans.tab = t(x)
n = length(x[,1])
mean.x1 = mean(x[,1])
mean.x2 = mean(x[,2])
mean.x3 = mean(x[,3])
spalte.mean = matrix(c(mean.x1, mean.x2,mean.x3))
zeile.mean = matrix(c(mean.x1, mean.x2, mean.x3), ncol= 3, byrow = TRUE)
cov.mat = 1/n * trans.tab %*% x - spalte.mean %*% zeile.mean
} else if (col == 4) {
trans.tab = t(x)
n = length(x[,1])
mean.x1 = mean(x[,1])
mean.x2 = mean(x[,2])
mean.x3 = mean(x[,3])
mean.x4 = mean(x[,4])
spalte.mean = matrix(c(mean.x1, mean.x2,mean.x3, mean.x4))
zeile.mean = matrix(c(mean.x1, mean.x2, mean.x3,mean.x4), ncol= 4, byrow = TRUE)
cov.mat = 1/n * trans.tab %*% x - spalte.mean %*% zeile.mean
} else if (col == 5) {
trans.tab = t(x)
n = length(x[,1])
mean.x1 = mean(x[,1])
mean.x2 = mean(x[,2])
mean.x3 = mean(x[,3])
mean.x4 = mean(x[,4])
mean.x5 = mean(x[,5])
spalte.mean = matrix(c(mean.x1, mean.x2,mean.x3, mean.x4,mean.x5))
zeile.mean = matrix(c(mean.x1, mean.x2, mean.x3,mean.x4, mean.x5), ncol= 5, byrow = TRUE)
cov.mat = 1/n * trans.tab %*% x - spalte.mean %*% zeile.mean
}
return(cov.mat)
}
matrix_beispiel = matrix(c(6,24,32,46,53,62,76,87,92,160), ncol = 5, byrow = T)
matrix_beispiel
cov.mat(matrix_beispiel, 5)
cov(matrix_beispiel)
cov(matrix_beispiel)*(nrow(matrix_beispiel)-1)/nrow(matrix_beispiel)
Sigma <- matrix(0.5, 10, 10); diag(Sigma) <- 2:11
data <- mvtnorm::rmvnorm(n = 100, mean = rep(0, 10), sigma = )
data <- mvtnorm::rmvnorm(n = 100, mean = rep(0, 10), sigma = Sigma)
cov(data)*(nrow(data)-1)/nrow(data)
COV1 <- cov(data)*(nrow(data)-1)/nrow(data)
d <- data
n <- nrow(d)
zeile.mean <- t(colMeans(d))
zeile.mean
data <- mvtnorm::rmvnorm(n = 100, mean = 1:10, sigma = Sigma)
COV1 <- cov(data)*(nrow(data)-1)/nrow(data)
d <- data
n <- nrow(d)
zeile.mean <- t(colMeans(d))
zeile.mean
zeile.mean <- t(colMeans(d))
trans.tab <- t(d)
cov.mat = 1/n * trans.tab %*% x - spalte.mean %*% zeile.mean
n <- nrow(x)
zeile.mean <- t(colMeans(x))
x <- data
n <- nrow(x)
zeile.mean <- t(colMeans(x))
trans.tab <- t(x)
cov.mat = 1/n * trans.tab %*% x - spalte.mean %*% zeile.mean
spalte.mean <- t(zeile.mean)
trans.tab <- t(x)
cov.mat = 1/n * trans.tab %*% x - spalte.mean %*% zeile.mean
cov.mat
mycov <- function(x)
{
n <- nrow(x)
zeile.mean <- t(colMeans(x))
spalte.mean <- t(zeile.mean)
trans.tab <- t(x)
cov.mat = 1/n * trans.tab %*% x - spalte.mean %*% zeile.mean
return(cov.mat)
}
mycov(data)
COV2 <- mycov(data)
COV1-COV2
mycov_loop <- function(x)
{
n <- nrow(x)
p <- ncol(x)
means <- rep(NA, p) # leerer Vektor der Länge p (Anzahl Spalten)
for(i in 1:p)
{
means[i] <- mean(x[,i])
}
zeile.mean <- t(means)
spalte.mean <- t(zeile.mean)
trans.tab <- t(x)
cov.mat = 1/n * trans.tab %*% x - spalte.mean %*% zeile.mean
return(cov.mat)
}
mycov_loop(data)
COV3 - COV1
COV3 <-mycov_loop(data)
COV3 - COV1
round(COV3 - COV1, 10)
round(COV1 - COV2, 10)
>>>>>>> df665f1f26029cb13a2f87c046bd58d80ad28012
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
![Grafische Darstellung einer einfachen linearen Regression](images/Bild1.png){width="80%"}
blogdown:::new_post_addin()
confirmed <- read.csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv')
deaths <- read.csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv')
View(confirmed)
#Daten abrufen
load(url("https://pandar.netlify.app/post/Schulleistungen.rda"))
View(Schulleistungen)
# Vorbereitungen
knitr::opts_chunk$set(echo = TRUE, fig.align = "center")
load(url("https://pandar.netlify.app/post/WorldPopulation.rda"))
View(WorldPopulation)
knitr::opts_chunk$set(echo = TRUE, fig.align = "center")
load(url("https://pandar.netlify.app/post/conspiracy.rda"))
View(conspiracy)
knitr::opts_chunk$set(echo = TRUE, fig.align = "center")
load(url("https://pandar.netlify.app/post/PISA2009.rda"))
View(PISA2009)
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
knitr::opts_chunk$set(echo = TRUE, fig.align = "center")
load(url("https://pandar.netlify.app/post/PISA2009.rda"))
View(PISA2009)
blogdown:::serve_site()
blogdown:::serve_site()
