L <- PCA1$loadings[,]
L %*% t(L)
t(L) %*% L
round(t(L) %*% L, 2)
PCA1 <- pca(r = cor(dataUV), nfactors = 2, rotate = "none")
L <- PCA1$loadings[,]
L %*% t(L)
round(t(L) %*% L, 2)
PCA1$values
PCA1$residual
PCA1$Structure
PCA1$factors
PCA1$residual
PCA1$fn
mx <- lm(y ~ x1 + x2 + x3 + x4 + x5 + x6, data = data)
vif(mx)
car::vif(mx)
PCs <- as.matrix(dataUV) %*% PCA3$weights
PCA3 <- pca(r = R_UV, nfactors = 2, rotate = "varimax")
PCs <- as.matrix(dataUV) %*% PCA3$weights
PCA3 <- pca(r = cor(dataUV), nfactors = 2, rotate = "varimax")
PCs <- as.matrix(dataUV) %*% PCA3$weights
Z <- PCs %*% PCA3$loadings
PCs
PCA3$loadings
Z <- PCs %*% PCA3$loadings[,]
PCs
PCA3$loadings[,]
Z <- PCs %*% t(PCA3$loadings[,])
Z
cor(Z)
PCA3$loadings[,]
t(PCA3$loadings[,])
cor(Z)
t(PCA3$loadings[,]) %*% (PCA3$loadings[,])
(PCA3$loadings[,]) %*% t(PCA3$loadings[,])
PCA3 <- pca(r = cor(dataUV), nfactors = 2, rotate = "none")
PCs <- as.matrix(dataUV) %*% PCA3$weights
Z <- PCs %*% t(PCA3$loadings[,])
cor(Z)
(PCA3$loadings[,]) %*% t(PCA3$loadings[,])
PCs <- as.matrix(dataUV) %*% PCA3$weights %*% (diag(PCA3$values))
(diag(PCA3$values))
PCA3 <- pca(r = cor(dataUV), nfactors = 2, rotate = "none")
PCA3
PCs <- as.matrix(dataUV) %*% PCA3$weights %*% (diag(PCA3$values[1:2]))
Z <- PCs %*% t(PCA3$loadings[,])
cor(Z)
(PCA3$loadings[,]) %*% t(PCA3$loadings[,])
PCA3
(PCA3$loadings[,]) %*% t(PCA3$loadings[,])
cor(dataUV, PCs)
PCA3 <- pca(r = cor(dataUV), nfactors = 2, rotate = "none")
PCs <- as.matrix(dataUV) %*% (PCA1$weights %*% diag(PCA1$values[1:2]))
Z <- PCs %*% t(PCA3$loadings[,])
cor(Z)
t(PCA3$loadings[,])
PCs
t(PCA3$loadings[,])
PCs %*% t(PCA3$loadings[,])
PCA3 <- pca(r = cor(dataUV), nfactors = 6, rotate = "none")
PCs <- as.matrix(dataUV) %*% (PCA1$weights %*% diag(PCA1$values[1:6]))
PCA3 <- pca(r = cor(dataUV), nfactors = 6, rotate = "none")
PCs <- as.matrix(dataUV) %*% (PCA3$weights %*% diag(PCA3$values[1:2]))
PCA3 <- pca(r = cor(dataUV), nfactors = 2, rotate = "none")
PCs <- as.matrix(dataUV) %*% (PCA3$weights %*% diag(PCA3$values[1:2]))
PCs
Z <- PCs %*% t(PCA3$loadings[,])
cor(Z)
(PCA3$weights %*% diag(PCA3$values[1:2]))
PCs <- as.matrix(dataUV) %*% (PCA3$weights %*% diag(PCA3$values[1:2]))
PCs
PCs
Z <- PCs %*% t(PCA3$loadings[,])
cor(Z)
(PCA3$loadings[,]) %*% t(PCA3$loadings[,])
cor(dataUV, PCs)
PCs
t(PCA3$loadings[,])
Z <- PCs %*% t(PCA3$loadings[,])
cor(Z)
cov(Z)
cor(Z)
PCA3 <- pca(r = cor(dataUV), nfactors = 2, rotate = "none")
PCs <- as.matrix(dataUV) %*% (PCA3$weights %*% diag(PCA3$values[1:2]))
PCA3 <- pca(r = cor(dataUV), nfactors = 6, rotate = "none")
PCs <- as.matrix(dataUV) %*% (PCA3$weights %*% diag(PCA3$values[1:6]))
PCs
Z <- PCs %*% t(PCA3$loadings[,])
cor(Z)
cor(dataUV)
PCs <- as.matrix(dataUV) %*% PCA3$weights #(PCA3$weights %*% diag(PCA3$values[1:6]))
Z <- PCs %*% t(PCA3$loadings[,])
cor(Z)
cor(dataUV)
PCA3 <- pca(r = cor(dataUV), nfactors = 2, rotate = "varimax")
PCs <- as.matrix(dataUV) %*% PCA3$weights
Z <- PCs %*% t(PCA3$loadings[,])
cor(Z)
cor(dataUV)
(PCA3$loadings[,]) %*% t(PCA3$loadings[,])
PCA3 <- pca(r = cor(dataUV), nfactors = 6, rotate = "none")
PCs <- as.matrix(dataUV) %*% PCA3$weights #(PCA3$weights %*% diag(PCA3$values[1:6]))
Z <- PCs %*% t(PCA3$loadings[,])
cor(Z)
cor(dataUV)
PCA3 <- pca(r = cor(dataUV), nfactors = 2, rotate = "none")
PCs <- as.matrix(dataUV) %*% PCA3$weights #(PCA3$weights %*% diag(PCA3$values[1:6]))
Z <- PCs %*% t(PCA3$loadings[,])
cor(Z)
cor(dataUV)
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
<<<<<<< Updated upstream
blogdown:::new_post_addin()
install.packages('rococo')
blogdown:::new_post_addin()
blogdown:::new_post_addin()
blogdown:::new_post_addin()
blogdown:::new_post_addin()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
install.packages(
'effsize'
)
blogdown:::serve_site()
install.packages("psych")
install.packages("nortest")
install.packages("car")
library(psych)
library(nortest)
library(car)
load('fb20.rda')
which(fb20 == '-99')
View(fb20)
blogdown:::serve_site()
tmp <- data.frame(A = c(1, 2, 3, 4), B = c(5, 6, -99, 8))
tmp
tmp[which(tmp == -99) ]
tmp[tmp == -99]
tmp[tmp == -99] <- NA
tmp
mean.lz <- mean(fb20$lz, na.rm = T) #Mittelwert
load('fb20.rda')
fb20 <- read.table('https://pandar.netlify.app/post/fb20.csv',
header = TRUE,
sep = ',',
na.strings = '-99')
mean.lz <- mean(fb20$lz, na.rm = T) #Mittelwert
fb20$lz
describe(fb20$lz) #Funktion aus Paket "psych"
library(psych)
library(nortest)
library(car)
describe(fb20$lz) #Funktion aus Paket "psych"
#geeigneter Plot: QQ-Plot. Alle Punkte sollten auf einer Linie liegen.
qqnorm(fb20$lz)
#Die qqPlot-Funktion zeichnet ein Konfidenzintervall in den QQ-Plot. Dies macht es f?r den Betrachter einfacher zu entscheiden, ob alle Punkte in etwa auf einer Linie liegen. Die Punkte sollten nicht au?erhalb der blauen Linien liegen.
qqPlot(fb20$lz)
#Der Lillie-Test (Kolmogorov-Smirnov Test), testet die H0, dass eine Normalverteilung vorliegt. Daher sollte der Test nicht signifikant sein
lillie.test(fb20$lz)
#Die qqPlot-Funktion zeichnet ein Konfidenzintervall in den QQ-Plot. Dies macht es f?r den Betrachter einfacher zu entscheiden, ob alle Punkte in etwa auf einer Linie liegen. Die Punkte sollten nicht au?erhalb der blauen Linien liegen.
qqPlot(fb20$lz)
t.test(fb20$lz, mu=4.7)
t.test(fb20$lz, mu=4.7, conf.level = 0.99) #Default ist 95%, deshalb erhoehen wir auf 99%
blogdown:::serve_site()
#Der Lillie-Test (Kolmogorov-Smirnov Test), testet die H0, dass eine Normalverteilung vorliegt. Daher sollte der Test nicht signifikant sein
lillie.test(fb20$lz)
fb20 <- read.table('https://pandar.netlify.app/post/fb20.csv',
header = TRUE,
sep = ',',
na.strings = '-99')
dim(fb20)
str(fb20)
library(psych)
library(nortest)
library(car)
mean.lz <- mean(fb20$lz, na.rm = T) #Mittelwert
mean.lz
sd.lz <- sd(fb20$lz, na.rm = T) #Standardabweichung
sd.lz
n.lz <- length(fb20$lz) #Stichprobengroesse
se.lz <- sd.lz/sqrt(n.lz) #Standardfehler
se.lz
describe(fb20$lz) #Funktion aus Paket "psych"
#geeigneter Plot: QQ-Plot. Alle Punkte sollten auf einer Linie liegen.
qqnorm(fb20$lz)
#Die qqPlot-Funktion zeichnet ein Konfidenzintervall in den QQ-Plot. Dies macht es f?r den Betrachter einfacher zu entscheiden, ob alle Punkte in etwa auf einer Linie liegen. Die Punkte sollten nicht au?erhalb der blauen Linien liegen.
qqPlot(fb20$lz)
#Der Lillie-Test (Kolmogorov-Smirnov Test), testet die H0, dass eine Normalverteilung vorliegt. Daher sollte der Test nicht signifikant sein
lillie.test(fb20$lz)
t.test(fb20$lz, mu=4.7)
t.test(fb20$lz, mu=4.7, conf.level = 0.99) #Default ist 95%, deshalb erhoehen wir auf 99%
t.test(fb20$lz, mu=4.7)
t.test(fb20$lz, mu=4.7, conf.level = 0.99) #Default ist 95%, deshalb erhoehen wir auf 99%
fb20 <- read.table('https://pandar.netlify.app/post/fb20.csv',
header = TRUE,
sep = ',',
na.strings = '-99')
dim(fb20)
str(fb20)
library(psych)
library(nortest)
library(car)
mean.lz <- mean(fb20$lz, na.rm = T) #Mittelwert
mean.lz
sd.lz <- sd(fb20$lz, na.rm = T) #Standardabweichung
sd.lz
n.lz <- length(fb20$lz) #Stichprobengroesse
se.lz <- sd.lz/sqrt(n.lz) #Standardfehler
se.lz
describe(fb20$lz) #Funktion aus Paket "psych"
#geeigneter Plot: QQ-Plot. Alle Punkte sollten auf einer Linie liegen.
qqnorm(fb20$lz)
#Die qqPlot-Funktion zeichnet ein Konfidenzintervall in den QQ-Plot. Dies macht es f?r den Betrachter einfacher zu entscheiden, ob alle Punkte in etwa auf einer Linie liegen. Die Punkte sollten nicht au?erhalb der blauen Linien liegen.
qqPlot(fb20$lz)
#Der Lillie-Test (Kolmogorov-Smirnov Test), testet die H0, dass eine Normalverteilung vorliegt. Daher sollte der Test nicht signifikant sein
lillie.test(fb20$lz)
t.test(fb20$lz, mu=4.7)
t.test(fb20$lz, mu=4.7, conf.level = 0.99) #Default ist 95%, deshalb erhoehen wir auf 99%
blogdown:::new_post_addin()
blogdown:::serve_site()
blogdown:::serve_site()
getOption("na.action")
blogdown:::serve_site()
blogdown:::serve_site()
install.packages('OData')
blogdown:::serve_site()
install.packages('ISOcodes')
blogdown:::serve_site()
install.packages('ggthemes')
blogdown:::serve_site()
=======
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
>>>>>>> Stashed changes
blogdown:::serve_site()
0.016 + 1.96*sqrt(0.016*(1-0.016)/1000)
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
68.02/5
136.83/5
??psych::anova.psych
#### R-Skript zur 2. Sitzung zur EFA ####
# R Chunks wurden aus dem HTML exportiert und sind dort alle enthalten
#### Vorbereitung ----
library(corrplot) # Korrelationsmatrix grafisch darstellen
library(psych) # EFA durchführen
library(GPArotation) # EFA Lösung rotieren
# Datensatz laden
data("Big5", package = "PsyMSc1") # shortend Big 5 Questionnaire Data Set
head(Big5, n = 10) # gebe die ersten 10 Zeilen aus
# Französische Stichprobe herausholen und Dimensionen des Datensatzes betrachten
data_France <- Big5[Big5$country == "FR", ]
dim(data_France)
# Weiter kürzen
dataFR <- data_France[, -c(1:4)] # entferne demografische Daten und speichere als "dataFR"
#### Visualisierte Korrelationsmatrix in dataFR
corrplot(corr = cor(dataFR), # Korrelationsmatrix (Datengrundlage)
method = "color", # Zeichne die Ausprägung der Korrelation farblich kodiert
addCoef.col = "black", # schreibe die Korrelationskoeffizienten in schwarz in die Grafik
number.cex = 0.7) # Stelle die Schriftgröße der Koeffizienten ein
##### 2-faktorieller Subdatensatz -----
dataFR2 <- dataFR[,1:6] # Zunächst wählen wir die ersten 6 Items: E1 bis E3 und N1 bis N3
head(dataFR2)
# Visualisierte Korrelationsmatrix
corrplot(corr = cor(dataFR2), # Korrelationsmatrix (Datengrundlage)
method = "color", # Zeichne die Ausprägung der Korrelation farblich kodiert
addCoef.col = "black", # schreibe die Korrelationskoeffizienten in schwarz in die Grafik
number.cex = 1) # Stelle die Schriftgröße der Koeffizienten ein
## Parallelanalysen
fa.parallel(dataFR2)
fa.parallel(dataFR2, fa = "fa")
## Orthogonale EFA ---
fa(dataFR2, nfactors = 2, rotate = "varimax")
two_factor <- fa(dataFR2, nfactors = 2, rotate = "varimax")
names(two_factor)
# Faktorladungen
two_factor$loadings
# Strukturmatrix
two_factor$Structure
## Oblique EFA ---
two_factor_oblimin <- fa(dataFR2, nfactors = 2, rotate = "oblimin")
# Faktorladungen
two_factor_oblimin$loadings # Ladungsmatrix
# Phi
two_factor_oblimin$Phi # Korrelationsmatrix der Faktoren
# Strukturmatrix
two_factor_oblimin$Structure
### ML-EFA ---
one_factor_ML <- fa(dataFR2, nfactors = 1, rotate = "oblimin", fm = "ml")
two_factor_ML
two_factor_ML$communality
diag(two_factor_ML$loadings[,] %*% two_factor_ML$Phi %*% t(two_factor_ML$loadings[,]))
t(two_factor_ML$loadings[,]) %*% two_factor_ML$loadings[,]
t(two_factor_ML$Structure[,]) %*% two_factor_ML$Structure[,]
two_factor_ML$values
two_factor_ML$e.values
L <- two_factor_ML$loadings[,]
diag(t(L)  %*% L)
two_factor_ML$Vaccounted[1,]
diag(t(two_factor_ML$Structure[,]) %*% two_factor_ML$loadings[,])
aov12 <- anova(one_factor_ML, two_factor_ML)
#### R-Skript zur 2. Sitzung zur EFA ####
# R Chunks wurden aus dem HTML exportiert und sind dort alle enthalten
#### Vorbereitung ----
library(corrplot) # Korrelationsmatrix grafisch darstellen
library(psych) # EFA durchführen
library(GPArotation) # EFA Lösung rotieren
# Datensatz laden
data("Big5", package = "PsyMSc1") # shortend Big 5 Questionnaire Data Set
head(Big5, n = 10) # gebe die ersten 10 Zeilen aus
# Französische Stichprobe herausholen und Dimensionen des Datensatzes betrachten
data_France <- Big5[Big5$country == "FR", ]
dim(data_France)
# Weiter kürzen
dataFR <- data_France[, -c(1:4)] # entferne demografische Daten und speichere als "dataFR"
#### Visualisierte Korrelationsmatrix in dataFR
corrplot(corr = cor(dataFR), # Korrelationsmatrix (Datengrundlage)
method = "color", # Zeichne die Ausprägung der Korrelation farblich kodiert
addCoef.col = "black", # schreibe die Korrelationskoeffizienten in schwarz in die Grafik
number.cex = 0.7) # Stelle die Schriftgröße der Koeffizienten ein
##### 2-faktorieller Subdatensatz -----
dataFR2 <- dataFR[,1:6] # Zunächst wählen wir die ersten 6 Items: E1 bis E3 und N1 bis N3
head(dataFR2)
# Visualisierte Korrelationsmatrix
corrplot(corr = cor(dataFR2), # Korrelationsmatrix (Datengrundlage)
method = "color", # Zeichne die Ausprägung der Korrelation farblich kodiert
addCoef.col = "black", # schreibe die Korrelationskoeffizienten in schwarz in die Grafik
number.cex = 1) # Stelle die Schriftgröße der Koeffizienten ein
## Parallelanalysen
fa.parallel(dataFR2)
fa.parallel(dataFR2, fa = "fa")
## Orthogonale EFA ---
fa(dataFR2, nfactors = 2, rotate = "varimax")
two_factor <- fa(dataFR2, nfactors = 2, rotate = "varimax")
names(two_factor)
# Faktorladungen
two_factor$loadings
# Strukturmatrix
two_factor$Structure
## Oblique EFA ---
two_factor_oblimin <- fa(dataFR2, nfactors = 2, rotate = "oblimin")
# Faktorladungen
two_factor_oblimin$loadings # Ladungsmatrix
# Phi
two_factor_oblimin$Phi # Korrelationsmatrix der Faktoren
# Strukturmatrix
two_factor_oblimin$Structure
### ML-EFA ---
one_factor_ML <- fa(dataFR2, nfactors = 1, rotate = "oblimin", fm = "ml")
two_factor_ML <- fa(dataFR2, nfactors = 2, rotate = "oblimin", fm = "ml")
two_factor_ML$communality
diag(two_factor_ML$loadings[,] %*% two_factor_ML$Phi %*% t(two_factor_ML$loadings[,]))
t(two_factor_ML$loadings[,]) %*% two_factor_ML$loadings[,]
t(two_factor_ML$Structure[,]) %*% two_factor_ML$Structure[,]
two_factor_ML$values
two_factor_ML$e.values
L <- two_factor_ML$loadings[,]
diag(t(L)  %*% L)
two_factor_ML$Vaccounted[1,]
diag(t(two_factor_ML$Structure[,]) %*% two_factor_ML$loadings[,])
aov12 <- anova(one_factor_ML, two_factor_ML)
aov12$
aov12$PR
aov12
aov12$df
aov12$d.df
aov12$chiSq
aov12$d.chiSq
aov12$PR
aov12$test
aov12$empirical
aov12$d.empirical
aov12$d.BIC
??fa
stats::factanal
two_factor_ML <- fa(dataFR2, nfactors = 2, rotate = "oblimin", fm = "ml")
diag(two_factor_ML$loadings[,] %*% two_factor_ML$Phi %*% t(two_factor_ML$loadings[,]))
t(two_factor_ML$loadings[,]) %*% two_factor_ML$loadings[,]
t(two_factor_ML$Structure[,]) %*% two_factor_ML$Structure[,]
two_factor_ML$values
two_factor_ML <- fa(dataFR2, nfactors = 6, rotate = "oblimin", fm = "ml")
t(two_factor_ML$loadings[,]) %*% two_factor_ML$loadings[,]
t(two_factor_ML$Structure[,]) %*% two_factor_ML$Structure[,]
two_factor_ML$values
diag(t(two_factor_ML$Structure[,]) %*% two_factor_ML$loadings[,])
two_factor_ML$values
two_factor_ML$e.values
two_factor_ML$values
two_factor_ML$values
diag(t(L)  %*% L)
L <- two_factor_ML$loadings[,]
diag(t(L)  %*% L)
S <- two_factor_ML$Structure[,]
diag(t(S)  %*% L)
two_factor_ML$Vaccounted[1,]
load(url("https://pandar.netlify.app/post/PISA2009.rda"))
load(url("https://pandar.netlify.app/post/PISA2009.rda"))
1e-1
lm(Reading ~ HISEI + MotherEdu + Books, data = PISA2009)
summary(m)
m <- lm(Reading ~ HISEI + MotherEdu + Books, data = PISA2009)
summary(m)
m <- lm(Reading ~ HISEI + MotherEdu + Books*Age, data = PISA2009)
summary(m)
m <- lm(Reading ~ HISEI*Age + MotherEdu + Books, data = PISA2009)
summary(m)
m <- lm(Reading ~ HISEI + MotherEdu + Books*JoyRead, data = PISA2009)
summary(m)
m <- lm(Reading ~ HISEI*JoyRead + MotherEdu + Books, data = PISA2009)
summary(m)
PISA2009$LearnMins
m <- lm(Reading ~ JoyRead, data = PISA2009)
summary(m)
m <- lm(Reading ~ JoyRead*LearnMins, data = PISA2009)
summary(m)
PISA2009
head(PISA2009)
m <- lm(Reading ~ JoyRead*Grade, data = PISA2009)
summary(m)
m <- lm(Reading ~ JoyRead, data = PISA2009)
summary(m)
head(PISA2009)
m <- lm(Reading ~ JoyRead*Female, data = PISA2009)
summary(m)
head(PISA2009)
m <- lm(Reading ~ JoyRead*HISEI, data = PISA2009)
summary(m)
head(PISA2009)
m <- lm(Reading ~ JoyRead*LearnMins, data = PISA2009)
summary(m)
head(PISA2009)
m <- lm(Reading ~ JoyRead*CultPoss, data = PISA2009)
summary(m)
head(PISA2009)
m <- lm(Reading ~ JoyRead*Books, data = PISA2009)
summary(m)
head(PISA2009)
m <- lm(Reading ~ JoyRead*FatherEdu, data = PISA2009)
summary(m)
m <- lm(Reading ~ MotherEdu*FatherEdu, data = PISA2009)
summary(m)
m <- lm(Reading ~ MotherEdu+FatherEdu, data = PISA2009)
summary(m)
load(url("https://pandar.netlify.app/post/Schulleistungen.rda"))
head(Schulleistungen)
m <- lm(reading ~ IQ*math, data = Schulleistungen)
summary(m)
head(Schulleistungen)
m <- lm(reading ~ IQ*math, data = scale(Schulleistungen))
summary(m)
scale(Schulleistungen)
d <- data.frame(scale(Schulleistungen))
m <- lm(reading ~ IQ*math, data = d)
summary(m)
m <- lm.beta(lm(reading ~ IQ*math, data = d))
library(lm.beta)
m <- lm.beta(lm(reading ~ IQ*math, data = d))
summary(m)
install.packages("reghelper")
library(reghelper)
simple_slopes(model = m)
install.packages("pequod")
library(pequod)
simpleSlope(m)
m <- lm(reading ~ IQ*math, data = d)
simpleSlope(m)
library(ggplot2)
interact_plot(reading, pred = IQ, modx = math, data = d)
install.packages("interactions")
library(ggplot2)
interact_plot(m, pred = IQ, modx = math)
library(ggplot2); ibrary(interactions)
library(ggplot2); library(interactions)
interact_plot(m, pred = IQ, modx = math)
knitr::include_graphics("comments.jpg")
zahl <- 100
zahl = 100
log(100)
log(zahl)
args(round)
round(1.2859)
knitr::include_graphics("comments.jpg")
zahl <- 100
zahl = 100
log(100)
log(zahl)
args(round)
round(1.2859)
round(1.2859, digits = 2)
round(digits = 2, x = 1.2859)
zahlen <- c(8, 3, 4)
zahlen * 3
str(zahlen)
class(zahlen) #alternativer Befehl
abfrage <- zahlen == 3 #elementenweise logische Abfrage
str(abfrage)
zeichen <- as.character(zahlen)
str(zeichen)
gender <- c(0, 1, 0, 2, 1, 1, 0, 0, 2)
str(gender)
gender_factor*3 <- as.factor(gender)
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::serve_site()
