---
title: Simulation und Poweranalyse
author: 
date: '2021-09-21'
slug: simulation
categories:
  - BSc2
tags:
  - Simulation
  - Poweranalyse
subtitle: ''
summary: ''
authors: [irmer]
lastmod: '2021-12-10T13:13:57+01:00'
featured: no
header:
  image: "/header/BSc2_Sim_Power.jpg"
  caption: "[Courtesy of pxhere](https://pxhere.com/en/photo/674621)"
projects: []

---

```{r setup, cache = FALSE, echo = FALSE}
knitr::opts_chunk$set(error = TRUE)
```




<details><summary>Kernfragen dieser Lehreinheit</summary>

* Wie können Variablen und ganze Modelle simuliert werden?
* Wie lassen sich der $\alpha$-Fehler (Type I-error, Fehler erster Art) und die Power (Testmacht, Teststärke) empirisch bestimmen?
* Welche anderen Möglichkeiten, den  $\alpha$-Fehler und die Power zu bestimmen, gibt es?
* Wie lassen sich Power-Plots erstellen und was bedeuten sie?
</details>

*** 

## Einleitung {#Einleitung}
In den Vergangenen Sitzungen haben wir verschiedene Tests für unterschiedliche Fragestellungen kennengelernt: $t$-Test und  Wilcoxon-Test für Mittelwertsvergleiche zweier Gruppen sowie den Korrelationstest, um die Beziehung zweier Variablen zu untersuchen. Wie fast alle statistischen Tests folgen die erwähnten Tests einer gewissen Logik. Unter der Null-Hypothese ($H_0$, sie heißt Null-Hypothese, da in der Regel Effekte gegen Null abgesichert werden) und wenn alle Voraussetzungen des Tests erfüllt sind (bspw. Normalverteilung der Variablen beim $t$-Test und beim Korrelationstest) hat die jeweilige Teststatistik eine (mathematisch) herleitbare Verteilung. Diese kennen Sie bereits aus der Vorlesung. Bspw. ist es die $t(n-2)$ Verteilung beim $t$-Test für unabhängige Stichproben mit Gesamtstichprobengröße $n$. An Hand dieser Verteilung wird der kritische $t$-Wert $t_\text{krit.}$ abgelesen, ab welchem dann die Null-Hypothese verworfen wird, wenn der emprische $t$-Wert (im Betrag) extremer ausfällt. Also:

\begin{align}
|t_\text{emp.}| &\le t_\text{krit.} \Longrightarrow H_0\\
|t_\text{emp.}| &> t_\text{krit.} \Longrightarrow H_1
\end{align}

Wenn wir uns ein $\alpha$-Fehlerniveau von $5\%$ vorgeben, dann ist diese Aussage gleichbedeutend mit, dass der $p$-Wert jeweils kleiner ist als der "kritische" $p$-Wert, welcher dem $\alpha$-Niveau entspricht. Es gilt also:

\begin{align}
p &\ge \alpha\ (=5\%) \Longrightarrow H_0\\
p &< \alpha\ (=5\%) \Longrightarrow H_1
\end{align}

Gilt die Null-Hypothese, so sollte der Test nur in höchstens $\alpha=5\%$ der Fälle ein signifikantes Ergebnis anzeigen. Die Aussage beschreibt eigentlich ein Gedankenexperiment: *Wenn wir das gleiche Experiment unendlich häufig und unabhängig von einander wiederholen könnten und somit unendlich Häufig aus einer Population (Grundgesamtheit) ziehen könnte, in welcher es **keinen** Effekt (bspw. Unterschied zwischen Gruppen oder Zusammenhang zwischen Variablen) gibt, dann sollte in höchstens $5\%$ der Fälle (Replikationen) rein durch Zufall ein signifikantes Ergebnis beim Durchführen des inferenzstatistischen Test herauskommen.* 

Hierbei ist es extrem Wichtig, dass ein solcher Test diese Eigenschaften erfüllt, da wir bei einem erhöhten $\alpha$-Fehlerniveau (bspw. $30\%$), dann gar nicht mehr wüssten, ob ein signifikanten Ergebnis herauskam, weil es einen Effekt gibt, oder weil der Test nicht richtig funktioniert. Anders herum sollte ein inferenzstatistischer Test die $H_0$ möglichst häufig verwerfen, wenn die $H_0$ tatsächlich nicht gilt in der Population und es somit einen bedeutsamen Effekt gibt. Die Wahrscheinlichkeit die $H_0$ richtigerweise zu verwerfen (weil in Wahrheit $H_1$ gilt), nennen wir die Power (Testmacht, Teststärke) eines Tests. Sie sollte möglichst hoch sein. Unter Statistikerinnen und Statistikern hat sich die Konvention eingegliedert, dass eine Power von $80\%$ als gut angesehen wird. *Zurück zu unserem Gedankenexperiment: Eine Power von $80\%$ bedeutet, wenn wir das gleiche Experiment unendlich häufig und unabhängig von einander wiederholen könnten und somit unendlich Häufig aus einer Population (Grundgesamtheit) ziehen könnte, in welcher es **einen bedeutsamen** Effekt gibt, dann sollte in mindestens $80\%$ der Fälle ein signifikantes Ergebnis beim Durchführen des inferenzstatistischen Test herauskommen.*

Im Folgenden wollen wir dieses Gedankenexperiment in die Tat umsetzen und unsere Kenntnisse über das Simulieren von Zufallszahlen verwenden, um die Power und den Fehler 1. Art ($\alpha$-Fehler) empirisch zu prüfen. Hierbei beschränken wir uns auf den $t$-Test (in `R t.test`) und den Korrelationstest (in `R cor.test`).

## Simulation und $\alpha$-Fehler
Sie haben nun die Möglichkeit in einen möglichen Forschungsbereich von Methodikerinnen und Methodikern hineinzublicken: Simulationsstudien. Wir wollen das eben beschriebene Gedankenexperiment nun in die Tat umsetzen. Dazu müssen wir jeweils Variablen simulieren und diese dann mit den entsprechenden Tests analysieren.

### Mittelwertsvergleiche: $t$-Test unter $H_0$
Der (klassische) $t$-Test hat folgende Voraussetzungen:

* die Erhebungen sind von einander unabhängig
* die Varianzen in den beiden Gruppen sind gleich groß
* die Variablen sind normalverteilt

Wir simulieren nun zwei Vektoren $X$ und $Y$ der Länge $N=20$, welche standardnormalverteilt sind. Somit sind die die Varianzen über die Gruppen gleich. `R` simuliert unabhängige Zufallszahlen, sodass alle Voraussetzungen erfüllt sind. Mit `set.seed(1234)` machen wir die Analysen außerdem vergleichbar.

```{r}
N <- 20
set.seed(1234)
X <- rnorm(N)
Y <- rnorm(N)
```

Da beide Variablen standardnormalverteilt sind, bedeutet dies, dass die $H_0$ hier erfüllt ist, da beide Variablen einen Mittelwert von 0 haben: $\mu_X=\mu_Y = 0$. Da es sich hier allerdings um eine Zufallsziehung handelt, sind die Mittelwerte der beiden Variablen natürlich nicht exakt 0:

```{r}
mean(X)
mean(Y)
```

sondern sie weichen (zufällig) von der 0 ab. Diese Abweichung ist wird auch Samplevariation genannt. Wir können nun mit Hilfe des $t$-Tests untersuchen, ob die beiden Variablen dengleichen Mittelwert haben (da wir den originalen $t$-Test durchführen wollen, müssen wir `var.equal = TRUE` wählen, da sonst die Variante von `Welch` gerechnet wird): 

```{r}
ttestH0 <- t.test(X, Y, var.equal = TRUE)
ttestH0
ttestH0$statistic # t-Wert
ttestH0$p.value   # zugehöriger p-Wert
```


Die Mittelwertsdifferenz liegt bei `r round(mean(Y)-mean(X), 4)`, was einen $t_\text{emp.}$-Wert von  $t_\text{emp.}=$`r round(ttestH0$statistic, 4)` ergibt. Der zugehörige $p$-Wert liegt bei $p=$`r round(ttestH0$p.value, 4)`. Somit ist diese Mittelwertsdifferenz auf einem $\alpha$-Niveau von $5\%$ nicht statistisch bedeutsam. Wir sind an dieser Stelle aber nicht daran interessiert, wie der $p$-Wert in diesem spezifischen Experiment (mit Seed = 1234) ausfällt, sondern, wir möchten wissen, ob für häufiges Wiederholen, die Null-Hypothese in ca. $5\%$ der Fälle verworfen wird. Wir könnten uns jetzt hinsetzen und den oben gezeigten Code immer wieder ausführen und den $p$-Wert notieren. Das erscheint sehr lästig. Wir können hier bspw. die `replicate`-Funktion verwenden. Dieser können wir die 4 Zeilen, die zum $p$-Wert führen,  in geschwungenden Klammern (`{`...`}`) übergeben und die Funktion führt dann den Code so häufig durch, wie wir dies gerne hätten. Bspw. könnten wir den Code zunächst 10 Mal ausführen lassen:

```{r}
set.seed(1234)
replicate(n = 10, expr = {X <- rnorm(N)
                          Y <- rnorm(N)
                          ttestH0 <- t.test(X, Y, var.equal = TRUE)
                          ttestH0$p.value})
```

Uns werden insgesamt 10 $p$-Werte übergeben. Wenn wir genau hinsehen, dann erkennen wir den ersten $p$-Wert wieder. Dies ist der $p$-Wert unseres Experiments weiter oben. Wiederholen wir nun das Experiment nicht nur 10 Mal, sondern 10000 Mal, dann erhalten wir eine gute Übersicht über das Verhalten der $p$-Werte unter der Null-Hypothese. Dies speichern wir unter dem Namen `pt_H0` ab (für $p$-Werte für den $t$-Test unter der $H_0$-Hypothese):

```{r}
set.seed(1234)
pt_H0 <- replicate(n = 10000, expr = {X <- rnorm(N)
                                      Y <- rnorm(N)
                                      ttestH0 <- t.test(X, Y, var.equal = TRUE)
                                      ttestH0$p.value})
```

Schauen wir uns doch mal die Verteilung der $p$-Werte an:

```{r}
hist(pt_H0, breaks = 20) 
```

Die $p$-Werte erscheinen einigermaßen gleichverteilt auf dem Intervall [0,1]. Dies ist auch genau gewünscht. Unter der $H_0$-Hypothese ist der $p$-Wert uniform (gleichverteilt) auf dem Intervall [0,1]. Somit tritt jeder Wert mit der selben Wahrscheinlichkeit auf. Dies bedeutet gleichzeitig, dass in dem Intervall [0, 0.05) gerade $5\%$ der Fälle Landen sollten. Dies ist gerade das Intervall in dem die signifikanten Ergebnisse landen. Somit erkennen wir, dass unter $H_0$ die Null-Hypothese nur in $5\%$ verworfen werden sollte, wenn wir uns eine $\alpha$-Niveau von $5\%$ vorgeben. Wir können prüfen, ob dem so ist, indem wir den relativen Anteil bestimmen, in welchem die $H_0$ verworfen wird. Dies ist genau dann der Fall, wenn der $p$-Wert kleiner als $0.05$ ist (siehe [Einleitung](#Einleitung)). Die relative Häufigkeit bestimmen wir so:

```{r}
mean(pt_H0 < 0.05)
```

Somit wird die Null-Hypothese hier in `r mean(pt_H0 < 0.05)*100`% der Fälle verworfen. Dies zeigt uns, dass der $t$-Test unter $H_0$ für $N=20$ gut funktioniert, da die empirische Rate des Fehlers 1. Art bei ungefähr $5\%$ liegt. Wir könnten auch untersuchen, wie robust ein Test ist, indem wir eine Annahme verletzten (z.B. Varianzhomogenität) und untersuchen, wie sich das auf das empirische $\alpha$-Niveau und die Verteilung der Teststatistik oder der $p$-Werte ausübt.

Im Übrigen hätten wir auch die ganze Prozedur mit Hilfe der empirischen $t$-Werte durchführen können. Den kritischen $t$-Wert bekommen wir mit `qt(p = .975, df = 38)`, die Dichte erhalten wir mit `dt(x = x, df = 38)`, wobei `x` die gewünschte x-Koordinate ist:

```{r}
set.seed(1234)
tt_H0 <- replicate(n = 10000, expr = {X <- rnorm(N)
                                      Y <- rnorm(N)
                                      ttestH0 <- t.test(X, Y, var.equal = TRUE)
                                      ttestH0$statistic})
hist(tt_H0, breaks = 50, freq = FALSE) # freq = FALSE, damit relative Häufigkeiten eingetragen werden!
x <- seq(-4, 4, 0.01) # Sequenz von -4 bis 4 in 0.01 Schritten
lines(x = x, y = dt(x = x, df = 38), lwd = 2) # lwd = Liniendicke
mean(abs(tt_H0) > qt(p = .975, df = 38)) # empirischer Alpha-Fehler
```

Die Analyse kommt zum exakt gleichen Ergebnis. Das liegt daran, dass der $p$-Wert und der $t$-Wert in einander überführbar sind. Das Histogramm zeigt uns außerdem die $t$-Verteilung unter der Null-Hypothese mit 38 Freiheitsgraden ($N-2$). Die theoretische Kurve (mit `dt`) passt sehr gut zum Histogramm!

### Lineare Beziehungen zwischen Variablen: Korrelationstest unter $H_0$
Der (klassische) Korrelationstest hat fast die identischen Voraussetzungen im Vergleich zum $t$-Test:

* die Erhebungen sind von einander unabhängig
* die Varianzen in den beiden Gruppen sind gleich groß
* die Variablen sind normalverteilt
* die Variablen hängen linear zusammen

Wir können die gleichen Variablen wie zuvor heranziehen, und damit den Korrelationstest durchführen:

```{r}
set.seed(1234)
X <- rnorm(N)
Y <- rnorm(N)
cor(X, Y) # empirische Korrelation
cortestH0 <- cor.test(X, Y)
cortestH0$p.value # empirischer p-Wert
```
Die empirische Korrelation liegt bei `r round(cor(X, Y), 2)`. Die wahre Korrelation liegt bei 0, da `R` Zufallsvektoren unabhängig von einander simuliert. Der $p$-Wert des Korrelationstests liegt bei `r round(cortestH0$p.value, 4)`. Damit ist das Ergebnis nicht statistisch bedeutsam. Die Korrelation von `r round(cor(X, Y), 2)` ist zufällig aufgetreten. Wir wiederholen nun auch dieses Experiment:

```{r}
set.seed(1234)
pcor_H0 <- replicate(n = 10000, expr = {X <- rnorm(N)
                                        Y <- rnorm(N)
                                        cortestH0 <- cor.test(X, Y)
                                        cortestH0$p.value})
```

Die $p$-Werte sind wieder eingermaßen uniform auf [0,1] verteilt:

```{r}
hist(pcor_H0, breaks = 20) 
```

Das empirische $\alpha$-Niveau liegt bei:

```{r}
mean(pcor_H0 < 0.05)
```

Somit wird die Null-Hypothese hier in `r mean(pcor_H0 < 0.05)*100`% der Fälle verworfen. Dies zeigt uns, dass auch der Korrelationstest unter $H_0$ für $N=20$ gut funktioniert, da die empirische Rate des Fehlers 1. Art bei ungefähr $5\%$ liegt.

## Poweranalysen
Mit einer Power-Analyse untersuchen wir im Grunde, wie sich die Wahrscheinlichkeit die Null-Hypothese zu verwerfen, verändert, je nach dem wie groß der Effekt ist. Dem wollen wir nun nachgehen und entsprechend unsere Daten unter der Alternativ-Hypothese simulieren.

### Mittelwertsvergleiche: $t$-Test unter $H_1$

In der Inferenzstatistik gibt es im Grunde nicht "die" Alternativ-Hypothese, sondern eine ganze Batterie an Alternativ-Hypothesen. Beim Mittelwertsvergleich sieht die Alternativ-Hypothese so aus:

$$H_1: \mu_1 \neq \mu_2,$$
was natürlich äquivalent zur folgenden Aussage zur Differenz der beiden Mittelwerte ist: $d=\mu_1-\mu_2:$

$$H_1: d =  \mu_1-\mu_2 = 0.$$
Hier wird $d$ als eine feste Zahl angenommen (z.B. 0.5, $-\sqrt{2}$, $\pi$, 123.456). Je größer der Effekt, desto größer ist die Wahrscheinlichkeit, dass dieser auch identifiziert wird. Hierbei wird die Größe des Effekts relativ zur zufälligen Streuung genommen. Da wir standardisierte Variablen verwendet hatten (standardnormalverteilt), ist es so, dass die Mittelwertsdifferenz $d$ gerade in Vielfachen der Standardabweichung zu interpretieren ist. Bspw. wäre $d=0.5$ eine halbe Standardabweichung. Wir erhalten eine Mittelwertsdifferenz von 0.5, indem wir zu dem zuvorigen Code einfach 0.5 zur Y-Gleichung dazu addieren. "In der Population unterscheiden sich nun `X` und `Y` um 0.5 im Mittelwert, da X einen Mittelwert von 0 hat und `Y` einen Mittelwert von 0 + 0.5 = 0.5."

```{r}
set.seed(12345)
X <- rnorm(N)
Y <- rnorm(N) + 0.5 
ttestH1 <- t.test(X, Y, var.equal = TRUE)
ttestH1$p.value
```

Der empirische $p$-Wert ist diesmal kleiner als $0.05$. Die Frage ist nun, wie häufig kommt das vor für eine Stichprobengröße von $N=20$ pro Gruppe. Wir führen wieder eine Simulation dazu durch:

```{r}
set.seed(12345)
pt_H1 <- replicate(n = 10000, expr = {X <- rnorm(N)
                                      Y <- rnorm(N) + 0.5 
                                      ttestH1 <- t.test(X, Y, var.equal = TRUE)
                                      ttestH1$p.value})
mean(pt_H1 < 0.05) # empirische Power
hist(pt_H1, breaks = 20)
```
Die empirische Power (die Wahrscheinlichkeit in unserer Simulation, dass die $H_0$ verworfen wird) liegt bei `r mean(pt_H1 < 0.05)`. Das Histogramm ist nun alles andere als gleichverteilt. Kleine $p$-Werte nahe Null kommen wesentlich häufiger vor als große $p$-Werte nahe 1. Eine Power von `r mean(pt_H1 < 0.05)` ist allerdings nicht besonders hoch. Nur in etwas mehr als einem Drittel der Replikationen wurde die Mittelwertsdifferenz als statistisch bedeutsam betitelt. Wir wissen aber, weil wir das Modell vorgegeben haben, dass die $H_0$ tatsächlich nicht gilt. Mit größerer Stichprobe wird die Power steigen (siehe dazu im Kapitel [Power-Plots](#PowerPlots) nach).


### Lineare Beziehungen zwischen Variablen: Korrelationstest unter $H_1$
Wir wollen uns ebenfalls die Power für den Korrelationstest ansehen. Dazu müssen wir allerdings korrelierte Variablen generieren. Um das hinzubekommen, müssen wir einige Eigenschaften der Normalverteilung ausnutzen: bspw. dass die Summe zweier normalverteilter Zufallsvariablen wieder normalverteilt ist. Für zwei unabhängige (unkorrelierte) standard-normalverteilte Zufallsvariablen $X$ und $Z$, ist die Zufallsvariable $Y$, die folgendermaßen gebildet wird

$$Y:= \rho X + \sqrt{1-\rho^2}Z,$$
wieder standard-normalverteilt und um den Korrelationskoeffizienten $\rho$ korreliert mit $X$. Wir können also relativ einfach zwei korrelierte Variablen generieren:

```{r}
set.seed(12345)
X <- rnorm(N)
Z <- rnorm(N)
Y <- 0.5*X + sqrt(1 - 0.5^2)*Z
cor(X, Y) # empirische Korrelation
cortestH1 <- cor.test(X, Y)
cortestH1
cortestH1$p.value
```
Der Test ist signifikant. Die empirische Korrelation von `r round(cor(X, Y), 4)` ist statistisch bedeutsam. Wir schauen uns nun für diese $\rho=0.5$ die Power des Korrelationstests an:

```{r}
set.seed(12345)
pcor_H1 <- replicate(n = 10000, expr = {X <- rnorm(N)
                                        Z <- rnorm(N)
                                        Y <- 0.5*X + sqrt(1 - 0.5^2)*Z
                                        cortestH1 <- cor.test(X, Y)
                                        cortestH1$p.value})
mean(pcor_H1 < 0.05) # empirische Power
hist(pcor_H1, breaks = 20)
```

Die empirische Power für eine Korrelation von 0.5 liegt bei `r mean(pcor_H1 < 0.05)`. Das Histogramm ist auch diesmal alles andere als gleichverteilt. Es sieht sogar deutlich stärker nach links gestaucht aus, als für den oben durchgeführten Mittelwertsvergleich. Das liegt an dieser Stelle daran, dass eine Korrelation von 0.5 ein größerer Effekt ist, als eine Mittelwertsdifferenz von 0.5 für Variablen mit einer Varianz von 1. 

## Power-Plots {#PowerPlots}
Mit einem einzelnen Power-Wert lässt sich in der Regel nicht so viel anfangen. Aus diesem Grund werden Power-Plots erstellt, welche darstellen, wie sich die Power bspw. über unterschiedliche Stichprobengrößen  (um die Asymptotik des Tests zu prüfen) oder über unterschiedliche Effektgrößen verändert. 

### Power-Plots für Mittelwertsunterschiede
Wir schauen uns die Power-Plots diesmal nur für die Mittelwertsunterschiede an. Zunächst beginnen wir mit der Asymptotik. Wir wiederholen im einfachsten Fall das Experiment von oben für 5 Stichprobengrößen: $N=20, 40, 60, 80, 100$ (wobei wir das Ergebnis für $N=20$ bereits bestimmt haben). Dazu kopieren wir jeweils den Code von oben und ändern die Stichprobengröße ab:

```{r}
set.seed(12345)
pt_H1_20 <- pt_H1
pt_H1_40 <- replicate(n = 10000, expr = {X <- rnorm(40)
                                         Y <- rnorm(40) + 0.5 
                                         ttestH1 <- t.test(X, Y, var.equal = TRUE)
                                         ttestH1$p.value})
pt_H1_60 <- replicate(n = 10000, expr = {X <- rnorm(60)
                                         Y <- rnorm(60) + 0.5 
                                         ttestH1 <- t.test(X, Y, var.equal = TRUE)
                                         ttestH1$p.value})
pt_H1_80 <- replicate(n = 10000, expr = {X <- rnorm(80)
                                         Y <- rnorm(80) + 0.5 
                                         ttestH1 <- t.test(X, Y, var.equal = TRUE)
                                         ttestH1$p.value})
pt_H1_100 <- replicate(n = 10000, expr = {X <- rnorm(100)
                                          Y <- rnorm(100) + 0.5 
                                          ttestH1 <- t.test(X, Y, var.equal = TRUE)
                                          ttestH1$p.value})
```

Nun haben wir eine ganze Menge an $p$-Werten abgespeichert. Jetzt müssen wir nur noch die Power für jede Bedingung bestimmen. Diese schreiben wir direkt in einen Vektor:

```{r}
t_power <- c(mean(pt_H1_20 < 0.05),
             mean(pt_H1_40 < 0.05),
             mean(pt_H1_60 < 0.05),
             mean(pt_H1_80 < 0.05),
             mean(pt_H1_100 < 0.05))
t_power
```
Wir sehen sehr gut, dass die Power ansteigt. Der zugehörige Power-Plot sieht nun so aus (zunächst legen wir die Stichproben in `Ns` ab):

```{r}
Ns <- seq(20, 100, 20)
plot(x = Ns, y = t_power, type = "b", main = "Power vs. N")
```
Dem Plot entnehmen wir, dass ab etwas über $N=60$ die Power oberhalb der gewünschten $80\%$-Marke liegt. Wir erkennen also, dass die Wahrscheinlichkeit einen Effekt zu finden, wenn dieser da ist, mit steigender Stichprobengröße wächst. Auf diesem Weg kann ein Experiment auch hinsichtlich der nötigen Stichprobengröße geplant werden. Wenn aus Voruntersuchungen oder der Literatur bekannt ist, wie groß ein Effekt zu erwarten ist, dann kann über Poweranalysen untersucht werden, wie groß eine Stichprobe sein muss, um einen Effekt mit hinreichend großer Wahrscheinlichkeit zu identifizieren.

Genauso könnten wir uns fragen, wie groß ein Effekt sein muss, damit mit der vorliegenden Stichprobengröße und mit hinreichend großer Wahrscheinlichkeit ein signifikantes Ergebnis gefunden wird. Um dieser Frage nachzugehen, untersuchen wir die Power, für einige Effektgrößen für $N=20$. Hierzu haben wir bereits zwei Analysen durchgeführt, nämlich einmal mit $d=0.5$ und einmal mit $d=0$ (als wir den $\alpha$-Fehler unter $H_0$ untersucht haben):


```{r}
set.seed(12345)
pt_H1_0 <- pt_H0
pt_H1_0.5 <- pt_H1
pt_H1_0.25 <- replicate(n = 10000, expr = {X <- rnorm(20)
                                           Y <- rnorm(20) + 0.25 
                                           ttestH1 <- t.test(X, Y, var.equal = TRUE)
                                           ttestH1$p.value})
pt_H1_0.75 <- replicate(n = 10000, expr = {X <- rnorm(20)
                                           Y <- rnorm(20) + 0.75 
                                           ttestH1 <- t.test(X, Y, var.equal = TRUE)
                                           ttestH1$p.value})
pt_H1_1 <- replicate(n = 10000, expr = {X <- rnorm(20)
                                        Y <- rnorm(20) + 1 
                                        ttestH1 <- t.test(X, Y, var.equal = TRUE)
                                        ttestH1$p.value})
pt_H1_1.25 <- replicate(n = 10000, expr = {X <- rnorm(20)
                                           Y <- rnorm(20) + 1.25 
                                           ttestH1 <- t.test(X, Y, var.equal = TRUE)
                                           ttestH1$p.value})
t_power_d <- c(mean(pt_H1_0 < 0.05),
               mean(pt_H1_0.25 < 0.05),
               mean(pt_H1_0.5 < 0.05),
               mean(pt_H1_0.75 < 0.05),
               mean(pt_H1_1 < 0.05),
               mean(pt_H1_1.25 < 0.05))
Ds <- seq(0, 1.25, 0.25)
plot(x = Ds, y = t_power_d, type = "b", main = "Power vs. d")
```

Diesem Plot ist nun zu entnehmen, dass eine Mittelwertsdifferenz von größer 0.8 nötig ist, damit die Power hinreichend groß ist. Außerdem wird in diesem Plot auch ersichtlich, dass wenn die Mittelwertsdifferenz 0 ist, dann sind wir gerade im Fall der $H_0$ gelandet. Die Power sollte hier dann nur bei $5\%$ liegen. Falls wir negative Mittelwertsdifferenzen gewählt hätten, dann wäre der Plot identisch, nur an der x-Achse gespiegelt. Natürlich können Informationen zur Effektstärke sowie zur Stichprobengröße gemeinsam in eine Grafik eingefügt werden. Wenn man dies auf die Spitze treibt, dann landet man vielleicht bei diesem schönen Plot:


```{r, echo = F,fig.align="center", fig.height=10, fig.width=10}
library(pwr)
Erg <- c()
for(n in c(2, seq(5, 500, 5)))
{
     d = seq(-1,1,0.02)   
     temp <- pwr.t.test(n = n, d = d)
     Erg <- rbind(Erg, cbind(temp$power, d, n))
     
}
Erg <- data.frame(Erg)
names(Erg) <- c("Power", "d", "n")

library(papaja)
library(ggplot2)
ggplot(data = Erg, aes(x = d, y = Power, col = n, group = n))+
     geom_line(lwd=1)+
     geom_abline(slope = 0,intercept = .05, lty = 3)+
     geom_abline(slope = 0,intercept = .8, lty = 2) + 
     scale_colour_gradientn(colours=rainbow(4))+
     ggtitle("Power vs. d and n", subtitle = " Using formulas instead of simulation")+ theme_apa(base_size = 20)
```

Hier wurden allerdings keine Simulationen durchgeführt (sonst wären die Linien nicht so "smooth"), denn für den $t$-Test lässt sich die Power auch noch leicht über Formeln bestimmen. Diese Formeln sind bspw. in dem `R`-Paket `pwr` hinterlegt. Für Interessierte ist in [Appendix A](#AppendixA) ein kleiner Exkurs in `pwr` dargestellt. 




### Appendix A {#AppendixA} 

<details><summary>**Poweranalysen: geschlossene Formeln**</summary>
Für den $t$-Test lässt sich die Power auch über Formeln finden. Diese sind im `pwr`-Paket implementiert. Die Funktion `pwr.t.test` ist die Richtige. Sie nimmt zwei wichtige Argumente entgegen: `n` und `d`. Hierbei ist $n$ die Stichprobengröße und $d$ ist die Effektstärke nach Cohen:

$$d:=\frac{|\mu_1-\mu_2|}{\sigma},$$
wobei $\mu_1$ und $\mu_2$ die beiden Mittelwerte in den Gruppen sind und $\sigma$
 ist die wahre Standardabweichung über die beiden Gruppen hinweg. Dadurch, dass bei uns die Varianz jeweils 1 in den Gruppen war, hatten wir durch Zufall auch oben die Effektstärke nach Cohen gewählt! Die Werte sind also durchaus vergleichbar. Wir schauen uns die Power für unsere erste Effektstärke von 0.5 bei einer Stichprobengröße von 20 an:
 
```{r}
# falls noch nicht installiert: "install.packages("pwr")"
 library(pwr)
 pwr.t.test(n = 20, d = 0.5)
```
Die Power liegt bei 0.3379, also bei 33.79%. Sie ist damit also gar nicht so verschieden zu unseren `r round(mean(pt_H1_0.5 < 0.05)*100,2)`%. Mehr Informationen zum `pwr`-Paket finden sie [hier](https://www.statmethods.net/stats/power.html).

</details>

