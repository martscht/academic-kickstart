---
title: "Regressionsanalyse I"
date: '2021-04-22'
slug: reg1
categories:
     - BSc7
tags:
- Regression
- Zusammenhangsanalyse
- Gerichtete Zusammenhänge
subtitle: ''
summary: ''
authors: [nehler, schroeder, gruetzmacher]
lastmod: '2021-05-03 12:00:12 CEST'
featured: no
header:
     image: "/header/PsyBSc7_Reg1.jpg"
     caption: "[Courtesy of pexels](https://www.pexels.com/photo/man-looking-in-binoculars-during-sunset-802412/)"
projects: []
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<div id="einleitung" class="section level2">
<h2>Einleitung</h2>
<p>In der <a href="/post/partial">letzten Sitzung</a> haben wir unter anderem Korrelationen zwischen zwei Variablen behandelt. Zur Wiederholung: Mithilfe einer Korrelation kann die Stärke des Zusammenhangs zwischen zwei Variablen quantifiziert werden. Dabei haben beide Variablen den gleichen Stellenwert, d.h. eigentlich ist es egal, welche Variable die x- und welche Variable die y-Variable ist. Wir haben außerdem Methoden kennengelernt, mit denen der Einfluss einer (oder mehrerer) Drittvariablen kontrolliert werden kann; die Partial- und Semipartialkorrelation. In der heutigen Sitzung wollen wir uns hingegen mit gerichteten Zusammenhängen, d.h. mit Regressionen, beschäftigen.</p>
</div>
<div id="lineare-regression" class="section level2">
<h2>Lineare Regression</h2>
<p>Das Ziel einer Regression besteht darin, eine Variable durch eine oder mehrere andere Variablen vorherzusagen (Prognose). Die vorhergesagte Variable wird als Kriterium, Regressand oder auch abhängige Variable (AV) bezeichnet und üblicherweise mit <span class="math inline">\(y\)</span> symbolisiert. Die Variablen zur Vorhersage der abhängigen Variablen werden als Prädiktoren, Regressoren oder unabhängige Variablen (UV) bezeichnet und üblicherweise mit <span class="math inline">\(x\)</span> symbolisiert. Im ersten Semester hatten wir stets nur einen Prädiktor - dies kann jedoch jetzt erweitert werden. Deshalb bekommen die Prädiktoren einen Indize <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span> usw.
Die häufigste Form der Regressionsanalyse ist die lineare Regression, bei der der Zusammenhang über eine Gerade bzw. eine Ebene (bei zwei Prädiktoren) beschrieben wird. Demzufolge kann die lineare Beziehung zwischen den vorgesagten Werten und den Werten der unabhängigen Variablen mathematisch folgendermaßen beschreiben werden:</p>
<p><span class="math display">\[y_i = b_0 +b_{1}x_{i1} + ... +b_{m}x_{im} + e_i\]</span></p>
<ul>
<li><span class="math inline">\(b_0\)</span> = y-Achsenabschnitt/ Ordinatenabschnitt/ Konstante/ Interzept:
<ul>
<li>Der Wert von <span class="math inline">\(y\)</span> bei einer Ausprägung von 0 in allen <span class="math inline">\(x\)</span>-Variablen</li>
</ul></li>
<li><span class="math inline">\(b_{1}/ b_m\)</span> = Regressionsgewichte der Prädiktoren:
<ul>
<li>beziffern die Steigung der Regressionsgeraden</li>
<li>Interpretation: die Steigung der Geraden lässt erkennen, um wie viele Einheiten <span class="math inline">\(y\)</span> zunimmt, wenn <span class="math inline">\(x\)</span> um eine Einheit zunimmt</li>
</ul></li>
<li><span class="math inline">\(e_i\)</span> = Regressionsresiduum (kurz: Residuum), Residualwert oder Fehlerwert:
<ul>
<li>die Differenz zwischen einem vorhergesagten (<span class="math inline">\(\hat{y}\)</span>) und beobachteten (<span class="math inline">\(y\)</span>) y-Wert</li>
<li>je größer die Fehlerwerte, umso größer ist die Abweichung eines beobachteten vom vorhergesagten Wert</li>
</ul></li>
</ul>
<div id="einfache-lineare-regression" class="section level3">
<h3>Einfache lineare Regression</h3>
<p>Die einfache lineare Regression hat nur einen Prädiktor. Daher entsteht eine Gerade im Modell. Das Modell folgt der uns bekannten Form:</p>
<p><span class="math inline">\(\hat{y_i} = b_0 +b_{1}x_{i1}\)</span> (Regressiongerade = vorhergesagte Werte)</p>
<div class="figure">
<img src="/post/Reg1.png" style="width:80.0%" alt="" />
<p class="caption">Grafische Darstellung einer einfachen linearen Regression</p>
</div>
</div>
<div id="multiple-regression-mehrere-prädiktoren" class="section level3">
<h3>Multiple Regression (mehrere Prädiktoren)</h3>
<p>Wenn wir nun 2 Prädiktoren haben, wird zwischen diesen und der abhängigen Variable eine Ebene aufgespannt. Die Modellgleichung lautet dabei wie bereits gezeigt:</p>
<p><span class="math inline">\(y_i = b_0 +b_{1}x_{i1} + b_{2}x_{i2} + e_i\)</span></p>
<div class="figure">
<img src="/post/Reg2.png" style="width:80.0%" alt="" />
<p class="caption">Grafische Darstellung einer multiplen Regression</p>
</div>
<p>Eine Erweiterung auf mehr als zwei Prädiktoren ist mathematisch problemlos möglich, aber grafisch nicht mehr schön darstellbar. Deshalb hören wir mit Zeichnungen an dieser Stelle auf.</p>
</div>
</div>
<div id="berechnung-der-regressionsgewichte-b_i-mit-hilfe-der-händischen-formeln-in-r" class="section level2">
<h2>Berechnung der Regressionsgewichte <span class="math inline">\(b_i\)</span> mit Hilfe der händischen Formeln in <code>R</code></h2>
<p>In der Vorlesung haben Sie das Vorgehen zur Bestimmung der Regressionskoeffizienten <span class="math inline">\(b_i\)</span> kennen gelernt. Dies ist mit einem einfachen Taschenrechner, aber natürlich auch mit der Hilfe von <code>R</code> möglich. Diesen Einsatz wollen wir hier demonstieren.</p>
<p>Folgendes Anwendungsbeispiel setzen wir dabei ein: Eine Stichprobe von 100 Schüler:innen hat einen Lese- und einen Mathematiktest sowie zusätzlich einen allgemeinen Intelligenztest bearbeitet. Die Testleistungen sind untereinander alle positiv korreliert. Auch die beiden fachspezifischen Tests für Lesen (<code>reading</code>) und Mathematik (<code>math</code>) korrelieren substanziell.</p>
<p>Oft wird argumentiert, dass zum Lösen von mathematischen Textaufgaben auch Lesekompetenz erforderlich ist (z. B. bei Textaufgaben). Anhand des Datensatzes soll untersucht werden, wie stark sich die Mathematikleistungen durch Lesekompetenz und allgemeine Intelligenz vorhersagen lassen.</p>
<p>Die Formel lautet demnach:</p>
<p><span class="math display">\[y_{i,math} = b_0 +b_{reading}x_{i,reading} + b_{IQ}x_{i,IQ} + e_i\]</span>
oder in Matrixform:</p>
<p><span class="math display">\[\begin{align}
\begin{bmatrix} y_1\\y_2\\y_3\\y_4\\...\\y_{100}\end{bmatrix} = b_{0} *
\begin {bmatrix}1\\1\\1\\1\\...\\1\end{bmatrix} + b_{reading} *
\begin {bmatrix}x_{reading1}\\x_{reading2}\\x_{reading3}\\x_{reading4}\\...\\y_{reading100}\end{bmatrix} + b_{IQ} *
\begin {bmatrix}x_{IQ1}\\x_{IQ2}\\x_{IQ3}\\x_{IQ4}\\...\\x_{IQ100}\end{bmatrix} +
\begin {bmatrix}e_1\\e_2\\e_3\\e_4\\...\\e_{100}\end{bmatrix}
\end{align}\]</span></p>
<p>Die Daten der Schüler:innen können Sie sich direkt ins Environment einladen.</p>
<pre class="r"><code># Datensatz laden
load(url(&quot;https://pandar.netlify.app/post/Schulleistungen.rda&quot;))
names(Schulleistungen)</code></pre>
<pre><code>## [1] &quot;female&quot;  &quot;IQ&quot;      &quot;reading&quot; &quot;math&quot;</code></pre>
<p>Zusätzlich zu den bereits beschriebenen Variablen gibt es hier noch <code>female</code>, wobei eine <code>1</code> für eine weibliche Person steht. Diese Variable werden wir heute allerdings nicht nutzen. Die Variable <code>math</code> gibt die Leistungen der Schüler:innen im Mathematik-Test wieder. Diese sollen unsere abhängige Variable darstellen, weshalb wir sie einem Objekt namens <code>y</code> zuordnen.</p>
<pre class="r"><code># Vektor y
y &lt;- Schulleistungen$math
str(y)</code></pre>
<pre><code>##  num [1:100] 452 590 509 560 659 ...</code></pre>
<p>Durch die Anwendung von <code>str</code> sehen wir, dass es sich bei dem Vektor erwartungsgemäß um einen numerischen handelt.</p>
<p>Als nächstes wollen wir unsere Prädiktoren vorbereiten. Diese werden gemeinsam in der Matrix <code>X</code> erfasst. Hierfür müssen wir die Spalten <code>reading</code> und <code>IQ</code> aus unserem Datensatz <code>Schulleistungen</code> auswählen.</p>
<pre class="r"><code># Matrix X vorbereiten 
X &lt;- as.matrix(Schulleistungen[,c(&quot;reading&quot;, &quot;IQ&quot;)])      </code></pre>
<p>Anschließend wird noch eine Spalte mit Einsen benötigt, die für die Regressionskonstante eintritt. Da die Regressionskonstante für alle Personen denselben Einfluss hat, können die zugehörigen <span class="math inline">\(x_0\)</span> Werte mit 1 beschrieben werden. Wir nennen den Vektor zunächst <code>constant</code>. Erstellt wird er mit der Funktion <code>rep</code>. Diese sorgt dafür, dass 1en (erstes Argument) wiederholt werden - und dabei genau <code>nrow(X)</code>-mal, da die Anzahl der Zeilen von <code>X</code> die Anzahl der Personen beschreibt. Typischerweise steht die Regressionskonstante als erstes, weshalb der 1en-Vektor <code>constant</code> als erstes in <code>cbind</code> eingeht. Wir fügen als zweites die vorher erstellte Matrix <code>X</code> ein und überschreiben diese.</p>
<pre class="r"><code># Matrix X erweitern
constant &lt;- rep(1, nrow(X))
X &lt;- cbind(constant, X)                         
head(X)</code></pre>
<pre><code>##      constant  reading        IQ
## [1,]        1 449.5884  81.77950
## [2,]        1 544.8495 106.75898
## [3,]        1 331.3466  99.14033
## [4,]        1 531.5384 111.91499
## [5,]        1 604.3759 116.12682
## [6,]        1 308.7457 106.14127</code></pre>
<p>Durch die Betrachtung der ersten 6 Zeilen mit <code>head</code> sehen wir, dass unsere Zusammenführung funktioniert hat. In handschriftlicher Notation würden unsere beiden erstellten Vektoren nun folgendermaßen aussehen.</p>
<p><span class="math display">\[\begin{align}y = \begin{bmatrix}451.98 \\589.65 \\509.33\\560.43\\...\\603.18\end{bmatrix}\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}X=\begin{bmatrix}1 &amp; 449.59 &amp; 81.78\\1 &amp; 544.85 &amp; 106.76\\1 &amp; 331.35 &amp; 99.14\\1 &amp; 531.54 &amp; 111.91\\... &amp; ... &amp; ... \\1 &amp; 487.22 &amp; 106.13\end{bmatrix}\end{align}\]</span></p>
<p>Mit <code>X</code> und <code>y</code> in unserem Environment können wir nun in die Berechnung starten.</p>
<div id="vorgehen-bei-der-berechnung-der-regressionsgewichte" class="section level3">
<h3>Vorgehen bei der Berechnung der Regressionsgewichte:</h3>
<p>Die Regressionsgewichte in der multiplen Regression können mit folgender Formel geschätzt werden:</p>
<p><span class="math display">\[\hat{b} = (X&#39;X)^{-1}X&#39;y\]</span></p>
<p>Wir wollen zunächst die Gleichung in einzelne Schritte zerlegen, um die nötigen Notationen in <code>R</code> kennenzulernen.</p>
<ol style="list-style-type: decimal">
<li>Berechnung der Kreuzproduktsumme (X’X)</li>
<li>Berechnung der Inversen der Kreuzproduktsumme (<span class="math inline">\((X&#39;X)^{-1}\)</span>)</li>
<li>Berechnung des Kreuzproduksummenvektors (X’y)</li>
<li>Berechnung des Einflussgewichtsvektor</li>
</ol>
<div id="berechnung-der-kreuzproduktsumme-xx" class="section level4">
<h4>1. Berechnung der Kreuzproduktsumme (X’X)</h4>
<p>Die Kreuzproduktsumme (X’X) wird berechnet, indem die transponierte Matrix X (X’) mit der Matrix X multipliziert wird. Die transponierte Matrix X’ erhalten Sie in R durch die Befehl <code>t(X)</code>.</p>
<pre class="r"><code>t(X) # X&#39; erhalten Sie durch t(X)</code></pre>
<pre><code>##              [,1]     [,2]      [,3]     [,4]     [,5]     [,6]      [,7]
## constant   1.0000   1.0000   1.00000   1.0000   1.0000   1.0000   1.00000
## reading  449.5884 544.8495 331.34664 531.5384 604.3759 308.7457 478.24576
## IQ        81.7795 106.7590  99.14033 111.9150 116.1268 106.1413  85.44854
##               [,8]     [,9]     [,10]     [,11]    [,12]    [,13]     [,14]
## constant   1.00000   1.0000   1.00000   1.00000   1.0000   1.0000   1.00000
## reading  550.18962 822.0051 413.84938 655.53908 605.8732 625.3337 391.33058
## IQ        93.24323 135.1974  89.90152  92.72073 115.9012 114.5409  83.28294
##             [,15]    [,16]     [,17]     [,18]    [,19]    [,20]     [,21]
## constant   1.0000   1.0000   1.00000   1.00000   1.0000   1.0000   1.00000
## reading  709.9150 490.0105 487.01641 444.92014 668.8459 502.2504 495.87525
## IQ       126.4167 107.2044  90.03418  98.34044 117.0687 115.5514  68.11351
##             [,22]     [,23]    [,24]     [,25]     [,26]    [,27]     [,28]
## constant   1.0000   1.00000   1.0000   1.00000   1.00000   1.0000   1.00000
## reading  485.7415 524.04072 461.2220 425.66559 385.71861 521.5388 453.28591
## IQ       125.6478  93.34804 106.9365  98.78466  78.93267 113.0538  92.86905
##              [,29]     [,30]    [,31]     [,32]     [,33]     [,34]     [,35]
## constant   1.00000   1.00000   1.0000   1.00000   1.00000   1.00000   1.00000
## reading  640.76462 265.02068 539.1343 524.99381 263.22535 536.29904 469.38977
## IQ        86.44483  70.17249 111.4461  93.78654  87.54754  87.01957  69.32581
##              [,36]     [,37]     [,38]    [,39]    [,40]     [,41]    [,42]
## constant   1.00000   1.00000   1.00000   1.0000   1.0000   1.00000   1.0000
## reading  275.25090 424.78162 416.07719 559.0645 572.9057 549.99247 524.8635
## IQ        92.85801  70.56712  74.17486 105.6119 110.6390  91.54624 105.7314
##             [,43]    [,44]    [,45]     [,46]     [,47]     [,48]     [,49]
## constant   1.0000   1.0000   1.0000   1.00000   1.00000   1.00000   1.00000
## reading  656.9045 475.6473 430.1947 338.43936 563.40059 543.48860 529.81881
## IQ       125.2621 101.1487 111.0958  79.99545  84.45429  84.50532  96.60821
##             [,50]     [,51]    [,52]    [,53]     [,54]    [,55]    [,56]
## constant   1.0000   1.00000   1.0000   1.0000   1.00000   1.0000   1.0000
## reading  668.8536 478.48482 596.3659 493.1522 333.78586 476.6246 433.0946
## IQ       103.9056  81.03395 126.1281  89.4765  80.78064 106.4885 103.5806
##              [,57]    [,58]     [,59]     [,60]    [,61]    [,62]    [,63]
## constant   1.00000   1.0000   1.00000   1.00000   1.0000   1.0000   1.0000
## reading  395.12879 615.4441 674.95494 551.51318 582.4867 550.8037 505.1084
## IQ        84.88878 115.9093  97.28407  91.60586 121.7788 110.2619 100.3214
##             [,64]    [,65]     [,66]     [,67]    [,68]    [,69]    [,70]
## constant   1.0000   1.0000   1.00000   1.00000   1.0000   1.0000   1.0000
## reading  579.3319 469.0869 463.25972 310.39499 494.8413 566.2383 412.3524
## IQ       112.6516 122.8403  96.45124  75.48471  91.2755 111.8578  92.7289
##              [,71]     [,72]    [,73]     [,74]    [,75]    [,76]    [,77]
## constant   1.00000   1.00000   1.0000   1.00000   1.0000   1.0000   1.0000
## reading  317.15817 454.67678 518.8499 522.48525 575.6985 509.0475 603.9479
## IQ        76.84326  92.93814 103.2558  81.15462  92.2719 106.4095  96.7028
##             [,78]    [,79]     [,80]     [,81]    [,82]    [,83]    [,84]
## constant   1.0000   1.0000   1.00000   1.00000   1.0000   1.0000   1.0000
## reading  391.1594 462.6678 540.62765 487.71683 519.8697 300.6192 512.1456
## IQ       104.0638 107.9850  60.76781  94.55947 103.5597 101.8328 113.0630
##              [,85]     [,86]    [,87]    [,88]    [,89]     [,90]    [,91]
## constant   1.00000   1.00000   1.0000   1.0000   1.0000   1.00000   1.0000
## reading  390.68524 380.51429 559.4885 525.6131 609.9593 409.51193 604.2912
## IQ        76.56824  97.56684 104.2866 106.0855 120.9776  82.65717 108.4118
##             [,92]    [,93]    [,94]     [,95]     [,96]     [,97]    [,98]
## constant   1.0000   1.0000   1.0000   1.00000   1.00000   1.00000   1.0000
## reading  543.1018 425.8297 659.4646 506.42468 454.15287 483.78242 531.9650
## IQ       103.3896 100.5953 122.7979  97.91853  92.96729  77.51862 105.0199
##              [,99]   [,100]
## constant   1.00000   1.0000
## reading  198.10626 487.2215
## IQ        54.05485 106.1264</code></pre>
<p><span class="math display">\[\begin{align}X=\begin{bmatrix}1 &amp; 449.59 &amp; 81.78\\1 &amp; 544.85 &amp; 106.76\\1 &amp; 331.35 &amp; 99.14\\1 &amp; 531.54 &amp; 111.91\\... &amp; ... &amp; ... \\1 &amp; 487.22 &amp; 106.13\end{bmatrix}\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}X&#39;=\begin{bmatrix} 1 &amp; 1 &amp; 1 &amp; 1 &amp; ... &amp; 1\\449.59 &amp; 544.85 &amp; 331.35 &amp; 531.54 &amp; ... &amp; 487.22\\81.78 &amp; 106.76 &amp; 99.14 &amp; 111.91 &amp; ... &amp; 106.13\end{bmatrix}\end{align}\]</span></p>
<p>Wir nennen das Kreuzprodukt an dieser Stelle <code>X.X</code> und nicht <code>X'X</code>, da dies mit der Bedeutung von ’ in der Sprache nicht funktioniert. Für das Erstellen der Kreuzproduktsumme muss das normale Zeichen für die Multiplikation <code>*</code> von zwei <code>%</code>-Zeichen umschlossen werden. An dieser Stelle würde sonst auch eine Fehlermeldung resultieren, aber bei quadratischen Matrizenist der Unterschied von Bedeutung.</p>
<pre class="r"><code># Berechnung der Kreuzproduktsumme X’X in R
X.X &lt;- t(X) %*% X       
X.X</code></pre>
<pre><code>##           constant    reading          IQ
## constant   100.000    49606.6    9813.425
## reading  49606.605 25730126.1 4962448.077
## IQ        9813.425  4962448.1  987595.824</code></pre>
<p>Die zugehörige handschriftliche Notation würde demnach so aussehen:</p>
<p><span class="math display">\[\begin{align}X&#39;X=\begin{bmatrix}100 &amp; 49606.6 &amp; 9813.4  \\49606.6 &amp; 25730126.1  &amp; 4962448.1 \\9813.4   &amp; 4962448.1  &amp;  987595.8 \end{bmatrix}\end{align}\]</span></p>
</div>
<div id="berechnung-der-inversen-der-kreuzproduktsumme-xx-1" class="section level4">
<h4>2. Berechnung der Inversen der Kreuzproduktsumme <span class="math inline">\((X&#39;X)^{-1}\)</span></h4>
<p>Die Inverse der Kreuzproduktsumme kann in <code>R</code> durch den <code>solve()</code> Befehl berechnet werden.</p>
<pre class="r"><code># Berechnung der Inversen (mit Regel nach Sarrus) in R
solve(X.X)</code></pre>
<pre><code>##               constant       reading            IQ
## constant  0.4207610612 -1.568521e-04 -3.392822e-03
## reading  -0.0001568521  1.316437e-06 -5.056210e-06
## IQ       -0.0033928220 -5.056210e-06  6.013228e-05</code></pre>
<p><span class="math display">\[\begin{align}(X&#39;X)^{-1}= \begin{bmatrix}0.42 &amp; -1.57e-04  &amp; -3.39e-03\\-1.57e-04 &amp; 1.32e-06&amp; -5.06e-06\\-3.39e-03 &amp; -5.06e-06 &amp; 6.01e-05\end{bmatrix}\end{align}\]</span></p>
</div>
<div id="berechnung-des-kreuzproduktsummenvektors-xy" class="section level4">
<h4>3. Berechnung des Kreuzproduktsummenvektors (X’y)</h4>
<p>Der Kreuzproduktsummenvektor (X’y) wird durch die Multiplikation der transponierten X Matrix (X’) und des Vektors y berechnet.</p>
<p><span class="math display">\[\begin{align}X&#39;=\begin{bmatrix}1 &amp; 1 &amp; 1 &amp; 1 &amp; ... &amp; 1\\449.59 &amp; 544.85 &amp; 331.35 &amp; 531.54 &amp; ... &amp; 487.22\\81.78 &amp; 106.76 &amp; 99.14 &amp; 111.91 &amp; ... &amp; 106.13\end{bmatrix}\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}y=\begin{bmatrix}451.98\\ 589.65\\ 509.33\\560.43\\...\\603.18\end{bmatrix}\end{align}\]</span></p>
<p>Die Verwendung von <code>%*%</code> zum Bilden des Kreuzprodukts und der Funktion <code>t()</code> zum Transponieren haben wir bereits kennengelernt und können hier problemlos den Code schreiben.</p>
<pre class="r"><code>#Berechnung des Kreuzproduksummenvektors X`y in R
X.y &lt;- t(X) %*% y        
X.y</code></pre>
<pre><code>##                 [,1]
## constant    56146.45
## reading  28313059.77
## IQ        5636931.00</code></pre>
<p><span class="math display">\[\begin{align}X&#39;y=\begin{bmatrix}56146.45\\28313060\\5636931\end{bmatrix}\end{align}\]</span></p>
</div>
<div id="berechnung-des-einflussgewichtsvektors" class="section level4">
<h4>4. Berechnung des Einflussgewichtsvektors</h4>
<p>Die geschätzten Regressionsgewichte nach dem Kriterium der kleinsten Quadrate werden berechnet, indem die Inverse der Kreuzproduktsumme <span class="math inline">\(((X&#39;X)^{-1})\)</span> mit dem Kreuzproduktsummenvektor (X’y) multipliziert wird.</p>
<p><span class="math display">\[\begin{align}(X&#39;X)^{-1}= \begin{bmatrix}0.42 &amp; -1.57e-04  &amp; -3.39e-03\\-1.57e-04 &amp; 1.32e-06&amp; -5.06e-06\\-3.39e-03 &amp; -5.06e-06 &amp; 6.01e-05\end{bmatrix}\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}X&#39;y=\begin{bmatrix}56146.45\\28313060\\5636931\end{bmatrix}\end{align}\]</span></p>
<pre class="r"><code># Berechnung des Einflussgewichtsvektor in R
b_hat &lt;- solve(X.X) %*% X.y     # Vektor der geschätzten Regressionsgewichte
b_hat</code></pre>
<pre><code>##                 [,1]
## constant 58.17167003
## reading  -0.03584686
## IQ        5.30981976</code></pre>
<p><span class="math display">\[\begin{align}\hat{b}=\begin{bmatrix}58.17\\-0.04 \\5.31\end{bmatrix}\end{align}\]</span></p>
</div>
<div id="vorhersage-der-mathematikleistung" class="section level4">
<h4>Vorhersage der Mathematikleistung</h4>
<p>Den Vektor mit den vorhergesagten Werten von y (<span class="math inline">\(\hat{y}\)</span>) können Sie durch die Multiplikation der Matrix <span class="math inline">\(X\)</span> mit den Regressionsgewichten (<span class="math inline">\(\hat{b}\)</span>) berechnen.</p>
<pre class="r"><code>y_hat &lt;- X %*% b_hat # Vorhersagewerte für jede einzelne Person 
head(y_hat)</code></pre>
<pre><code>##          [,1]
## [1,] 476.2897
## [2,] 605.5115
## [3,] 572.7112
## [4,] 633.3661
## [5,] 653.1192
## [6,] 610.6951</code></pre>
<p><span class="math display">\[\begin{align}
\hat{y}_{math} = \begin{bmatrix}476.29\\ 605.51\\572.71\\633.37\\...\\604.22\end{bmatrix}
\end{align}\]</span></p>
</div>
</div>
<div id="berechnung-der-standardisierten-regressionsgewichte" class="section level3">
<h3>Berechnung der standardisierten Regressionsgewichte</h3>
<p>Bisher wurden nur die <em>unstandardisierten Regressionsgewichte</em> berechnet. Diese haben den Vorteil leichter interpretierbar zu sein. So wird das unstandardisierte Regressionsgewicht folgendermaßen interpretiert: wenn sich die unabhängige Variable um eine Einheit verändert, verändert sich die abhängige Variable um den unstandardisierten Koeffizienten. Der Nachteil dieser unstandardisierten Regressionsgewichte ist jedoch, dass die Regressionsgewichte nicht zwischen verschiedenen Prädiktoren vergleichbar sind. Demzufolge kann anhand der Größe der Regressionsgewichte nicht gesagt werden, welcher Regressionskoeffizient, d.h. welcher Prädiktor, eine stärkere Erklärungskraft hat.</p>
<p>Daher werden die Regressionsgewichte häufig standardisiert. Durch die Standardisierung sind die Regressionsgewichte nicht mehr von der ursprünglichen Skala abhängig und haben daher den Vorteil, dass sie miteinander verglichen werden können. Allerdings sind die <em>standardisierten Regressionsgewichte</em> nicht mehr so leicht zu interpretieren. Die Interpretation der standardisierten Regressionsgewichte lautet: wenn sich die unabhängige Variable um eine Standardabweichung erhöht (und unter Kontrolle weiterer unabhängiger Variablen), so beträgt die erwartete Veränderung in der abhängigen Variable <span class="math inline">\(\beta\)</span> Standardabweichungen (das standardisierte Interzept ist Null).</p>
<p>Die standardisierten Regressionsgewichte können bestimmt werden, indem zunächst alle Variablen standardisiert werden. Dafür haben wir bereits im letzten Semester den Befehl <code>scale</code> kennen gelernt. Wir wenden ihn auf die Werte der abhängigen Variable <code>y</code> und die der Prädiktoren <code>X</code> an.</p>
<pre class="r"><code>#Berechnung der standardisierten Regressionsgewichte
y_s &lt;- scale(y) # Standardisierung y
X_s &lt;- scale(X) # Standardisierung X
head(X_s)</code></pre>
<pre><code>##      constant    reading          IQ
## [1,]      NaN -0.4365869 -1.03830315
## [2,]      NaN  0.4582463  0.54755176
## [3,]      NaN -1.5472873  0.06387225
## [4,]      NaN  0.3332090  0.87488817
## [5,]      NaN  1.0174054  1.14228148
## [6,]      NaN -1.7595886  0.50833564</code></pre>
<p>Bei der Ansicht des standardisierten Objektes <code>X_s</code> sehen wir, dass alle Werte in der ersten Spalte nun <code>NaN</code> sind. Dies liegt daran, dass in dieser Spalte ja keine Streuung vorhanden ist und die Standardisierung somit nicht funktioniert. Für die Berechnung der Regressionsgewichte im standardisierten Fall sollten auch hier wieder 1en stehen, weshalb wir alle Werte in der Spalte mit 1 ersetzen müssen.</p>
<pre class="r"><code>X_s[,1] &lt;- 1    # Einsenvektor wieder auffüllen</code></pre>
<p>Die bereits im unstandardisierten Fall zur Bestimmung der Regressionsgewichte könnten nun wieder einzeln durchgeführt werden. Allerdings kann man diese natürlich auch in einer Zeile Code durchführen, was hier demonstriert wird.</p>
<pre class="r"><code>b_hat_s &lt;- solve(t(X_s)%*% X_s) %*% t(X_s)%*%y_s #Regressionsgewichte aus den standardisierten Variablen
round(b_hat_s, 3)</code></pre>
<pre><code>##            [,1]
## constant  0.000
## reading  -0.033
## IQ        0.716</code></pre>
<p>Wir sehen im Ergebnis, dass die Regressionskonstante 0 ist. Die lineare Regression geht stets durch den Mittelwert aller Variablen. Durch die Standardisierung ist dieser für jede Variable 0, wodurch der Punkt (0/0/0) durchlaufen werden muss. Weiterhin erkennen wir, dass der Einfluss des IQs der Lesefähigkeit deskriptiv überlegen ist.</p>
</div>
<div id="berechnung-des-globalen-signifikanztest" class="section level3">
<h3>Berechnung des globalen Signifikanztest</h3>
<p>Deskriptiv haben wir nun ein Modell aufgestellt. Wie auch im letzten Semester stellt sich aber die Frage, ob wir dieses auch auf die Population übertragen können. Der Test des gesamten Modells wird auch als Omnibus-Test bezeichnet. Wir wollen im ersten Schritt betrachten, wie viel Varianz unser Modell erklären kann und es im Anschluss auf Signifikanz testen.</p>
<div id="determinationskoeffizient-r2" class="section level4">
<h4>Determinationskoeffizient <span class="math inline">\(R^2\)</span></h4>
<p>Der Determinationskoeffizient <span class="math inline">\(R^2\)</span> gibt an, wieviel Varianz in der abhängigen Variable durch die unabhängigen Variablen erklärt werden kann. Dafür müssen die Quadratsummen bestimmt werden, die die Streuung der Variablen repräsentieren. Einfach gesagt, hat die abhängige Variable eine totale Quadratsumme, die in einen durch die Regression erklärten und einen nicht erklärten Teil geteilt werden kann.</p>
<p><span class="math display">\[Q_{t} = Q_d + Q_e\]</span></p>
<p>Die totale Quadratsumme wird bestimmt, indem die Abweichungen aller Fälle vom Mittelwert der Variablen bestimmt werden.</p>
<p><span class="math display">\[ Q_{t} =  \sum^n_{i=1}(y_i - \bar{y})^2\]</span></p>
<pre class="r"><code>Q_t &lt;- sum((y - mean(y))^2)          # Totale Quadratsumme
Q_t</code></pre>
<pre><code>## [1] 1350290</code></pre>
<p>Der erklärte Teil wird auch als Regressionsquadratsumme bezeichnet und mit <span class="math inline">\(Q_d\)</span> repräsentiert. Erklärt werden kann der Teil der Abweichungen vom Mittelwert, der auf der von uns berechneten Regressionsgrade liegt - also die vorhergesagten Werte für die Personen <span class="math inline">\(\hat{y}_i\)</span>.</p>
<p><span class="math display">\[ Q_{d} =  \sum^n_{i=1}(\hat{y}_i - \bar{y})^2\]</span></p>
<pre class="r"><code>Q_d &lt;- sum((y_hat - mean(y))^2)    # Regressionsquadratsumme
Q_d</code></pre>
<pre><code>## [1] 658051.7</code></pre>
<p>Der nicht erklärte Anteil wird als Fehlerquadratsumme <span class="math inline">\(Q_e\)</span> bezeichnet. Hierbei wird die Abweichung unserer vorhergesagten Werte <span class="math inline">\(\hat{y}_{i}\)</span> vom wahren Wert der Personen <span class="math inline">\(y\)</span> bestimmt.</p>
<p><span class="math display">\[ Q_{d} =  \sum^n_{i=1}(y_{i} - \hat{y}_i)^2\]</span></p>
<pre class="r"><code>Q_e &lt;- sum((y - y_hat)^2)          # Fehlerquadratsumme
Q_e</code></pre>
<pre><code>## [1] 692238.6</code></pre>
<p>Die Summe aus aus der Regressionsquadratsumme und der Fehlerquadratsumme ergibt die totale Quadratsumme. Wir können also überprüfen, ob unsere bisherigen Berechnungen plausibel sind. Aufgrund der großen Zahlen kann es zu kleinen Ungenauigkeiten kommen, weshalb wir hier die Nachkommastellen durch Rundung auf zwei reduzieren.</p>
<pre class="r"><code>round(Q_t,2) == round(Q_d + Q_e, 2)</code></pre>
<pre><code>## [1] TRUE</code></pre>
<p>Spannender ist jedoch, dass wir durch den Quotient aus der Regressionsquadratsumme und der totalen Quadratsumme den Anteil der erklärten Varianz simpel bestimmen können.</p>
<p><span class="math display">\[R^2= \dfrac{Q_d}{Q_d + Q_e}\]</span></p>
<pre class="r"><code>R2 &lt;- Q_d / (Q_d + Q_e)            # Determinationskoeffizient R^2
# Alternativ Q_d / Q_t</code></pre>
<p><span class="math inline">\(R^2= \dfrac{Q_d}{Q_d + Q_e} = \dfrac{6.5805169\times 10^{5}}{6.5805169\times 10^{5} + 6.9223863\times 10^{5}} = 0.49\)</span></p>
<p>0.49 der Varianz in der Mathematikleistung können demnach durch unser Modell erklärt werden.</p>
</div>
<div id="f-wert" class="section level4">
<h4>F-Wert</h4>
<p>Der F-Wert dient zur Überprüfung der Gesamtsignifikanz des Modells. Er sagt aus, ob der Determinationskoeffizient <span class="math inline">\(R^2\)</span> signifikant von 0 verschieden ist. Der empirische Testwert wird über die folgende Formel bestimmt.</p>
<p><span class="math inline">\(F_{omn} = \dfrac{\dfrac{R^2}{m}}{\dfrac{1-R^2}{n-m-1}}\)</span></p>
<p><span class="math inline">\(R^2\)</span> haben wir bereits im vorherigen Abschnitt bestimmt. Ergänzend müssen wir also noch <span class="math inline">\(n\)</span> und <span class="math inline">\(m\)</span> bestimmen. <span class="math inline">\(n\)</span> ist die Anzahl an Peronen oder Fällen in unserem Datensatz. Diese können wir beispielweise über die Länge <code>length</code> des Objekts mit den abhängigen Varibalen <code>y</code> bestimmen. <span class="math inline">\(m\)</span> beschreibt die Anzahl der Prädiktoren. Wenn wir dies von <code>R</code> automatisch bestimmen lassen wollen, können wir die Anzahl der Spalten <code>ncol</code> unserer Prädiktorenmatrix <code>X</code> bestimmen. Allerdings müssen wir bedenken, dass wir noch eine Spalten mit 1en hinzugefügt haben. Diese muss von der Zahl der Spalten also noch abgezogen werden.</p>
<pre class="r"><code>n &lt;- length(y)                     # Fallzahl (n=100)
m &lt;- ncol(X)-1                     # Zahl der Prädiktoren (m=2)
F_omn &lt;- (R2/m) / ((1-R2)/(n-m-1)) # empirischer F-Wert
F_omn</code></pre>
<pre><code>## [1] 46.10478</code></pre>
<p><span class="math inline">\(F_{omn} = \dfrac{\dfrac{R^2}{m}}{\dfrac{1-R^2}{n-m-1}} = \dfrac{\dfrac{0.49}{2}}{\dfrac{1-0.49}{100-2-1}} = 46.1\)</span></p>
<p>Der berechnete Wert entspricht einem empirischen Wert, der mit einem kritischen Wert verglichen werden muss, um die Signifikanz zu bewerten. Hierfür müssen wir in die F-Verteilung schauen. Die zugehörigen Freiheitsgrade können aus den bereits berechneten <span class="math inline">\(m\)</span> und <span class="math inline">\(n\)</span> bestimmt werden.</p>
<p><span class="math inline">\(df_1 = 2, df_1 = n-m-1 = 100-2-1 =97\)</span></p>
<p>Zusätzlich benötigen wir natürlich noch einen <span class="math inline">\(\alpha\)</span>-Fehler, der auch hier häufig auf 5% gesetzt wird. Im letzten Semester haben wir gelernt, dass wir Quantile aus Verteilungen mit dem Präfix <code>q</code> und dann dem Familiennamen (bspw. <code>qnorm</code> oder <code>qt</code>) ausgeben können. Wenig überraschend funktioniert an dieser Stelle also der Befehl <code>qf</code>.</p>
<pre class="r"><code>F_krit &lt;- qf(.95, df1=m, df2=n-m-1)  # kritischer F-Wert (alpha=5%)</code></pre>
<p><span class="math inline">\(F_{krit}(\alpha=.05, df_1=2, df_2= 97)= 3.09\)</span></p>
<p>Für eine Signifikanzentscheidung müssen die F-Werte nun miteinander verglichen werden.</p>
<pre class="r"><code>F_krit &lt; F_omn  # Vergleich durch logische Überprüfung</code></pre>
<pre><code>## [1] TRUE</code></pre>
<p>Da der kritische Wert kleiner ist, verwerfen wir die Nullhypothese. Das Gesamtmodell wird als signifikant angesehen. Man kann anstatt des kritischen Wertes auch einfach den <span class="math inline">\(p\)</span>-Wert bestimmen, der zu unserem empirischen <code>F_omn</code> korrespondiert. Für die Bestimmung von Wahrscheinlichkeiten in Verteilungen haben wir den Präfix <code>p</code> kennen gelernt. Mit Angabe der zugehörigen Freiheitsgrade können wir den <span class="math inline">\(p\)</span>-Wert erhalten und mit unserem <span class="math inline">\(\alpha\)</span> vergleichen.</p>
<pre class="r"><code>p &lt;- 1-pf(F_omn, m, n-m-1)           # p-Wert
p &lt; 0.05</code></pre>
<pre><code>## [1] TRUE</code></pre>
<p><span class="math inline">\(p=0.00000000000000844\)</span></p>
<p>Das Ergebnis ist hier natürlich das gleiche - die Nullhypothese wird verworfen.</p>
</div>
</div>
</div>
<div id="berechnung-der-regression-mit-lm-funktionen-in-r" class="section level2">
<h2>Berechnung der Regression mit <code>lm</code>-Funktionen in <code>R</code></h2>
<p>Im ersten Semester haben wir bereits die einfache lineare Regression in <code>R</code> durchgeführt. Natürlich gibt es auch für die multiple lineare Regression eine Funktionalität. Wir werden uns zunächst den unstandardisierten und dann den standardisierten Fall anschauen und auch nochmal die Interpretation klar machen.</p>
<div id="unstandardisierte-regression" class="section level3">
<h3>Unstandardisierte Regression</h3>
<p>Für die Schätzung von Regressionsmodellen kann, wie im Fall mit einem Prädiktor, die Basis-Funktion <code>lm</code> verwendet werden. Verschiedene Prädiktoren werden alle hinter der Tilde <code>~</code> aufgeführt und mit dem <code>+</code>-Zeichen getrennt. Wir wollen hier weiterhin in unserem Datensatz <code>Schulleistungen</code> die Mathematikleistung (<code>math</code>) durch die Leseleistung (<code>reading</code>) und den IQ (<code>IQ</code>) vorhersagen. Die beiden Prädiktoren werden nach der vorangegangenen Beschreibung also mit <code>+</code> getrennt.</p>
<pre class="r"><code># Regressionsanalyse mit lm
reg &lt;- lm(math ~ reading + IQ, data = Schulleistungen)</code></pre>
<p>Nun wollen wir überprüfen, ob die Ergebnisse den händisch berechneten entsprechen. Eine gute Übersicht über das Modell erhält man mit der Funktion <code>summary</code>.</p>
<pre class="r"><code>summary(reg)</code></pre>
<pre><code>## 
## Call:
## lm(formula = math ~ reading + IQ, data = Schulleistungen)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -86.60 -48.89 -19.10  23.59 467.25 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 58.17167   54.79738   1.062    0.291    
## reading     -0.03585    0.09693  -0.370    0.712    
## IQ           5.30982    0.65508   8.106  1.6e-12 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 84.48 on 97 degrees of freedom
## Multiple R-squared:  0.4873, Adjusted R-squared:  0.4768 
## F-statistic:  46.1 on 2 and 97 DF,  p-value: 8.447e-15</code></pre>
<p>Die Übereinstimmung mit den händisch berechneten Regressionsgewichten wird bereits deutlich. Im nächsten Abschnitt befassen wir uns nochmal genauer mit der Interpretation.</p>
<div id="ergebnisinterpretation-unstandardisierte-regression" class="section level4">
<h4>Ergebnisinterpretation unstandardisierte Regression</h4>
<ul>
<li>die Lesekompetenz und allgemeine Intelligenz erklären gemeinsam 48.73% der Varianz in der Mathematiktestleistung</li>
<li>Dieser Varianzanteil ist signifikant von null verschieden</li>
<li>Regressionsgewichte:
<ul>
<li>Regressionskonstante <span class="math inline">\(b_0\)</span>:
<ul>
<li>Der Erwartungswert der Mathematikleistung für ein Individuum mit null Punkten im IQ und null Punkten in Lesekompetenz beträgt 58.17 Punkte.</li>
</ul></li>
<li>Regressionsgewicht <span class="math inline">\(b_1\)</span>:
<ul>
<li>bei einem Punkt mehr in der Lesekompetenz und unter Kontrolle (Konstanthaltung) des IQ beträgt die erwartete Veränderung in der Mathematikleistung -0.04 Punkte.</li>
<li>Der Einfluss von Lesekompetenz auf Mathematikleistung ist nicht signfikant von null verschieden (<span class="math inline">\(p\)</span>=0.712)</li>
</ul></li>
<li>Regressionsgewicht <span class="math inline">\(b_2\)</span>:
<ul>
<li>unter Kontrolle (Konstanthaltung) der Lesekompetenz beträgt die erwartete Veränderung in der Mathematikleistung bei einem Punkt mehr im IQ 5.31 Punkte.</li>
<li>Der Einfluss der allgemeinen Intelligenz auf Mathematikleistung ist signfikant von null verschieden (<span class="math inline">\(p\)</span>=1.6001989^{-12})</li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
<div id="standardisierte-regression" class="section level3">
<h3>Standardisierte Regression</h3>
<p>Zur Durchführung der standardisierten Regression könnten wir natürlich händisch alle Variablen mit <code>scale</code> standardisieren und dann in <code>lm</code> übergeben. Es gibt aber auch eine bereits geschriebene Funktion <code>lm.beta</code> aus dem gleichnamigen Paket <code>lm.beta</code>, die für uns eine zusätzliche Anzeige der standardisierten Regressionsgewichte bereit hält.. Das Paket haben wir im normalen Ablauf der Tutorials noch nicht insatlliert, weshalb wir hier nochmal darauf verweisen.</p>
<pre class="r"><code>install.packages(&quot;lm.beta&quot;) #Paket installieren (wenn nötig)</code></pre>
<p>Nach der (ggf. nötigen) Installation müssen wir das Paket für die Bearbeitung laden.</p>
<pre class="r"><code>library(lm.beta)</code></pre>
<pre><code>## Warning: Paket &#39;lm.beta&#39; wurde unter R Version 4.1.1 erstellt</code></pre>
<p>Die Funktion <code>lm.beta</code> muss auf ein Ergebnis der normalen <code>lm</code>-Funktion angewendet werden. Wir haben dieses Ergebnis im Objekt <code>reg</code> hinterlegt. Anschließend wollen wir uns für die Interpreation wieder das <code>summary</code> ausgeben lassen. Natürlich kann man diese Schritte auch mit der Pipe lösen, was als Kommentar noch aufgeführt ist.</p>
<pre class="r"><code># Ergebnisausgabe einschließlich standardisierter Koeffizienten mit lm.beta
reg_s &lt;- lm.beta(reg)
summary(reg_s)         # reg |&gt; lm.beta() |&gt; summary()</code></pre>
<pre><code>## 
## Call:
## lm(formula = math ~ reading + IQ, data = Schulleistungen)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -86.60 -48.89 -19.10  23.59 467.25 
## 
## Coefficients:
##             Estimate Standardized Std. Error t value Pr(&gt;|t|)    
## (Intercept) 58.17167      0.00000   54.79738   1.062    0.291    
## reading     -0.03585     -0.03268    0.09693  -0.370    0.712    
## IQ           5.30982      0.71615    0.65508   8.106  1.6e-12 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 84.48 on 97 degrees of freedom
## Multiple R-squared:  0.4873, Adjusted R-squared:  0.4768 
## F-statistic:  46.1 on 2 and 97 DF,  p-value: 8.447e-15</code></pre>
<p>Wir sehen, dass die ursprüngliche Ausgabe um die Spalte <code>standardized</code> erweitert wurde. Ansonsten ändert sich an der Ausgabe nichts, weshalb wir uns im folgenden Abschnitt der Interpreation auf diese Spalte konzentrieren.</p>
<div id="ergebnisinterpretation-standardisierte-regression" class="section level4">
<h4>Ergebnisinterpretation standardisierte Regression</h4>
<ul>
<li>Standardisierte Regressionsgewichte
<ul>
<li>Standardisiertes Regressionsgewicht <span class="math inline">\(\beta_0\)</span>: Der Achsenabschnitt ist null, da die Ebene durch den Punkt der Mittelwerte aller Variablen geht (0/0/0).</li>
<li>Standardisiertes Regressionsgewicht <span class="math inline">\(\beta_1\)</span>: Unter Kontrolle (Konstanthaltung) des IQ beträgt die erwartete Veränderung in der Mathematikleistung bei einer Standardabweichung mehr in Lesekompetenz -0.03 Standardabweichungen.</li>
<li>Standardisiertes Regressionsgewicht <span class="math inline">\(\beta_2\)</span>: Unter Kontrolle (Konstanthaltung) der Lesekompetenz beträgt die erwartete Veränderung in der Mathematikleistung bei einer Standardabweichung mehr im IQ 0.72 Standardabweichungen.</li>
</ul></li>
</ul>
<p>Verweis zu letzter Sitzung: In solch einer multiplen Regression können Suppressoreffekte gut aufgedeckt werden. Diese zeigen sich dann, wenn die <span class="math inline">\(\beta\)</span>-Gewichte in der multiplen Regression dem Betrag nach größer sind, als deren korrespondierende <span class="math inline">\(\beta\)</span>-Gewichte in einer einfachen Regression. Dies ist in unserem Beispiel jedoch nicht der Fall.</p>
<hr />
</div>
</div>
</div>
<div id="r-skript" class="section level2">
<h2>R-Skript</h2>
<p>Den gesamten <code>R</code>-Code, der in dieser Sitzung genutzt wird, können Sie <a href="/post/PsyBSc7_R_Files/Regression-I.R"><svg aria-hidden="true" role="img" viewBox="0 0 512 512" style="height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;"><path d="M216 0h80c13.3 0 24 10.7 24 24v168h87.7c17.8 0 26.7 21.5 14.1 34.1L269.7 378.3c-7.5 7.5-19.8 7.5-27.3 0L90.1 226.1c-12.6-12.6-3.7-34.1 14.1-34.1H192V24c0-13.3 10.7-24 24-24zm296 376v112c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V376c0-13.3 10.7-24 24-24h146.7l49 49c20.1 20.1 52.5 20.1 72.6 0l49-49H488c13.3 0 24 10.7 24 24zm-124 88c0-11-9-20-20-20s-20 9-20 20 9 20 20 20 20-9 20-20zm64 0c0-11-9-20-20-20s-20 9-20 20 9 20 20 20 20-9 20-20z"/></svg> hier herunterladen</a>.</p>
<hr />
</div>
