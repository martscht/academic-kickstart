---
title: "Regressionsanalyse I"
date: '2021-04-22'
slug: reg1
categories:
     - BSc7
tags:
- Regression
- Zusammenhangsanalyse
- Gerichtete Zusammenhänge
subtitle: ''
summary: ''
authors: [schroeder, nehler, gruetzmacher]
lastmod: '2021-05-03 12:00:12 CEST'
featured: no
header:
     image: "/header/PsyBSc7_Reg1.jpg"
     caption: "[Courtesy of pexels](https://www.pexels.com/photo/man-looking-in-binoculars-during-sunset-802412/)"
projects: []
---


## Einleitung

In der [letzten Sitzung](/post/partial) haben wir unter anderem Korrelationen zwischen zwei Variablen behandelt. Zur Wiederholung: Mithilfe einer Korrelation kann die Stärke des Zusammenhangs zwischen zwei Variablen quantifiziert werden. Dabei haben beide Variablen den gleichen Stellenwert, d.h. eigentlich ist es egal welche Variable die x- und welche Variable die y-Variable ist. Wir haben außerdem Methoden kennengelernt, mit denen der Einfluss einer (oder mehrerer) Drittvariablen kontrolliert werden kann; die Partial- und Semipartialkorrelation. In der heutigen Sitzung wollen wir uns hingegen mit gerichteten Zusammenhängen, d.h. mit Regressionen, beschäftigen.

## Lineare Regression

Das Ziel einer Regression besteht darin, eine Variable durch eine oder mehrere andere Variablen vorherzusagen (Prognose). Die vorhergesagte Variable wird als Kriterium, Regressand oder auch abhängige Variable (AV) bezeichnet und üblicherweise mit $y$ symbolisiert. Die Variablen zur Vorhersage der abhängigen Variablen werden als Prädiktoren, Regressoren oder unabhängige Variablen (UV) bezeichnet und üblicherweise mit $x$ symbolisiert. Im ersten Semester hatten wir stets nur einen Prädiktor - dies kann jedoch jetzt erweitert werden. Deshalb bekommen die Prädiktoren einen Indize $x_1$, $x_2$ usw. 
Die häufigste Form der Regressionsanalyse ist die lineare Regression, bei der der Zusammenhang über eine Gerade bzw. eine Ebene (bei zwei Prädiktoren) beschrieben wird. Demzufolge kann die lineare Beziehung zwischen den vorgesagten Werten und den Werten der unabhängigen Variablen mathematisch folgendermaßen beschreiben werden:

$$y_i = b_0 +b_{1}x_{i1} + ... +b_{m}x_{im} + e_i$$

* $b_0$ = y-Achsenabschnitt/ Ordinatenabschnitt/ Konstante/ Interzept:
    + Der Wert von $y$ bei einer Ausprägung von 0 in allen $x$-Variablen
    
* $b_{1}/ b_m$ = Regressionsgewichte der Prädiktoren:
    + beziffern die Steigung der Regressionsgeraden
    + Interpretation: die Steigung der Geraden lässt erkennen, um wie viele Einheiten $y$ zunimmt, wenn $x$ um eine Einheit zunimmt
* $e_i$ = Regressionsresiduum (kurz: Residuum), Residualwert oder Fehlerwert:
    + die Differenz zwischen einem vorhergesagten ($\hat{y}$) und beobachteten ($y$) y-Wert 
    + je größer die Fehlerwerte, umso größer ist die Abweichung eines beobachteten vom vorhergesagten Wert
    
### Einfache lineare Regression

Die einfache lineare Regression hat nur einen Prädiktor. Daher entsteht eine Gerade im Modell. Das Modell folgt der uns bekannten Form:

$\hat{y_i} = b_0 +b_{1}x_{i1}$ (Regressiongerade = vorhergesagte Werte)


![Grafische Darstellung einer einfachen linearen Regression](/post/Reg1.png){width="80%"}

### Multiple Regression (mehrere Prädiktoren)

Wenn wir nun 2 Prädiktoren haben, wird zwischen diesen und der abhängigen Variable eine Ebene aufgespannt. Die Modellgleichung lautet dabei wie bereits gezeigt:

$y_i = b_0 +b_{1}x_{i1} + b_{2}x_{i2} + e_i$


![Grafische Darstellung einer multiplen Regression](/post/Reg2.png){width="80%"}

Eine Erweiterung auf mehr als zwei Prädiktoren ist mathematisch problemlos möglich, aber grafisch nicht mehr schön darstellbar. Deshalb hören wir mit Zeichnungen an dieser Stelle auf.


## Berechnung der Regressionsgewichte $b_i$ mit Hilfe der händischen Formeln in `R`

In der Vorlesung haben Sie das Vorgehen zur Bestimmung der Regressionskoeffizienten $b_i$ kennen gelernt. Dies ist mit einem einfachen Taschenrechner, aber natürlich auch mit der Hilfe von `R` möglich. Diesen Einsatz wollen wir hier demonstieren.

Folgendes Anwednungsbeispiel setzen wir dabei ein: Eine Stichprobe von 100 Schüler:innen hat einen Lese- und einen Mathematiktest sowie zusätzlich einen allgemeinen Intelligenztest bearbeitet. Die Testleistungen sind untereinander alle positiv korreliert. Auch die beiden fachspezifischen Tests für Lesen (`reading`) und Mathematik (`math`) korrelieren substanziell.

Oft wird argumentiert, dass zum Lösen von mathematischen Textaufgaben auch Lesekompetenz erforderlich ist (z. B. bei Textaufgaben). Anhand des Datensatzes soll untersucht werden, wie stark sich die Mathematikleistungen durch Lesekompetenz und allgemeine Intelligenz vorhersagen lassen.

Die Formel lautet demnach:

$$y_{i,math} = b_0 +b_{reading}x_{i,reading} + b_{IQ}x_{i,IQ} + e_i$$
oder in Matrixform:

$$\begin{align}
\begin{bmatrix} y_1\\y_2\\y_3\\y_4\\...\\y_{100}\end{bmatrix} = b_{0} *
\begin {bmatrix}1\\1\\1\\1\\...\\1\end{bmatrix} + b_{reading} *
\begin {bmatrix}x_{reading1}\\x_{reading2}\\x_{reading3}\\x_{reading4}\\...\\y_{reading100}\end{bmatrix} + b_{IQ} *
\begin {bmatrix}x_{IQ1}\\x_{IQ2}\\x_{IQ3}\\x_{IQ4}\\...\\x_{IQ100}\end{bmatrix} +
\begin {bmatrix}e_1\\e_2\\e_3\\e_4\\...\\e_{100}\end{bmatrix}
\end{align}$$

Die Daten der Schüler:innen können Sie sich direkt ins Environment einladen.

```{r}
# Datensatz laden
load(url("https://pandar.netlify.app/post/Schulleistungen.rda"))
names(Schulleistungen)
```

Die Variable `math` gibt die Leistungen der Schüler:innen im Mathematik-Test wieder. Diese sollen unsere abhängige Variable darstellen, weshalb wir sie einem Objekt namens `y` zuordnen.


```{r}
# Vektor y
y <- Schulleistungen$math
str(y)
```
Durch die Anwendung von `str` sehen wir, dass es sich bei dem Vektor erwartungsgemäß um einen numerischen handelt.

Als nächstes wollen wir unsere Prädiktoren vorbereiten. Diese werden gemeinsam in der Matrix `X` erfasst. Hierfür müssen wir die Spalten `reading` und `IQ` aus unserem Datensatz `Schulleistungen` auswählen.

```{r}
# Matrix X vorbereiten 
X <- as.matrix(Schulleistungen[,c("reading", "IQ")])      
```

Anschließend wird noch eine Spalte mit Einsen benötigt, die für die Regressionskonstante eintritt. Da die Regressionskonstante für alle Personen den selben Einfluss hat, können die zugehörigen $x_0$ Werte mit 1 beschrieben werden. Wir nennen den Vektor zunächst `constant`. Erstellt wird er mit der Funktion `rep`. Diese sorgt dafür, dass 1en (erstes Argument) wiederholt werden - und dabei genau `nrow(X)` Mal, da die Anzahl der Zeilen von `X` die Anzahl der Personen beschreibt. Typischerweise steht die Regressionskonstante als erstes, weshalb der 1en Vektor `constant` als erstes in `cbind` eingeht. Wir fügen als zweites die vorher erstellte Matrix `X` ein und überschreiben diese.

```{r}
# Matrix X erweitern
constant <- rep(1,nrow(X))
X <- cbind(constant, X)                         
head(X)
```

Durch die Betrachtung der ersten 6 Zeilen mit `head` sehen wir, dass unsere Zusammenführung funktioniert hat. In handschriftlicher Notation würden unsere beiden erstellten Vektoren nun folgendermaßen aussehen. 

\begin{align}y = \begin{bmatrix}451,98\\589,65\\509,33\\560,43\\...\\603,18\end{bmatrix}\end{align}

\begin{align}X = \begin{bmatrix}1 & 449,59 & 81,78\\1 & 544,85 & 106,76\\1 & 331,35 & 99,14\\1 & 531,54 & 111,91\\ ... & ... & ... \\1 & 487,22 & 106,13\end{bmatrix}\end{align}

Mit `X` und `y` in unserem Environment können wir nun in die Berechnung starten.

### Vorgehen bei der Berechnung der Regressionsgewichte:

Die Regressionsgewichte in der multiplen Regression können mit folgender Formel geschätzt werden:

$$\hat{b} = (X'X)^{-1}X'y$$

Wir wollen zunächst die Gleichung in einzelne Schritte zerlegen, um die nötigen Notationen in `R` kennenzulernen.

1. Berechnung der Kreuzproduktsumme (X’X)
2. Berechnung der Inversen der Kreuzproduktsumme ($(X'X)^{-1}$)
3. Berechnung des Kreuzproduksummenvektors (X'y)
4. Berechnung des Einflussgewichtsvektor

#### 1. Berechnung der Kreuzproduktsumme (X’X)

Die Kreuzproduktsumme (X'X) wird berechnet, indem die transponierte Matrix X (X') mit der Matrix X multipliziert wird. Die transponierte Matrix X' erhalten Sie in R durch die Befehl `t(X)`.

```{r}
t(X) # X' erhalten Sie durch t(X)
```

\begin{align}X=\begin{bmatrix}1 & 449,59 & 81,78\\1 & 544,85 & 106,76\\1 & 331,35 & 99,14\\1 & 531,54 & 111,91\\... & ... & ... \\1 & 487,22 & 106,13\end{bmatrix}\end{align}


\begin{align}X'=\begin{bmatrix} 1 & 1 & 1 & 1 & ... & 1\\449,58 & 544,85 & 331,35 & 531,54 & ... & 487,22\\81,78 & 106,76 & 99,14 & 111,91 & ... & 106,13\end{bmatrix}\end{align}

Wir nennen das Kreuzprodukt an dieser Stelle `X.X` und nicht `X'X`, da dies mit der Bedeutung von ' in der Sprache nicht funktioniert. Für das Erstellen der Kreuzproduktsumme muss das normale Zeichen für die Multiplikation `*` von zwei `%`-Zeichen umschlossen werden. An dieser Stelle würde sonst auch eine Fehlermeldung resultieren, aber bei quadratischen Matrizenist der Unterschied von Bedeutung.

```{r}
# Berechnung der der Kreuzproduktsumme X’X in R
X.X <- t(X) %*% X       
X.X
```

Die zugehörige handschriftliche Notation würde demnach so aussehen:

\begin{align}X'X=\begin{bmatrix}100,00 & 49606,60 & 9813,43\\49606,61 & 25730126,10 & 4962448,08\\9813,43 & 4962448,10 & 987595,82\end{bmatrix}\end{align}

#### 2. Berechnung der Inversen der Kreuzproduktsumme $(X'X)^{-1}$

Die Inverse der Kreuzproduktsumme kann in R durch den `solve()` Befehl berechnet werden.

```{r}
# Berechnung der Inversen (mit Regel nach Sarrus) in R
solve(X.X)
```

\begin{align}(X'X)^{-1}= \begin{bmatrix}0,42 & -1,56e^{-04} & -3,39^{-03}\\-0,00 & 1,32e^{-06} & -5,06e^{-06}\\-0,00 & -5,06e^{-06} & 6,01e^{-05}\end{bmatrix}\end{align}

#### 3. Berechnung des Kreuzproduktsummenvektors (X'y)

Der Kreuzproduktsummenvektor (X'y) wird durch die Multiplikation der transponierten X Matrix (X') und des Vektors y berechnet.  


\begin{align}X'=\begin{bmatrix}1 & 1 & 1 & 1 & ... & 1\\449,58 & 544,85 & 331,35 & 531,54 & ... & 487,22\\81,78 & 106,76 & 99,14 & 111,91 & ... & 106,13\end{bmatrix}\end{align}

\begin{align}y=\begin{bmatrix}451,98\\589,65\\509,33\\560,43\\...\\603,18\end{bmatrix}\end{align}

Die Verwendung von `%*%` zum Bilden des Kreuzprodukts und der Funktion `t()` zum Transponieren haben wir bereits kennen gelernt und können hier problemlos den Code schreiben.

```{r}

#Berechnung des Kreuzproduksummenvektors X`y in R
X.y <- t(X) %*% y        
X.y

```

\begin{align}X'y=\begin{bmatrix}56146,45\\28313059,77\\5636931,00\end{bmatrix}\end{align}


#### 4. Berechnung des Einflussgewichtsvektors

Die geschätzten Regressionsgewichte nach dem Kriterium der kleinsten Quadrate werden berechnet, indem die Inverse der Kreuzproduktsumme $((X'X)^{-1})$ mit dem Kreuzproduktsummenvektor (X'y) multipliziert wird.   

\begin{align}(X'X)^{-1}= \begin{bmatrix}0,42 & -1,56e^{-04} & -3,39^{-03}\\-0,00 & 1,32e^{-06} & -5,06e^{-06}\\-0,00 & -5,06e^{-06} & 6,01e^{-05}\end{bmatrix}\end{align}

\begin{align}X'y=\begin{bmatrix}56146,45\\28313059,77\\5636931,00\end{bmatrix}\end{align}


```{r}

# Berechnung des Einflussgewichtsvektor in R
b_hat <- solve(X.X) %*% X.y     # Vektor der geschätzten Regressionsgewichte
b_hat
```


\begin{align}\hat{b}=\begin{bmatrix}58,17\\-0,04\\5,31\end{bmatrix}\end{align}


#### Vorhersage der Mathematikleistung

Den Vektor mit den vorhergesagten Werten von y ($\hat{y}$) können Sie durch die Multiplikation der Matrix $X$ mit den Regressionsgewichten ($\hat{b}$) berechnen.


```{r}
y_hat <- X %*% b_hat # Vorhersagewerte für jede einzelne Person 
head(y_hat)

```
\begin{align}
\hat{y}_{math} = \begin{bmatrix}476,29\\605,51\\572,71\\633,37\\...\\604,22\end{bmatrix}
\end{align}


### Berechnung der standardisierten Regressionsgewichte 

Bisher wurden  nur die *unstandardisierten Regressionsgewichte* berechnet. Diese haben den Vorteil leichter interpretierbar zu sein. So wird das unstandardisierte Regressionsgewicht folgendermaßen interpretiert: wenn sich die unabhängige Variable um eine Einheit verändert, verändert sich die abhängige Variable um den unstandardisierten Koeffizienten. Der Nachteil dieser unstandardisierten Regressionsgewichte ist jedoch, dass die Regressionsgewichte nicht zwischen verschiedenen Prädiktoren vergleichbar sind. Demzufolge kann anhand der Größe der Regressionsgewichte nicht gesagt werden, welcher Regressionskoeffizient, d.h. welcher Prädiktor, eine stärkere Erklärungskraft hat.

Daher werden die Regressionsgewichte häufig standardisiert. Durch die Standardisierung sind die Regressionsgewichte nicht mehr von der ursprünglichen Skala abhängig und haben daher den Vorteil, dass sie miteinander verglichen werden können. Allerdings sind die *standardisierten Regressionsgewichte* nicht mehr so leicht zu interpretieren. Die Interpretation der standardisierten Regressionsgewichte lautet: wenn sich die unabhängige Variable um eine Standardabweichung erhöht (und unter Kontrolle weiterer unabhängiger Variablen), so beträgt die erwartete Veränderung in der abhängigen Variable $\beta$ Standardabweichungen (das standardisierte Interzept ist Null).

Die standardisierten Regressionsgewichte können bestimmt werden, indem zunächst alle Variablen standardisiert werden. Dafür haben wir bereits im letzten Semester den Befehl `scale` kennen gelernt. Wir wenden ihn auf die Werte der abhängigen Variable `y` und die der Prädiktoren `X` an. 

```{r}
#Berechnung der standardisierten Regressionsgewichte
y_s <- scale(y) # Standardisierung y
X_s <- scale(X) # Standardisierung X
head(X_s)
```

Bei der Ansicht des standardisierten Objektes `X_s` sehen wir, dass die alle Werte in der ersten Spalte nun `NaN` sind. Dies liegt daran, dass in dieser Spalte ja keine Streuung vorhanden ist und die Standardisierung somit nicht funktioniert. Für die Berechnung der Regressionsgewichte im standardisierten Fall sollten auch hier wieder 1en stehen, weshalb wir alle Werte in der Spalte mit 1 ersetzen müssen.

```{r}
X_s[,1] <- 1    # Einsenvektor wieder auffüllen
```

Die bereits im unstandardisierten Fall zur Bestimmung der Regressionsgewichte könnten nun wieder einzeln durchgeführt werden. Allerdings kann man diese natürlich auch in einer Zeile Code durchführen, was hier demonstriert wird. 

```{r}
b_hat_s <- solve(t(X_s)%*% X_s) %*% t(X_s)%*%y_s #Regressionsgewichte aus den standardisierten Variablen
round(b_hat_s, 3)
```

Wir sehen im Ergebnis, dass die Regressionskonstante 0 ist. Die lineare Regression geht stets durch den Mittelwert aller Variablen. Durch die Standardisierung ist dieser für jede Variable 0, wodurch der Punkt (0/0/0) durchlaufen werden muss. Weiterhin erkennen wir, dass der Einfluss des IQs der Lesefähigkeit deskriptiv überlegen ist. 

### Berechnung des globalen Signifikanztest

#### Determinationskoeffizient $R^2$

Der Determinationskoeffizient $R^2$ gibt an wieviel Varianz in der abhängigen Variable durch die unabhängigen Variablen erklärt werden kann:

$R^2= \dfrac{Q_d}{Q_d + Q_e}$

```{r}

# Determinationskoeffizient R2
Q_d <- sum((y_hat - mean(y))^2)    # Regressionsquadratsumme
Q_e <- sum((y - y_hat)^2)          # Fehlerquadratsumme
R2 <- Q_d / (Q_d + Q_e)            # Determinationskoeffizient R^2

```

$R^2= \dfrac{Q_d}{Q_d + Q_e} = \dfrac{`r round (Q_d,2)`}{`r round (Q_d,2)` + `r round (Q_e,2)`} = `r round (R2,2)`$


#### F-Wert

Der F-Wert dient zur Überprüfung der Gesamtsignifikanz des Modells. Er sagt aus, ob der Determinationskoeffizient $R^2$ signifikant von 0 verschieden ist.


```{r}

# F-Wert
n <- length(y)                     # Fallzahl (n=100)
m <- ncol(X)-1                     # Zahl der Prädiktoren (m=2)
F_omn <- (R2/m) / ((1-R2)/(n-m-1))   # F-Wert
F_krit <- qf(.95, df1=m, df2=n-m-1)  # kritischer F-Wert (alpha=5%)
p <- 1-pf(F_omn, m, n-m-1)           # p-Wert

```


$F_{omn} = \dfrac{\dfrac{R^2}{m}}{\dfrac{1-R^2}{n-m-1}} = \dfrac{\dfrac{`r round(R2,2)`}{`r m`}}{\dfrac{1-`r round(R2,2)`}{`r n`-`r m`-1}} = `r round(F_omn,2)`$

$df_1 = 2, df_1 = n-m-1 = `r n`-`r m`-1 =97$

$F_{krit}(\alpha=.05, df_1=2, df_2= 97)= `r round (F_krit,2)`$

$p=`r sprintf(format(1-pf(F_omn, m, n-m-1), digits = 3, trim = TRUE, scientific = FALSE))`$


## Berechnung der Regression mit lm-Funktionen in R

Für die Schätzung von Regressionsmodellen kann die Basis-Funktion `lm` verwendet werden. Um zusätzlich die standardisierten Koeffizienten zu erhalten, kann die Funktion `lm.beta` aus dem gleichnamigen Paket `lm.beta` genutzt werden.

```{r}

#Paket installieren (wenn nötig)
#install.packages("lm.beta", repos = "http://cran.us.r-project.org")
library(lm.beta)

# Regressionsanalyse mit lm
reg <- lm(math ~ reading + IQ, data = Schulleistungen)

# Ergebnisausgabe einschließlich standardisierter Koeffizienten mit lm.beta
summary(lm.beta(reg))

```

**Ergebnisinterpretation:**

* die Lesekompetenz und allgemeine Intelligenz erklären gemeinsam 48,73% der Varianz in der Mathematiktestleistung
* Dieser Varianzanteil ist signifikant von null verschieden
* Regressionsgewichte:
    + Regressionskonstante $b_0$:
        + Der Erwartungswert der Mathematikleistung für ein Individuum mit null Punkten im IQ und null Punkten in Lesekompetenz beträgt 58,17 Punkte.
    + Regressionsgewicht $b_1$:
        + bei einem Punkt mehr in der Lesekompetenz und unter Kontrolle des IQ beträgt die erwartete Veränderung in der Mathematikleistung `r round(b_hat[2],2)` Punkte.
        + Der Einfluss von Lesekompetenz auf Mathematikleistung ist nicht signfikant von null verschieden ($p=`r round(summary(reg)$coefficients[,4][2], 3)`$)
    + Regressionsgewicht $b_2$:
        + unter Kontrolle der Lesekompetenz beträgt die erwartete Veränderung in der Mathematikleistung bei einem Punkt mehr im IQ `r round(b_hat[3],2)` Punkte.
        + Der Einfluss der allgemeinen Intelligenz auf Mathematikleistung ist signfikant von null verschieden ($p=`r round(summary(reg)$coefficients[,4][3], 3)`$)    

* Standardisierte Regressionsgewichte
    + Standardisiertes Regressionsgewicht $\beta_1$: Unter Kontrolle des IQ beträgt die erwartete Veränderung in der Mathematikleistung bei einer Standardabweichung mehr in Lesekompetenz `r round(b_hat_s[2],2)` Standardabweichungen.
    + Standardisiertes Regressionsgewicht $\beta_2$: Unter Kontrolle der Lesekompetenz beträgt die erwartete Veränderung in der Mathematikleistung bei einer Standardabweichung mehr im IQ `r round(b_hat_s[3],2)` Standardabweichungen.

Verweis zu letzter Sitzung: In solch einer multiplen Regression können Suppressoreffekte gut aufgedeckt werden. Diese zeigen sich dann, wenn die $\beta$ Gewichte in der multiplen Regression dem Betrag nach größer sind, als deren korrespondierende $\beta$ Gewichte in einer einfachen Regression. Dies ist in unserem Beispiel jedoch nicht der Fall.
    
***

## R-Skript
Den gesamten `R`-Code, der in dieser Sitzung genutzt wird, können Sie [`r fontawesome::fa("download")` hier herunterladen](/post/PsyBSc7_R_Files/Regression-I.R).
