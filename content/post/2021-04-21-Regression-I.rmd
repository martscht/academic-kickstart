---
title: "Regressionsanalyse I"
output:
  html_document
date: '2021-04-26'
slug: reg1
categories:
     - BSc7
tags:
- Regression
- Zusammenhangsanalyse
- Gerichtete Zusammenhänge
subtitle: ''
summary: ''
authors: [schroeder, gruetzmacher]
lastmod: '2021-04-20 12:00:12 CEST'
featured: no
header:
     image: "/header/FEI_Sitzung5_post.jpg"
     caption: "[Courtesy of pexels](https://www.pexels.com/photo/man-looking-in-binoculars-during-sunset-802412/)"
projects: []
  
---

```{r}

# Datensatz laden
load(url("https://pandar.netlify.app/post/Schulleistungen.rda"))

```


## Einleitung

In der [letzten Sitzung](/post/partial) haben wir unter anderem Korrelationen zwischen zwei Variablen behandelt. Zur Wiederholung: mithilfe einer Korrelation kann die Stärke des Zusammenhangs zwischen zwei Variablen quantifiziert werden. Dabei haben beide Variablen den gleichen Stellenwert, d.h. eigentlich ist es egal welche Variable die x- und welche Variable die y-Variable ist. Wir haben außerdem Methoden kennengelernt, mit denen der Einfluss einer (oder mehrerer) Drittvariablen kontrolliert werden kann; die Partial- und Semipartialkorrelation. In der heutigen Sitzung wollen wir uns hingegen mit gerichteten Zusammenhängen, d.h. mit Regressionen, beschäftigen.

## Lineare Regression

Das Ziel einer Regression besteht darin, eine Variable durch eine oder mehrere andere Variablen vorherzusagen (Prognose). Die vorhergesagte Variable wird als Kriterium, Regressand oder auch abhängige Variable (AV) bezeichnet und üblicherweise mit *y* symbolisiert. Die Variablen zur Vorhersage der abhängigen Variablen werden als Prädiktoren, Regressoren oder unabhängige Variablen (UV) bezeichnet und üblicherweise mit *x* symbolisiert.
Die häufigste Form der Regressionsanalyse ist die lineare Regression, bei der der Zusammenhang über eine Gerade bzw. eine Ebene (bei zwei Prädiktoren) beschrieben wird. Demzufolge kann die lineare Beziehung zwischen den vorgesagten Werten und den Werten der unabhängigen Variablen mathematisch folgendermaßen beschreiben werden:

$$y_i = b_0 +b_{1}x_{i1} + ... +b_{m}x_{im} + e_i$$

* $b_0$ = y-Achsenabschnitt/ Ordinatenabschnitt/ Konstante/ Interzept:
    + Der Wert von y bei einer Ausprägung von 0 in allen *x*-Variablen
    
* $b_{1}/ b_m$ = Regressionsgewichte der Prädiktoren:
    + beziffern die Steigung der Regressionsgeraden
    + Interpretation: die Steigung der Geraden lässt erkennen, um wie viele Einheiten *y* zunimmt, wenn *x* um eine Einheit zunimmt
* $e_i$ = Regressionsresiduum (kurz: Residuum), Residualwert oder Fehlerwert:
    + die Differenz zwischen einem vorhergesagten ($\hat{y}$) und beobachteten ($y$) y-Wert 
    + je größer die Fehlerwerte, umso größer ist die Abweichung eines beobachteten vom vorhergesagten Wert
    
**Einfache lineare Regression**

![Grafische Darstellung einer einfachen linearen Regression](/post/Reg1.png){width="80%"}

$\hat{y_i} = b_0 +b_{1}x_{i1}$ (Regressiongerade = vorhergesagte Werte)

**Multiple Regression (mehrere Prädiktoren)**

![Grafische Darstellung einer multiplen Regression](/post/Reg2.png){width="80%"}

$y_i = b_0 +b_{1}x_{i1} + b_{2}x_{i2} + e_i$


## Berechnung der Regressionsgewichte $b_i$ "per Hand"

Eine Stichprobe von 100 Schülerinnen und Schülern hat einen Lese- und einen Mathematiktest beantwortet, zusätzlich einen allgemeinen Intelligenztest. Die Testleistungen sind untereinander alle positiv korreliert. Auch die beiden fachspezifischen Tests für Lesen (`reading`) und Mathematik (`math`) korrelieren substanziell.

Oft wird argumentiert, dass zum Lösen von mathematischen Textaufgaben auch Lesekompetenz erforderlich ist (z. B. bei Textaufgaben). Anhand des Datensatzes soll untersucht werden, wie stark sich die Mathematikleistungen durch Lesekompetenz und allgemeine Intelligenz vorhersagen lassen.

Die Formel lautet demnach:

$$y_{i,math} = b_0 +b_{reading}x_{i,reading} + b_{IQ}x_{i,IQ} + e_i$$
oder in Matrixform:

$$\begin{align}
\begin{bmatrix} y_1\\y_2\\y_3\\y_4\\...\\y_{100}\end{bmatrix} = b_{0} *
\begin {bmatrix}1\\1\\1\\1\\...\\1\end{bmatrix} + b_{reading} *
\begin {bmatrix}x_{reading1}\\x_{reading2}\\x_{reading3}\\x_{reading4}\\...\\y_{reading100}\end{bmatrix} + b_{IQ} *
\begin {bmatrix}x_{IQ1}\\x_{IQ2}\\x_{IQ3}\\x_{IQ4}\\...\\x_{IQ100}\end{bmatrix} +
\begin {bmatrix}e_1\\e_2\\e_3\\e_4\\...\\e_{100}\end{bmatrix}
\end{align}$$


```{r}

# Vektor Y
y <- Schulleistungen$math
head(y)

# Matrix X vorbereiten (Spalten mit beiden Prädiktoren + Spalte mit Einsen anfügen)
X <- as.matrix(Schulleistungen[,c("reading", "IQ")])       
X <- cbind(rep(1,nrow(X)), X)                         
head(X)

```

\begin{align}y = \begin{bmatrix}451,98\\589,65\\509,33\\560,43\\...\\603,18\end{bmatrix}\end{align}

\begin{align}X = \begin{bmatrix}1 & 449,59 & 81,78\\1 & 544,85 & 106,76\\1 & 331,35 & 99,14\\1 & 531,54 & 111,91\\ ... & ... & ... \\1 & 487,22 & 106,13\end{bmatrix}\end{align}


**Vorgehen bei der Berechnung der Regressionsgewichte:**

1. Berechnung der Kreuzproduktsumme (X’X)
2. Berechnung der Inversen der Kreuzproduktsumme ($(X'X)^{-1}$)
3. Berechnung des Kreuzproduksummenvektors (X'y)
4. Berechnung des Einflussgewichtsvektor

**1. Berechnung der Kreuzproduktsumme (X’X)**

Die Kreuzproduktsumme (X'X) wird berechnet, indem die transponierte Matrix X (X') mit der Matrix X multipliziert wird. Die transponierte Matrix X' erhalten Sie in R durch die Befehl `t(X)`.

\begin{align}X=\begin{bmatrix}1 & 449,59 & 81,78\\1 & 544,85 & 106,76\\1 & 331,35 & 99,14\\1 & 531,54 & 111,91\\... & ... & ... \\1 & 487,22 & 106,13\end{bmatrix}\end{align}


\begin{align}X'=\begin{bmatrix} 1 & 1 & 1 & 1 & ... & 1\\449,58 & 544,85 & 331,35 & 531,54 & ... & 487,22\\81,78 & 106,76 & 99,14 & 111,91 & ... & 106,13\end{bmatrix}\end{align}


\begin{align}X'X=\begin{bmatrix}100,00 & 49606,60 & 9813,43\\49606,61 & 25730126,10 & 4962448,08\\9813,43 & 4962448,10 & 987595,82\end{bmatrix}\end{align}


```{r}

# Berechnung der der Kreuzproduktsumme X’X in R
X.X <- t(X) %*% X        # X' erhalten Sie durch t(X)
X.X

```

**2. Berechnung der Inversen der Kreuzproduktsumme $((X'X)^{-1})$**

Die Inverse der Kreuzproduktsumme kann in R durch den `solve()` Befehl berechnet werden.

```{r}

# Berechnung der Inversen (mit Regel nach Sarrus) in R
solve(X.X)

```

\begin{align}(X'X)^{-1}= \begin{bmatrix}0,42 & -1,56e^{-04} & -3,39^{-03}\\-0,00 & 1,32e^{-06} & -5,06e^{-06}\\-0,00 & -5,06e^{-06} & 6,01e^{-05}\end{bmatrix}\end{align}

**3. Berechnung des Kreuzproduktsummenvektors (X'y)**

Der Kreuzproduktsummenvektor (X'y) wird durch die Multiplikation der transponierten X Matrix (X') und des Vektors y berechnet.  


\begin{align}X'=\begin{bmatrix}1 & 1 & 1 & 1 & ... & 1\\449,58 & 544,85 & 331,35 & 531,54 & ... & 487,22\\81,78 & 106,76 & 99,14 & 111,91 & ... & 106,13\end{bmatrix}\end{align}

\begin{align}y=\begin{bmatrix}451,98\\589,65\\509,33\\560,43\\...\\603,18\end{bmatrix}\end{align}

\begin{align}X'y=\begin{bmatrix}56146,45\\28313059,77\\5636931,00\end{bmatrix}\end{align}


```{r}

#Berechnung des Kreuzproduksummenvektors X`y in R
X.y <- t(X) %*% y        
X.y

```

**4. Berechnung des Einflussgewichtsvektors**

Die geschätzten Regressionsgewichte nach dem Kriterium der kleinsten Quadrate werden berechnet, indem die Inverse der Kreuzproduktsumme $((X'X)^{-1})$ mit dem Kreuzproduktsummenvektor (X'y) multipliziert wird. Den Vektor mit den vorhergesagten Werten von y ($\hat{y}$) können Sie durch die Multiplikation der X Matrix mit den Regressionsgewichten ($\hat{b}$) berechnen.  

\begin{align}(X'X)^{-1}= \begin{bmatrix}0,42 & -1,56e^{-04} & -3,39^{-03}\\-0,00 & 1,32e^{-06} & -5,06e^{-06}\\-0,00 & -5,06e^{-06} & 6,01e^{-05}\end{bmatrix}\end{align}

\begin{align}X'y=\begin{bmatrix}56146,45\\28313059,77\\5636931,00\end{bmatrix}\end{align}

\begin{align}\hat{b}=\begin{bmatrix}58,17\\-0,04\\5,31\end{bmatrix}\end{align}


```{r}

# Berechnung des Einflussgewichtsvektor in R
b.hat <- solve(X.X) %*% X.y     # Vektor der geschätzten Regressionsgewichte
b.hat
```

**Vorhersage der Mathematikleistung**
```{r}
y.hat <- as.vector(X %*% b.hat) # Vorhersagewerte für jede einzelne Person 
head(y.hat)

```
\begin{align}
\hat{y}_{math} = \begin{bmatrix}476,29\\605,51\\572,71\\633,37\\...\\604,22\end{bmatrix}
\end{align}

**Berechnung der standardisierten Regressionsgewichte**

Bisher wurden aber nur die *unstandardisierten Regressionsgewichte* berechnet. Diese haben den Vorteil leichter interpretierbar zu sein. So wird das unstandardisierte Regressionsgewicht folgendermaßen interpretiert: wenn sich die unabhängige Variable um eine Einheit verändert, verändert sich die abhängige Variable um den unstandardisierten Koeffizienten. Der Nachteil dieser unstandardisierten Regressionsgewichte ist jedoch, dass die Regressionsgewichte nicht zwischen verschiedenen Prädiktoren vergleichbar sind. Demzufolge kann anhand der Größe der Regressionsgewichte nicht gesagt werden, welcher Regressionskoeffizient, d.h. welcher Prädiktor, eine stärkere Erklärungskraft hat.
Daher werden die Regressionsgewichte häufig standardisiert. Durch die Standardisierung sind die Regressionsgewichte nicht mehr von der ursprünglichen Skala abhängig und haben daher den Vorteil, dass sie miteinander verglichen werden können. Allerdings sind die *standardisierten Regressionsgewichte* nicht mehr so leicht zu interpretieren. Die Interpretation der standardisierten Regressionsgewichte lautet: wenn sich die unabhängige Variable um eine Standardabweichung erhöht (und unter Kontrolle weiterer unabhängiger Variablen), so beträgt die erwartete Veränderung in der abhängigen Variable $\beta$ Standardabweichungen (das standardisierte Interzept ist Null).
Die standardisierten Regressionsgewichte können über den standardisierten y Vektor und die standardisierte Matrix (zur Standardisierung `scale()` Befehl in R verwenden) ermittelt werden.   

```{r}

#Berechnung der standardisierten Regressionsgewichte
y.s <- scale(y) # Standardisierung y
X.s <- scale(X) # Standardisierung X
X.s[,1] <- 1    # Einsenvektor ist nach Standardisierung zunächst NaN, muss wieder gefüllt werden

#Kombination der Einzelschritte zur Bestimmung der Regressionsgewichte
b.hat.s <- solve(t(X.s)%*% X.s) %*% t(X.s)%*%y.s #Regressionsgewichte aus den standardisierten Variablen
round(b.hat.s, 3)

```


**Berechnung des globalen Signifikanztest**

*Determinationskoeffizient $R^2$*

Der Determinationskoeffizient $R^2$ gibt an wieviel Varianz in der abhängigen Variable durch die unabhängigen Variablen erklärt werden kann:

$R^2= \dfrac{Q_d}{Q_d + Q_e}$

```{r}

# Determinationskoeffizient R2
Q.d <- sum((y.hat - mean(y))^2)    # Regressionsquadratsumme
Q.e <- sum((y - y.hat)^2)          # Fehlerquadratsumme
R2 <- Q.d / (Q.d + Q.e)            # Determinationskoeffizient R^2

```

$R^2= \dfrac{Q_d}{Q_d + Q_e} = \dfrac{`r round (Q.d,2)`}{`r round (Q.d,2)` + `r round (Q.e,2)`} = `r round (R2,2)`$


*F-Wert*

Der F-Wert dient zur Überprüfung der Gesamtsignifikanz des Modells. Er sagt aus, ob der Determinationskoeffizient $R^2$ signifikant von 0 verschieden ist.


```{r}

# F-Wert
n <- length(y)                     # Fallzahl (n=100)
m <- ncol(X)-1                     # Zahl der Prädiktoren (m=2)
F.omn <- (R2/m) / ((1-R2)/(n-m-1))   # F-Wert
F.krit <- qf(.95, df1=m, df2=n-m-1)  # kritischer F-Wert (alpha=5%)
p <- 1-pf(F.omn, m, n-m-1)           # p-Wert

```


$F_{omn} = \dfrac{\dfrac{R^2}{m}}{\dfrac{1-R^2}{n-m-1}} = \dfrac{\dfrac{`r round(R2,2)`}{`r m`}}{\dfrac{1-`r round(R2,2)`}{`r n`-`r m`-1}} = `r round(F.omn,2)`$

$df_1 = 2, df_1 = n-m-1 = `r n`-`r m`-1 =97$

$F_{krit}(\alpha=.05, df_1=2, df_2= 97)= `r round (F.krit,2)`$

$p=`r sprintf(format(1-pf(F.omn, m, n-m-1), digits = 3, trim = TRUE, scientific = FALSE))`$


## Berechnung der Regression mit lm-Funktionen in R

Für die Schätzung von Regressionsmodellen kann die Basis-Funktion `lm` verwendet werden. Um zusätzlich die standardisierten Koeffizienten zu erhalten, kann die Funktion `lm.beta` aus dem gleichnamigen Paket `lm.beta` genutzt werden.

```{r}

#Paket installieren (wenn nötig)
#install.packages("lm.beta", repos = "http://cran.us.r-project.org")
library(lm.beta)

# Regressionsanalyse mit lm
reg <- lm(math ~ reading + IQ, data = Schulleistungen)

# Ergebnisausgabe einschließlich standardisierter Koeffizienten mit lm.beta
summary(lm.beta(reg))

```

**Ergebnisinterpretation:**

* die Lesekompetenz und allgemeine Intelligenz erklären gemeinsam 48,73% der Varianz in der Mathematiktestleistung
* Dieser Varianzanteil ist signifikant von null verschieden
* Regressionsgewichte:
    + Regressionskonstante $b_0$:
        + Der Erwartungswert der Mathematikleistung für ein Individuum mit null Punkten im IQ und null Punkten in Lesekompetenz beträgt 58,17 Punkte.
    + Regressionsgewicht $b_1$:
        + bei einem Punkt mehr in der Lesekompetenz und unter Kontrolle des IQ beträgt die erwartete Veränderung in der Mathematikleistung `r round(b.hat[2],2)` Punkte.
        + Der Einfluss von Lesekompetenz auf Mathematikleistung ist nicht signfikant von null verschieden ($p=`r round(summary(reg)$coefficients[,4][2], 3)`$)
    + Regressionsgewicht $b_2$:
        + unter Kontrolle der Lesekompetenz beträgt die erwartete Veränderung in der Mathematikleistung bei einem Punkt mehr im IQ `r round(b.hat[3],2)` Punkte.
        + Der Einfluss der allgemeinen Intelligenz auf Mathematikleistung ist signfikant von null verschieden ($p=`r round(summary(reg)$coefficients[,4][3], 3)`$)    

* Standardisierte Regressionsgewichte
    + Standardisiertes Regressionsgewicht $\beta_1$: Unter Kontrolle des IQ beträgt die erwartete Veränderung in der Mathematikleistung bei einer Standardabweichung mehr in Lesekompetenz `r round(b.hat.s[2],2)` Standardabweichungen.
    + Standardisiertes Regressionsgewicht $\beta_2$: Unter Kontrolle der Lesekompetenz beträgt die erwartete Veränderung in der Mathematikleistung bei einer Standardabweichung mehr im IQ `r round(b.hat.s[3],2)` Standardabweichungen.

Verweis zu letzter Sitzung: In solch einer multiplen Regression können Suppressoreffekte gut aufgedeckt werden. Diese zeigen sich dann, wenn die $\beta$ Gewichte in der multiplen Regression dem Betrag nach größer sind, als deren korrespondierende $\beta$ Gewichte in einer einfachen Regression. Dies ist in unserem Beispiel jedoch nicht der Fall.
    
