---
title: Multiple Regression und Ausreißerdiagnostik
date: '2021-10-11'
slug: regression-aussreisser-klipps
categories:
  - MSc5a
tags:
  - Regression
  - Ausreißer
  - ggplot
  - linear
subtitle: ''
summary: ''
authors: [nehler, irmer, hartig]
lastmod: '2021-10-05T16:40:21+02:00'
featured: no
header:
  image: "/header/FEI_Sitzung1_post.jpg"
  caption: "[Courtesy of pxhere](https://pxhere.com/en/photo/806441)"
projects: []
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<div id="einleitung" class="section level2">
<h2>Einleitung</h2>
<p>In der <a href="/post/einleitung-klipps">Einführungssitzung</a> hatten wir einfache Operationen in <code>R</code>, das Einlesen von Datensätzen, einfache Deskriptivstatistiken, die lineare Regression, den <span class="math inline">\(t\)</span>-Test und einige Grundlagen der Inferenzstatistik wiederholt. Nun wollen wir mit etwas komplexeren, aber bereits bekannten, Methoden weitermachen und eine multiple Regression in <code>R</code> durchführen. Hierbei werden wir uns auch nochmal mit Ausreißern beschäftigen.</p>
<p>Bevor wir dazu die Daten einlesen, sollten wir als erstes die nötigen <code>R</code>-Pakete laden. <code>R</code> funktioniert wie eine Bibliothek, in der verschiedene Bücher erst vorhanden sein müssen, bevor man sie dann für eine Zeit leihen kann. Die <code>R</code>-Pakete, die wir im Folgenden brauchen, sind: das <code>car</code>-Paket, das <code>MASS</code>-Paket sowie das Paket mit dem Namen <code>lm.beta</code>. Diese Pakete müssen zunächst installiert werden (also im Sinne der Bibliothek eingelagert). Dies können Sie via <code>install.packages</code> machen:</p>
<pre class="r"><code>install.packages(&quot;car&quot;)            # Die Installation ist nur einmalig von Nöten!
install.packages(&quot;lm.beta&quot;)        # Sie müssen nur zu Update-Zwecken erneut installiert werden.
install.packages(&quot;MASS&quot;)</code></pre>
<p>Anschließend werden Pakete mit der <code>library</code>-Funktion geladen (für eine Zeit - nämlich den Verlauf einer Sitzung - ausgeliehen):</p>
<pre class="r"><code>library(lm.beta)  # Standardisierte beta-Koeffizienten für die Regression</code></pre>
<pre><code>## Warning: Paket &#39;lm.beta&#39; wurde unter R Version 4.1.1 erstellt</code></pre>
<pre class="r"><code>library(car)      # Zusätzliche Funktion für Diagnostik von Datensätzen</code></pre>
<pre><code>## Warning: Paket &#39;car&#39; wurde unter R Version 4.1.1 erstellt</code></pre>
<pre class="r"><code>library(MASS)     # Zusätzliche Funktion für Diagnostik von Datensätzen </code></pre>
<div id="daten-einladen" class="section level3">
<h3>Daten einladen</h3>
<p>Der Datensatz ist der selbe, wie in der <a href="/post/einleitung-klipps">Einführungssitzung</a>: Eine Stichprobe von 90 Personen wurde hinsichtlich der Lebenszufriedenheit, der Anzahl von depressiven Emotionen, der Depressivitaet und des Neurotizismus gemessen. Weiterhin gibt es eine Variable hinsichtlich der Intervention und des Geschlechts (0=m, 1=w). Sie können den <a href="/post/Depression.rda"><svg aria-hidden="true" role="img" viewBox="0 0 512 512" style="height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;"><path d="M216 0h80c13.3 0 24 10.7 24 24v168h87.7c17.8 0 26.7 21.5 14.1 34.1L269.7 378.3c-7.5 7.5-19.8 7.5-27.3 0L90.1 226.1c-12.6-12.6-3.7-34.1 14.1-34.1H192V24c0-13.3 10.7-24 24-24zm296 376v112c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V376c0-13.3 10.7-24 24-24h146.7l49 49c20.1 20.1 52.5 20.1 72.6 0l49-49H488c13.3 0 24 10.7 24 24zm-124 88c0-11-9-20-20-20s-20 9-20 20 9 20 20 20 20-9 20-20zm64 0c0-11-9-20-20-20s-20 9-20 20 9 20 20 20 20-9 20-20z"/></svg> Datensatz “Depression.rda” hier herunterladen</a>.</p>
<p>Nun müssen wir mit <code>load</code> die Daten laden. Liegt der Datensatz bspw. auf dem Desktop, so müssen wir den Dateipfad dorthin legen und können dann den Datensatz laden (wir gehen hier davon aus, dass Ihr PC “Musterfrau” heißt) <em>Tipp: Verwenden Sie unbedingt die automatische Vervollständigung von <code>R</code>-Studio, wie in der letzten Sitzung beschrieben</em>.</p>
<pre class="r"><code>load(&quot;C:/Users/Musterfrau/Desktop/Depression.rda&quot;)</code></pre>
<p>Genauso sind Sie in der Lage, den Datensatz direkt aus dem Internet zu laden. Hierzu brauchen Sie nur die URL und müssen <code>R</code> sagen, dass es sich bei dieser um eine URL handelt, indem Sie die Funktion <code>url</code> auf den Link anwenden. Der funktionierende Befehl sieht so aus (wobei die URL in Anführungszeichen geschrieben werden muss):</p>
<pre class="r"><code>load(url(&quot;https://pandar.netlify.app/post/Depression.rda&quot;))</code></pre>
<p>Nun sollte in <code>R</code>-Studio oben rechts in dem Fenster <em>Environment</em> unter der Rubrik “Data” unser Datensatz mit dem Namen “<em>Depression</em>” erscheinen.</p>
</div>
<div id="ein-überblick-über-die-daten" class="section level3">
<h3>Ein Überblick über die Daten</h3>
<p>Wir können uns die ersten (6) Zeilen des Datensatzes mit der Funktion <code>head</code> ansehen. Dazu müssen wir diese Funktion auf den Datensatz (das Objekt) <code>Depression</code> anwenden:</p>
<pre class="r"><code>head(Depression)</code></pre>
<pre><code>##   Lebenszufriedenheit Episodenanzahl Depressivitaet Neurotizismus
## 1                   7              4              7             5
## 2                   5              5              8             3
## 3                   8              7              6             6
## 4                   6              4              5             5
## 5                   6              9              8             5
## 6                   8              7              8             6
##     Intervention Geschlecht
## 1 Kontrollgruppe          0
## 2 Kontrollgruppe          1
## 3 Kontrollgruppe          0
## 4 Kontrollgruppe          1
## 5 Kontrollgruppe          1
## 6 Kontrollgruppe          1</code></pre>
<p>Wir erkennen die eben beschriebenen Spalten. Weiterhin sehen wir, dass die Änderungen aus der letzten Sitzung an der Variable Geschlecht natürlich nicht mehr enthalten sind, wenn der Datensatz neu geladen wird. Daher müssen wir die <code>levels</code> wieder anpassen und den falsch eingetragenen Wert für Person 5 korrigieren.</p>
<pre class="r"><code>levels(Depression$Geschlecht) &lt;- c(&quot;maennlich&quot;, &quot;weiblich&quot;)
Depression[5, 6] &lt;- &quot;maennlich&quot;    </code></pre>
<p>Das Folgende fundiert zum Teil auf Sitzungen zur Korrelation und Regression aus <a href="/lehre/#bsc7">Veranstaltungen aus dem Bachelorstudium zur Statistik Vertiefung</a>.</p>
</div>
</div>
<div id="multiple--lineare-regression" class="section level2">
<h2>(Multiple-) Lineare Regression</h2>
<p>Eine Wiederholung der Regressionsanalyse (und der Korrelation) finden Sie bspw. in <a href="https://hds.hebis.de/ubffm/Record/HEB366849158">Eid, Gollwitzer und Schmitt (2017)</a> Kapitel 16 bis 19, <a href="https://hds.hebis.de/ubffm/Record/HEB369761391">Agresti und Finlay (2013)</a>, Kapitel 9 bis 11 und <a href="https://hds.hebis.de/ubffm/Record/HEB371183324">Pituch und Stevens (2016)</a> in Kapitel 3.</p>
<p>Das Ziel einer Regression besteht darin, die Variation einer Variable durch eine oder mehrere andere Variablen vorherzusagen (Prognose und Erklärung). Die vorhergesagte Variable wird als Kriterium, Regressand oder auch abhängige Variable (AV) bezeichnet und üblicherweise mit <span class="math inline">\(y\)</span> symbolisiert. Die Variablen zur Vorhersage der abhängigen Variablen werden als Prädiktoren, Regressoren oder unabhängige Variablen (UV) bezeichnet und üblicherweise mit <span class="math inline">\(x\)</span> symbolisiert.
Die häufigste Form der Regressionsanalyse ist die lineare Regression, bei der der Zusammenhang über eine Gerade bzw. eine (Hyper-)Ebene beschrieben wird. Demzufolge kann die lineare Beziehung zwischen den vorhergesagten Werten und den Werten der unabhängigen Variablen mathematisch folgendermaßen beschrieben werden:</p>
<p><span class="math display">\[y_i = b_0 +b_{1}x_{i1} + ... +b_{m}x_{im} + e_i\]</span></p>
<ul>
<li>Ordinatenabschnitt/ <span class="math inline">\(y\)</span>-Achsenabschnitt/ Konstante/ Interzept <span class="math inline">\(b_0\)</span>:
<ul>
<li>Schnittpunkt der Regressionsgeraden mit der <span class="math inline">\(y\)</span>-Achse</li>
<li>Erwartung von y, wenn alle UVs den Wert 0 annehmen</li>
</ul></li>
<li>Regressionsgewichte <span class="math inline">\(b_{1},\dots, b_m\)</span>:
<ul>
<li>beziffern die Steigung der Regressionsgeraden</li>
<li>Interpretation: die Steigung der Geraden lässt erkennen, um wie viele Einheiten <span class="math inline">\(y\)</span> zunimmt, wenn (das jeweilige) x um eine Einheit zunimmt (unter Kontrolle aller weiteren Variablen im Modell)</li>
</ul></li>
<li>Regressionsresiduum (kurz: Residuum), Residualwert oder Fehlerwert <span class="math inline">\(e_i\)</span>:
<ul>
<li>die Differenz zwischen einem beobachteten und einem vorhergesagten y-Wert (<span class="math inline">\(e_i=y_i-\hat{y}_i\)</span>)</li>
<li>je größer die Fehlerwerte (betraglich), umso größer ist die Abweichung (betraglich) eines beobachteten vom vorhergesagten Wert</li>
</ul></li>
</ul>
<p><code>R</code> kann natürlich die Schätzung der Regressionskoeffizienten für Sie übernehmen. Für eine händische Berechnung würde die folgende Gleichung zur Kleinste-Quadrate-Schätzung verwendet, die wir aber nicht präziser besprechen werden:
<span class="math display">\[\hat{\mathbf{b}}=(X&#39;X)^{-1}X&#39;Y\]</span>
Falls Sie sich über die mathematischen Operationen hinter der Bestimmung von verschiedenen Kennwerten in der Regression (bspw. <span class="math inline">\(R^2\)</span>) informieren wollen, können Sie im <a href="/post/regression-und-ausreisserdiagnostik/#AppendixA">Appendix A</a> des PsyMSc Studiengags nachlesen.</p>
<div id="unser-modell-und-das-lesen-von-r-outputs" class="section level3">
<h3>Unser Modell und das Lesen von <code>R</code>-Outputs</h3>
<p>Wir wollen mit Hilfe eines Regressionsmodells die Depressivitaet durch das Geschlecht und die Lebenszufriedenheit vorhersagen. Dies funktioniert in <code>R</code> ganz leicht mit der <code>lm</code> ("<strong>l</strong>inear <strong>m</strong>odeling) Funktion. Dieser müssen wir zwei Argumente übergeben: 1) unsere angenommene Beziehung zwischen den Variablen; 2) den Datensatz, in welchem die Variablen zu finden sind:</p>
<pre class="r"><code>lm(Depressivitaet ~ 1 + Geschlecht + Lebenszufriedenheit, data = Depression)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Depressivitaet ~ 1 + Geschlecht + Lebenszufriedenheit, 
##     data = Depression)
## 
## Coefficients:
##         (Intercept)   Geschlechtweiblich  Lebenszufriedenheit  
##              7.2353               1.9117              -0.3663</code></pre>
<p>Hierbei zeigt die Tilde (<code>~</code>) auf, welche Variable die AV ist (sie steht links der Tilde), welche die UVs sind (sie stehen rechts der Tilde und werden durch <code>+</code> getrennt) und ob das Interzept mitgeschätzt werden soll (per Default wird dieses mitgeschätzt, was bedeutet, dass “<code>1 +</code>” redundant ist und daher hier weggelassen werden könnte - nicht mit einbezogen wird das Interzept via “<code>0 +</code>”). Diese Notation wird in sehr vielen Modell verwendet, in welchen es um die Beziehung zwischen unabhängigen und abhängigen Variablen geht! Im <a href="#AppendixA">Appendix A</a> können Sie nachlesen, welche weiteren Befehle zum gleichen Ergebnis führen und wie bspw. explizit das Interzept in die Gleichung mit aufgenommen werden kann (oder darauf verzichtet wird!).</p>
<p>Im Output sehen wir die Parameterschätzungen unseres Regressionsmodells, das folgendermaßen aussieht:
<span class="math display">\[\text{Depressivitaet}_i=b_0+b_1\text{Geschlecht}_i+b_2\text{Lebenszufriedenheit}_i+\varepsilon_i,\]</span>
für <span class="math inline">\(i=1,\dots,100=:n\)</span>. Wir wollen uns die Ergebnisse unserer Regressionsanalyse noch detaillierter anschauen. Dazu können wir wieder die <code>summary</code> Funktion anwenden. Wir weisen dafür den <code>lm</code> Befehl einem Objekt zu, welches wir weiterverwenden können, um darauf beispielsweise <code>summary</code> auszuführen. Zur Erinnerung: Wir speichern dieses Objekt ab, indem wir eine Zuordnung durchführen via <code>&lt;-</code> und einen Namen <code>model</code> vergeben.</p>
<pre class="r"><code>model &lt;- lm(Depressivitaet ~ 1 + Geschlecht + Lebenszufriedenheit, data = Depression)
summary(model)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Depressivitaet ~ 1 + Geschlecht + Lebenszufriedenheit, 
##     data = Depression)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.4037 -0.6711  0.0121  0.6952  3.3289 
## 
## Coefficients:
##                     Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)          7.23528    0.64773   11.17  &lt; 2e-16 ***
## Geschlechtweiblich   1.91174    0.28879    6.62 2.83e-09 ***
## Lebenszufriedenheit -0.36632    0.09227   -3.97 0.000147 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.286 on 87 degrees of freedom
## Multiple R-squared:  0.4833, Adjusted R-squared:  0.4714 
## F-statistic: 40.68 on 2 and 87 DF,  p-value: 3.362e-13</code></pre>
<p>Um auch die standardisierten Ergebnisse zu erhalten, verwenden wir die Funktion <code>lm.beta</code> (<em>lm</em> steht hier für lineares Modell und <em>beta</em> für die standardisierten Koeffizienten. Achtung: Häufig werden allerdings auch unstandardisierten Regressionskoeffizienten als <span class="math inline">\(\beta\)</span>s bezeichnet, sodass darauf stets zu achten ist). Diese muss nach dem Erstellen eines linearen Modells (in unserem Fall <code>model</code>) auf dieses angewendet werden. Anschließend wollen wir uns noch ein <code>summary</code> des Modells ausgeben lassen. So erhalten wir zusätzlich standardisierte Koeffizienten. Für die Kombination von Funktionen haben wir in der letzten Sitzung die Verwendung des Pipes <code>|&gt;</code> kennen gelernt.</p>
<pre class="r"><code>lm.beta(model) |&gt; summary()</code></pre>
<pre><code>## 
## Call:
## lm(formula = Depressivitaet ~ 1 + Geschlecht + Lebenszufriedenheit, 
##     data = Depression)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.4037 -0.6711  0.0121  0.6952  3.3289 
## 
## Coefficients:
##                     Estimate Standardized Std. Error t value Pr(&gt;|t|)    
## (Intercept)          7.23528      0.00000    0.64773   11.17  &lt; 2e-16 ***
## Geschlechtweiblich   1.91174      0.53257    0.28879    6.62 2.83e-09 ***
## Lebenszufriedenheit -0.36632     -0.31940    0.09227   -3.97 0.000147 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.286 on 87 degrees of freedom
## Multiple R-squared:  0.4833, Adjusted R-squared:  0.4714 
## F-statistic: 40.68 on 2 and 87 DF,  p-value: 3.362e-13</code></pre>
<pre class="r"><code>model |&gt; lm.beta() |&gt; summary() # noch genauer</code></pre>
<pre><code>## 
## Call:
## lm(formula = Depressivitaet ~ 1 + Geschlecht + Lebenszufriedenheit, 
##     data = Depression)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.4037 -0.6711  0.0121  0.6952  3.3289 
## 
## Coefficients:
##                     Estimate Standardized Std. Error t value Pr(&gt;|t|)    
## (Intercept)          7.23528      0.00000    0.64773   11.17  &lt; 2e-16 ***
## Geschlechtweiblich   1.91174      0.53257    0.28879    6.62 2.83e-09 ***
## Lebenszufriedenheit -0.36632     -0.31940    0.09227   -3.97 0.000147 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.286 on 87 degrees of freedom
## Multiple R-squared:  0.4833, Adjusted R-squared:  0.4714 
## F-statistic: 40.68 on 2 and 87 DF,  p-value: 3.362e-13</code></pre>
<p>Nochmal zur Wiederholung: Der Pipe-Operator übergibt immer das resultiernde Objekt des vorherigen Befehls an die erste Stelle der folgenden Funktion. Somit können wir die Pipe wie folgt lesen: “Nehme <code>model</code> und wende darauf <code>lm.beta()</code> and, nehme anschließend das resultierende Objekt und wende darauf <code>summary()</code> an.”</p>
<p><code>summary</code> ist eine weiterverbreitete Funktion, die Objekte zusammenfasst und interessante Informationen für uns auf einmal bereitstellt. <code>R</code> Outputs sehen fast immer so aus, weswegen es von unabdingbarer Wichtigkeit ist, dass wir uns mit diesem Output vertraut machen. Im Grunde werden uns alle nötigen Informationen, was in dieser Zusammenfassung steht, durch die Überschrift und Spaltenüberschriften gegeben. Was können wir diesem nun Schritt für Schritt entnehmen?</p>
<pre><code>## 
##  Call:
##  lm(formula = Depressivitaet ~ 1 + Geschlecht + Lebenszufriedenheit, data = Depression)</code></pre>
<p>Fasst noch einmal zusammen, welches Objekt “zusammengefasst” wird. Hier steht sozusagen das zuvor untersuchte <code>lm</code>-Objekt (<code>model</code>, bzw. <code>lm.beta(model)</code>).</p>
<pre><code>## 
## Residuals:
##     Min       1Q   Median       3Q      Max 
## -3.4037  -0.6711   0.0121   0.6952   3.3289</code></pre>
<p>Diese Deskriptivstatistiken (gerundet auf 3 Nachkommastellen) geben uns ein Gefühl für die Datengrundlage: die Überschrift sagt uns, dass es hierbei um die Residuen im Regressionsmodell geht. <code>Min</code> steht für das Minimum (-3.404), <code>1Q</code> beschreibt das erste Quartil (-0.671); also den Prozentrang von 25% - es liegen 25% darunter und 75% der Werte darüber; <code>Median</code> beschreibt den 50. Prozentrang (0.012), <code>3Q</code> beschreibt das 3. Quartil, also Prozentrang 75% (0.695) und <code>Max</code> ist der maximale Wert der Residuen (3.329). Der Mittelwert trägt hier keine Information, da die Residuen immer so bestimmt werden, dass sie im Mittel verschwinden, also ihr Mittelwert bei 0 liegt. Da der Median auch sehr nah an der 0 liegt, zeigt dies, dass die Residuen wahrscheinlich recht symmetrisch verteilt sind. Auch das 1. und 3. Quartil verteilen sich ähnlich (also entgegengesetzte Vorzeichen aber betraglich ähnliche Werte), was ebenfalls für die Symmetrie spricht. Wir können die Residuen unserem <code>model</code> Objekt ganz leicht entlocken, indem wir den Befehl <code>resid</code> auf dieses Objekt anwenden oder <code>model$residuals</code> tippen. Bspw. ergibt sich der Mittelwert als:</p>
<pre class="r"><code>mean(x = resid(model)) # Mittelwert mit Referenzierung aus dem lm Objekt &quot;model&quot;</code></pre>
<pre><code>## [1] -6.002143e-17</code></pre>
<p>Natürlich könnte man statt der Funktion <code>resid</code> auch das Element Residuals im Objekt ansprechen mittels <code>model$residuals</code>.</p>
<p>Die Zahl, die beim Mittelwert ausgegeben wird, ist folgendermaßen zu lesen: <code>e-16</code> steht für <span class="math inline">\(*10^{-17}=0.00000000000000001\)</span> (die Dezimalstelle wird um 17 Stellen nach links verschoben), somit ist <code>6.002143e-17</code><span class="math inline">\(=6.002143*10^{-15}=0.000000000000006002143\approx 0\)</span>. Hier kommt in diesem Beispiel nicht exakt 0 heraus, da innerhalb der Berechnungen immer auf eine gewisse Genauigkeit gerundet wird. Diese hängt von der sogennanten Maschinengenauigkeit von <code>R</code> ab. Eine noch höhere Genauigkeit der Darstellung von Zahlen würde einfach zu viel Speicherplatz verbrauchen!</p>
<p>Nun kommen wir zum eigentlich Spannenden, nämlich der Übersicht über die Parameterschätzung (<code>Coefficients</code>). Diese sieht in sehr vielen Analysen sehr ähnlich aus (z.B. logistische Regression oder Multi-Level-Analysen/hierarchische Regression aus diesem Kurs).</p>
<pre><code>## 
## Coefficients:
##                     Estimate Standardized Std. Error t value Pr(&gt;|t|)    
## (Intercept)         7.23528       0.00000    0.64773   11.17  &lt; 2e-16 *** 
## Geschlechtweiblich  1.91174       0.53257    0.28879    6.62 2.83e-09 *** 
## Lebenszufriedenehit -0.36632     -0.31940    0.09227   -3.97 0.000147 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Insgesamt gibt es 6 Spalten, wobei die <code>Standardized</code>-Spalte extra durch das Paket <code>lm.beta</code> angefordert wurde (sie ist also nicht immer in der Summary enthalten). In der ersten Spalte stehen die Variablennamen, die selbsterklärend sein sollten. Die Spalte <code>Estimate</code> zeigt den unstandardisierten Parameter (hier Regressionsgewicht). Z.B. liegt das Interzept <span class="math inline">\(b_0\)</span> bei 7.2353. Das Partialregressionsgewicht vom Geschlecht <span class="math inline">\(b_\text{Geschlecht}\)</span> liegt bei 1.9117. Da Frauen mit <code>1</code> kodiert sind, bedeutet dies: Wenn Frauen im Vergleich zu Jungen betrachtet werden, so steigt die Depressivitaet durchschnittlich um 1.9117 Punkte (“durchschnittlich” in der Interpretation ist enorm wichtig, da es ja noch den Vorhersagefehler gibt). Folglich können wir das Interzept ebenfalls interpretieren: Ein Mann mit einem Lebenszufriedenheit von 0 (dieser Wert ist leider unrealistisch, da die Skala hier nicht zentriert wurde - die Effekte von Zentrierung schauen wir uns in der Sitzung zur <a href="/post/under-construction">Hierarchischen Regression</a> genauer an!) hat eine durchschnittliche Leseleistung von 7.2353.</p>
<p>In der Spalte <code>Standardized</code> stehen die standardisierten Regressionsgewichte. Hier werden die Daten so transformiert, dass sowohl die AV als auch die UVs jeweils Mittelwerte von 0 und Varianzen (bzw. Standardabweichungen) von 1 aufweisen. Das Interzept ist nun nicht länger interessant, da, wenn <span class="math inline">\(y\)</span> einen Mittelwert von 0 hat und auch die unabhängigen Variablen zentriert sind (also Mittelwerte von 0 haben), dann ist das Interzept gerade jener vorhergesagte Wert für <span class="math inline">\(y\)</span>, der anfällt, wenn alle Prädiktoren den Wert 0 -also ihren Mittelwert - annehmen. Bei der Regression ist es aber so, dass an der Stelle, an der die Prädiktoren ihren Mittelwert annehmen, auch der Mittelwert von <span class="math inline">\(y\)</span> liegt; hier also der Wert 0. Folglich ist das Interzept im standardisierten Fall <strong><em>immer</em></strong> 0.
Das standardisierte Regressionsgewicht der Lebenszufriedenheit <span class="math inline">\(\beta_\text{Lebenszufriedenheit}\)</span> liegt bei -0.3194, was bedeutet, dass bei einer Erhöhung der Lebenszufriedenheit um eine Standardabweichung die Depressivitaet im Mittel (im Durchschnitt) um -0.3194 Standardabweichungen fällt.</p>
<p>Für die Interpretation des Geschlechts als Prädiktor bringt die Standardisierung eine Erschwerung mit sich. Die beiden Ausprägungen sind nun nicht mehr eine Einheit bzw. Standardabweichung voneinander entfernt. Daher kann man den Vergleich nicht mehr mit einbeziehen. Es lässt sich nur sagen: Steigt die Variable Geschlecht um eine Standardabweichung auf der Dimension zwischen Männern und Frauen, so steigt die Depressivitaet um durchschnittlich 0.5326 Standardabweichungen.</p>
<p>Die Spalte <code>Std.Error</code> enthält die Standardfehler. Diese werden in <span class="math inline">\(t\)</span>-Werte umgerechnet via <span class="math inline">\(\frac{Est}{SE}\)</span>: es wird also die Parameterschätzung durch seinen Standardfehler geteilt und in der nächsten Spalte <code>t value</code> dargestellt. In einigen Summaries wird auch anstelle des <span class="math inline">\(t\)</span>-Wertes der <span class="math inline">\(z\)</span>-Wert verwendet. Hierbei ändert sich nichts, nur wird zur Herleitung der <span class="math inline">\(p\)</span>-Werte eben die <span class="math inline">\(z\)</span>- anstatt der <span class="math inline">\(t\)</span>-Verteilung verwendet. Wenn wir uns allerdings an Statistik aus dem Bachelor erinnern, so bemerken wir, dass für große Stichproben die <span class="math inline">\(t\)</span> und die <span class="math inline">\(z\)</span>-Verteilung identisch (bzw. sehr nahe beieinander liegend) sind. Als groß gilt hierbei bereits eine Stichprobengröße von 50!</p>
<p>In der Spalte <code>Pr(&gt;|t|)</code> stehen die zugehörigen <span class="math inline">\(p\)</span>-Werte. “Pr” steht für die Wahrscheinlichkeit (<strong>Pr</strong>obability), dass die Teststatistik (hier der <span class="math inline">\(t\)</span>-Wert) im Betrag ein extremeres Ergebnis aufzeigt, als das Beobachtete. Außerdem bekommen wir noch mit Sternchen angezeigt, auf welchem Signifikanzniveau die einzelnen Parameter statistisch bedeutsam sind.</p>
<p>Zusammenfassend entnehmen wir dem Output, dass das Interzept bedeutsam von 0 verschieden ist - auch die Effekte der Prädiktoren sind auf dem 5%-Niveau statistisch signifikant. Dies bedeutet bspw. für die Lebenszufriedenheit, dass <em>mit einer Irrtumswahrscheinlichkeit von 5% der Regressionsparameter der Lebenszufriedenheit in der Population nicht 0 ist und somit auch in der Population mit dieser Irrtumswahrscheinlichkeit von einem Effekt zu sprechen ist</em>. Hierbei ist es essentiell, dass sich die statistiche Interpretation immer auf die Population bezieht. Dass ein Koeffizient nicht 0 ist (in der Stichprobe), erkennen wir einfach daran, dass er von 0 abweicht, allerdings kann dieses Ergebnis eben durch Zufall aufgetreten sein. Häufig weichen Werte in unserer Stichprobe offensichtlich von 0 ab, allerdings nicht stark genug, als dass wir dies auch für die Population schlussfolgern (<em>auch:</em> schließen/inferieren, desewegen auch <strong>Inferenzstatistik</strong>/schließende Statistik) würden (mit einer Irrtumswahrscheinlichkeit von 5%).</p>
<p>Regressionskoeffizienten können einzeln signifikant sein, ohne, dass signifikante Anteile der Variation der abhängigen Variable erklärt werden.</p>
<pre><code>## 
## Residual standard error: 1.286 on 87 degrees of freedom
## Multiple R-squared:  0.4833, Adjusted R-squared:  0.4714 
## F-statistic: 40.68 on 2 and 87 DF,  p-value: 3.362e-13</code></pre>
<p>Dazu entnehmen wir dem letzten Block den Standardfehler der Residuen (<code>Residual standard error</code>), der im Grunde die Fehlervariation von <span class="math inline">\(y\)</span> beschreibt, sowie das multiple <span class="math inline">\(R^2\)</span> (<code>Multiple R-squared</code>), welches anzeigt, dass ca. 35.5% der Variation der Depressivitaet auf die Prädiktoren Geschlecht und Lebenszufriedenheit zurückgeführt werden kann. Dieses Varianzinkrement ist statistisch signifikant, was wir am <span class="math inline">\(F\)</span>-Test in der letzten Zeile ablesen können. Der <span class="math inline">\(F\)</span>-Wert (<code>F-statistic</code>) liegt bei 26.75, wobei die Hypothesenfreiheitsgrade hier gerade 2 sind (<span class="math inline">\(df_h\)</span>) und die Residualfreiheitsgrade bei 97 (<span class="math inline">\(df_e\)</span>) liegen. Der zugehörige <span class="math inline">\(p\)</span>-Wert ist deutlich kleiner als 5% und liegt bei <span class="math inline">\(5.594*10^{-10}\)</span>. Dies bedeutet, dass mit einer Irrtumswahrscheinlichkeit von 5% auch in der Population eine Vorhersage der Leseleistung durch Geschlecht und Intelligenz gemeinsam angenommen werden kann (<span class="math inline">\(R^2\neq0\)</span>). In einem Artikel (oder einer Abschlussarbeit) würden wir zur Untermauerung <em>F</em>(2,97)=26.75, p&lt;.001 in den Fließtext schreiben.</p>
<p>Außerdem könnten wir natürlich auch das mit <code>summary</code> erstellte Objekt unter einem Namen abspeichern und ihm dann weitere Informationen entlocken. Bspw. erhalten wir mit <code>$coefficients</code> die Tabelle der Koeffizienten.</p>
<pre class="r"><code>summary_model &lt;- summary(lm.beta(model))
summary_model$coefficients # Koeffiziententabelle</code></pre>
<pre><code>##                       Estimate Standardized Std. Error   t value     Pr(&gt;|t|)
## (Intercept)          7.2352799    0.0000000 0.64773072 11.170197 1.722491e-18
## Geschlechtweiblich   1.9117353    0.5325749 0.28878522  6.619921 2.834596e-09
## Lebenszufriedenheit -0.3663154   -0.3194030 0.09226649 -3.970189 1.474393e-04</code></pre>
<pre class="r"><code>names(summary_model)      # weitere mögliche Argumente, die wir erhalten können</code></pre>
<pre><code>##  [1] &quot;call&quot;          &quot;terms&quot;         &quot;residuals&quot;     &quot;coefficients&quot; 
##  [5] &quot;aliased&quot;       &quot;sigma&quot;         &quot;df&quot;            &quot;r.squared&quot;    
##  [9] &quot;adj.r.squared&quot; &quot;fstatistic&quot;    &quot;cov.unscaled&quot;</code></pre>
<pre class="r"><code>summary_model$r.squared  # R^2</code></pre>
<pre><code>## [1] 0.4832806</code></pre>
<p>Gleiches können wir mit allen Summary-Objekten auch in späteren Sitzungen machen!</p>
<p>Wenn wir diese Siginfikanzentscheidungen nutzen wollen, um die Effekte in der Population auf diese Weise zu interpretieren, so müssen einige Voraussetzungen erfüllt sein, die zunächst noch geprüft werden müssten. Bspw. nehmen wir für ein Regressionsmodell an, dass die Regressoren lineare Beziehungen mit dem Kriterium aufweisen. Die Personen/Erhebungen sollten bspw. unabhängig und identisch verteilt sein (sie sollten aus einer i.i.d. Population stammen, also keinerlei Beziehung untereinander aufweisen und dem gleichen Populationsmodell folgen). Die Residuen innerhalb der Regression werden als normalverteilt und homoskedastisch (also mit gleicher Varianz über alle Ausprägungen der Prädiktoren) angenommen. Nur unter bestimmten Voraussetzungen lassen sich Signifikanzentscheidungen im Allgemeinen überhaupt interpretieren. Außerdem beeinflussen Ausreißer die Schätzungen der Regressionskoeffizienten drastisch.</p>
</div>
</div>
<div id="prüfen-der-voraussetzungen" class="section level2">
<h2>Prüfen der Voraussetzungen</h2>
<p>Dieser Abschnitt stellt nur in einem Auszug die Prüfung der Voraussetzungen bereit. Wenn Sie alle Prüfungen wiederholen wollen, können Sie in der Bachelorsitzung zu <a href="/post/reg3">Regression III</a> nachlesen. Die Voraussetzung der Unabhängigkeit und der Gleichverteiltheit ist und bleibt eine Annahme, die wir nicht prüfen können. Wir können jedoch durch das Studiendesign (Randomisierung, Repräsentativität) diese Annahme plausibilisieren. Die Linearitätsannahme und die Voraussetzungen der Residuen werden wir hier nicht genauer betrachten. Fokus legen wir auf die Multikollinearität und mögliche Ausreißer. Nochmal aufgezählt alle Voraussetzungen:</p>
<ul>
<li>korrekte Spezifikation des Modells</li>
<li>Unabhängigkeit der Residuen</li>
<li>Messfehlerfreiheit der unabhängigen Variablen</li>
<li>Linearität bei Modellierung linearer Effekt</li>
<li>Homoskedastizität (unabhängigkeit der Residualvarianz)</li>
<li>Normalverteilung der Residuen bzw. multivariate Normalverteilung</li>
<li>Multikollinearität</li>
<li>Identifikation von Ausreißern und einflussreichen Datenpunkten</li>
</ul>
<p>Im Folgenden werden wir mit dem unstandardisierten Modell weiterarbeiten, welches wir im Objekt <code>model</code> gespeichert hatten.</p>
<p>Vertiefende Literatur zum folgenden Stoff finden wir bspw. in <a href="(https://hds.hebis.de/ubffm/Record/HEB366849158)">Eid, et al. (2017)</a> in Kapitel 19.13 und <a href="https://hds.hebis.de/ubffm/Record/HEB371183324">Pituch und Stevens (2016)</a> in Kapitel 3.10 - 3.14.</p>
</div>
<div id="multikollinearität" class="section level2">
<h2>Multikollinearität</h2>
<p>Multikollinearität ist ein potenzielles Problem der multiplen Regressionsanalyse und liegt vor, wenn zwei oder mehrere Prädiktoren hoch miteinander korrelieren. Hohe Multikollinearität</p>
<ul>
<li>schränkt die mögliche multiple Korrelation ein, da die Prädiktoren redundant sind und überlappende Varianzanteile in <span class="math inline">\(y\)</span> erklären.</li>
<li>erschwert die Identifikation von bedeutsamen Prädiktoren, da deren Effekte untereinander konfundiert sind (die Effekte können schwer voneinander getrennt werden).</li>
<li>bewirkt eine Erhöhung der Standardfehler der Regressionskoeffizienten <em>(der Standardfehler ist die Standardabweichung zu der Varianz der Regressionskoeffizienten bei wiederholter Stichprobenziehung und Schätzung)</em>. Dies bedeutet, dass die Schätzungen der Regressionsparameter instabil, und damit weniger verlässlich, werden.
Weitere Informationen zur Instabilität und zu Standardfehlern kann der/die interessierte Leser*in in <a href="/post/regression-und-ausreisserdiagnostik/#AppendixD">Appendix D des Tutorials</a> im PsyMSc1 nachlesen. Beachten Sie dabei, dass hier mit Matrixnotationsweise gearbeitet wird.</li>
</ul>
<p>Multikollinearität kann durch Inspektion der <em>bivariaten Zusammenhänge</em> (Korrelationsmatrix) der Prädiktoren <span class="math inline">\(x_j\)</span> untersucht werden.</p>
<pre class="r"><code># Korrelation der Prädiktoren
cor(as.numeric(Depression$Geschlecht), Depression$Lebenszufriedenheit)</code></pre>
<pre><code>## [1] -0.2869571</code></pre>
<p>Dieser Wert ist jedoch nur ein erstes Indiz für die Performance. Leider kann der Korrelationskoeffizient nicht alle Formen von Multikollinearität aufdecken. Darüber hinaus ist die Berechnung der sogenannten <em>Toleranz</em> (T) und des <em>Varianzinflationsfaktors</em> (VIF) für jeden Prädiktor möglich. Hierfür wird nacheinander für jeden Prädiktor <span class="math inline">\(x_j\)</span> der Varianzanteil <span class="math inline">\(R_j^2\)</span> berechnet, der durch Vorhersage von <span class="math inline">\(x_j\)</span> durch <em>alle anderen Prädiktoren</em> in der Regression erklärt wird. Toleranz und VIF sind wie folgt definiert:</p>
<ul>
<li><span class="math inline">\(T_j = 1-R^2_j = \frac{1}{VIF_j}\)</span></li>
<li><span class="math inline">\(VIF = \frac{1}{1-R^2_j} = \frac{1}{T_j}\)</span></li>
</ul>
<p>Offensichtlich genügt eine der beiden Statistiken, da sie vollständig ineinander überführbar und damit redundant sind. Empfehlungen als Grenzwert für Kollinearitätsprobleme sind z. B. <span class="math inline">\(VIF_j&gt;10\)</span> (<span class="math inline">\(T_j&lt;0.1\)</span>; siehe <a href="https://hds.hebis.de/ubffm/Record/HEB366849158">Eid, et al., 2017, S. 712 und folgend</a>). Die Varianzinflationsfaktoren der Prädiktoren im Modell können mit der Funktion <code>vif</code> des <code>car</code>-Paktes bestimmt werden, der Toleranzwert als Kehrwert des VIFs.</p>
<pre class="r"><code>vif(model)        # VIF</code></pre>
<pre><code>##          Geschlecht Lebenszufriedenheit 
##            1.089733            1.089733</code></pre>
<pre class="r"><code>1/vif(model)      # Toleranz</code></pre>
<pre><code>##          Geschlecht Lebenszufriedenheit 
##           0.9176556           0.9176556</code></pre>
<p>In diesem Beispiel mit nur 2 Prädiktoren ist <span class="math inline">\(R_j^2=cor(\text{Geschlecht},\text{Lebenszufriedenheit})^2\)</span> und die Formeln sind daher sehr einfach auch mit Hand zu bestimmen:</p>
<pre class="r"><code>1/(1-cor(as.numeric(Depression$Geschlecht), Depression$Lebenszufriedenheit)^2) # 1/(1-R^2) = VIF</code></pre>
<pre><code>## [1] 1.089733</code></pre>
<pre class="r"><code>1-cor(as.numeric(Depression$Geschlecht), Depression$Lebenszufriedenheit)^2 # 1-R^2 = Toleranz</code></pre>
<pre><code>## [1] 0.9176556</code></pre>
<p>Für unser Modell wird ersichtlich, dass die Prädiktoren zwar eine Korrelation haben, aber keiner der Werte außerhalb der bewährten Grenzen liegt. Dementsprechend liegt kein Multikollinearitätsproblem vor. Unabhängigkeit folgt hieraus allerdings nicht, da nicht-lineare Beziehungen zwischen den Variablen bestehen könnten, die durch diese Indizes nicht abgebildet werden.</p>
</div>
<div id="identifikation-von-ausreißern-und-einflussreichen-datenpunkten" class="section level2">
<h2>Identifikation von Ausreißern und einflussreichen Datenpunkten</h2>
<p>Die Plausibilität unserer Daten ist enorm wichtig. Aus diesem Grund sollten Ausreißer oder einflussreiche Datenpunkte analysiert werden. Diese können bspw. durch Eingabefehler (Alter von 211 Jahren anstatt 21), durch absichtliche Fälschung bzw. Unlust in der Beantwortung entstehen oder es sind seltene Fälle (hochintelligentes Kind in einer Normstichprobe), welche so in natürlicher Weise (aber mit sehr geringer Häufigkeit) auftreten können. Es muss dann entschieden werden, ob Ausreißer die Repräsentativität der Stichprobe gefährden und ob diese daher besser ausgeschlossen werden sollten.</p>
<div id="hebelwerte" class="section level3">
<h3>Hebelwerte</h3>
<p><em>Hebelwerte</em> <span class="math inline">\(h_j\)</span> (engl.: leverage values) erlauben die Identifikation von Ausreißern aus der gemeinsamen Verteilung der unabhängigen Variablen, d.h. sie geben an, wie weit ein Wert vom Mittelwert einer Prädiktorvariable entfernt ist. Je höher der Hebelwert, desto weiter liegt der einzelne Fall vom Mittelwert der gemeinsamen Verteilung der unabhängigen Variablen entfernt und desto stärker kann somit der Einfluss auf die Regressionsgewichte sein. Diese werden mit der Funktion <code>hatvalues</code> ermittelt. Kriterien zur Beurteilung der Hebelwerte variieren, so werden von <a href="https://hds.hebis.de/ubffm/Record/HEB366849158">Eid et al. (2017, S. 707 und folgend)</a> Grenzen von <span class="math inline">\(2\cdot k / n\)</span> für große und <span class="math inline">\(3\cdot k / n\)</span> für kleine Stichproben vorgeschlagen, in den Vorlesungsfolien von Prof. Dr. Klein aus dem Bachelor werden Werte von <span class="math inline">\(4/n\)</span> als auffällig eingestuft (hierbei ist <span class="math inline">\(k\)</span> die Anzahl an Prädiktoren und <span class="math inline">\(n\)</span> die Anzahl der Beobachtungen). Alternativ zu einem festen Cut-Off-Kriterium kann die Verteilung der Hebelwerte inspiziert werden, wobei diejenigen Werte als kritisch bewertet werden, die aus der Verteilung ausreißen. Die Funktion <code>hatvalues</code> erzeugt die Hebelwerte aus einem Regression-Objekt. Wir wollen diese als Histogramm darstellen.</p>
<pre class="r"><code>n &lt;- length(residuals(model))   # Anzahl an Personen bestimmen
h &lt;- hatvalues(model)           # Hebelwerte
hist(h, breaks  = 20)               
abline(v = 4/n, col = &quot;red&quot;)  # Cut-off bei 4/n</code></pre>
<p><img src="/post/2021-10-11-Regression_Ausreisser_KliPPs_files/figure-html/unnamed-chunk-19-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Hier eine kurze Beschreibung aller Argumente in der Grafik: Das Zusatzargument <code>breaks = 20</code> in <code>hist</code> gibt an, dass 20 Balken gezeichnet werden sollen. <code>abline</code> ist eine Funktion, die eine Gerade einem Plot hinzufügt. Dem Argument <code>v</code> wird hierbei der Punkt übergeben, an welchem eine <strong>v</strong>ertikale Linie eingezeichnet werden soll. <code>col = "red"</code> gibt an, dass diese Linie rot sein soll.</p>
</div>
<div id="cooks-distanz" class="section level3">
<h3>Cook’s Distanz</h3>
<p><em>Cook’s Distanz</em> <span class="math inline">\(CD_i\)</span> bezieht sich auf Ausreißer auf der abhängigen Variablen und gibt eine Schätzung an, wie stark sich die Regressionsgewichte verändern, wenn eine Person <span class="math inline">\(i\)</span> aus dem Datensatz entfernt wird. Fälle, deren Elimination zu einer deutlichen Veränderung der Ergebnisse führen würden, sollten kritisch geprüft werden. Als einfache Daumenregel gilt, dass <span class="math inline">\(CD_i&gt;1\)</span> auf einen einflussreichen Datenpunkt hinweist. Cook’s Distanz kann mit der Funktion <code>cooks.distance</code> berechnet werden.</p>
<pre class="r"><code># Cooks Distanz
CD &lt;- cooks.distance(model) # Cooks Distanz
hist(CD, breaks  = 20)
abline(v = 1, col = &quot;red&quot;)  # Cut-off bei 1</code></pre>
<p><img src="/post/2021-10-11-Regression_Ausreisser_KliPPs_files/figure-html/unnamed-chunk-20-1.png" width="672" style="display: block; margin: auto;" />
In diesem Plot ist die vertikale Linie nicht enthalten, da der Plot schon zu früh entlang der x-Achse aufhört. Wir können die Grenzen mit <code>xlim = c(0,1)</code> explizit von 0 bis 1 vorgeben:</p>
<pre class="r"><code># Cooks Distanz nochmal
hist(CD, breaks  = 20, xlim = c(0, 1))
abline(v = 1, col = &quot;red&quot;)  # Cut-off bei 1</code></pre>
<p><img src="/post/2021-10-11-Regression_Ausreisser_KliPPs_files/figure-html/unnamed-chunk-21-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="blasendiagramm" class="section level3">
<h3>Blasendiagramm</h3>
<p>Die Funktion <code>influencePlot</code> des <code>car</code>-Paketes erzeugt ein “Blasendiagramm” zur simultanen grafischen Darstellung von Hebelwerten (auf der x-Achse), studentisierten Residuen (auf der y-Achse) und Cook’s Distanz (als Größe der Blasen). Vertikale Bezugslinien markieren das Doppelte und Dreifache des durchschnittlichen Hebelwertes, horizontale Bezugslinien die Werte -2, 0 und 2 auf der Skala der studentisierten Residuen. Fälle, die nach einem der drei Kriterien als Ausreißer identifiziert werden, werden im Streudiagramm durch ihre Zeilennummer gekennzeichnet. Diese Zeilennummern können verwendet werden, um sich die Daten der auffälligen Fälle anzeigen zu lassen. Sie werden durch <code>InfPlot</code> ausgegeben. Auf diese kann durch <code>as.numeric(row.names(InfPlot))</code> zugegriffen werden.</p>
<pre class="r"><code># Blasendiagramm mit Hebelwerten, studentisierten Residuen und Cooks Distanz
# In &quot;IDs&quot; werden die Zeilennummern der auffälligen Fälle gespeichert,
# welche gleichzeitig als Zahlen im Blasendiagramm ausgegeben werden
InfPlot &lt;- influencePlot(model)</code></pre>
<p><img src="/post/2021-10-11-Regression_Ausreisser_KliPPs_files/figure-html/unnamed-chunk-22-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>IDs &lt;- as.numeric(row.names(InfPlot))
# Werte der identifizierten Fälle
InfPlot</code></pre>
<pre><code>##      StudRes        Hat       CookD
## 41 -0.418397 0.13516814 0.009207384
## 49 -2.797895 0.03445764 0.086345854
## 64 -2.118632 0.07260556 0.112621347
## 78  2.707307 0.01881699 0.043677143
## 85  1.490516 0.11113495 0.091308435</code></pre>
<p>Schauen wir uns die möglichen Ausreißer an und standardisieren die Ergebnisse für eine bessere Interpretierbarkeit.</p>
<pre class="r"><code># Rohdaten der auffälligen Fälle (gerundet für bessere Übersichtlichkeit)
Depression[IDs,]</code></pre>
<pre><code>##    Lebenszufriedenheit Episodenanzahl Depressivitaet Neurotizismus
## 41                   2              4              6             3
## 49                   5              4              2             8
## 64                  10              4              1            10
## 78                   7              9              8             6
## 85                  11              7              5            10
##                   Intervention Geschlecht
## 41                 VT Coaching  maennlich
## 49                 VT Coaching  maennlich
## 64 VT Coaching + Gruppenuebung  maennlich
## 78 VT Coaching + Gruppenuebung  maennlich
## 85 VT Coaching + Gruppenuebung  maennlich</code></pre>
<pre class="r"><code># z-standardisierte Werte der auffälligen Fälle
scale(Depression[,1:4])[IDs,]</code></pre>
<pre><code>##      Lebenszufriedenheit Episodenanzahl Depressivitaet Neurotizismus
## [1,]          -2.8535787     -1.5202002      0.1947782    -2.0315461
## [2,]          -0.9079568     -1.5202002     -2.0671617     0.8799053
## [3,]           2.3347462     -1.5202002     -2.6326467     2.0444859
## [4,]           0.3891244      2.1775841      1.3257481    -0.2846752
## [5,]           2.9832868      0.6984704     -0.3707068     2.0444859</code></pre>
<p>Die Funktion <code>scale</code> z-standardisiert den Datensatz. Dies macht natürlich hauptsächlich für die intervallskalierten Skalen Sinn (also die ersten vier Spalten im Datensatz). Mit Hilfe von <code>[IDs,]</code>, werden die entsprechenden Zeilen der Ausreißer aus dem Datensatz ausgegeben und anschließend auf 2 Nachkommastellen gerundet. Hierbei ist es extrem wichtig, dass wir <code>scale(Depression[,1:4])[IDs,]</code> und nicht <code>scale(Depression[IDs,1:4])</code> schreiben, da bei der zweiten Schreibweise die Daten reskaliert (z-standardisiert) werden, allerdings auf Basis der ausgewählten Fälle (n=5) und nicht auf Basis der gesamten Stichprobe (n=100). Mit Hilfe der z-standardisierten Ergebnisse lassen sich Ausreißer hinsichtlich ihrer Ausprägungen einordnen:</p>
<div id="interpretation" class="section level4">
<h4>Interpretation</h4>
<p>Was ist an den fünf identifizierten Fällen konkret auffällig? Zur Beantwortung dieser Frage sollten wir uns auf die Variablen konzentrieren, die in unserem Modell enthalten sind.</p>
<ul>
<li><em>Fall 41</em>: durchschnittlicher Depressivitaetswert bei unterdurchschnittlicher Lebenszufriedenheit als Mann</li>
<li><em>Fall 49</em>: Sehr niedriger Depressivitaetswert bei niedriger Lebenszufriedenheit als Mann</li>
<li><em>Fall 64</em>: Stark überdurchschnittliche Lebenzufriedenheit und sehr stark unterdurchschnittlicher Depressivitaetswert als Mann</li>
<li><em>Fall 78</em>: Überdurchschnittlicher Depressivitaetswert bei durchschnittlicher Lebenszufriedenheit als Mann</li>
<li><em>Fall 85</em>: Leicht unterdurchschnittlicher Depressivitaetswert bei sehr hoher Lebenszufriedenheit als Mann</li>
</ul>
<p>Die Entscheidung, ob Ausreißer oder auffällige Datenpunkte aus Analysen ausgeschlossen werden, ist schwierig und kann nicht pauschal beantwortet werden. Im vorliegenden Fall wäre z.B. zu überlegen, ob Fälle 41 und 85, den Fragebogen zur Lebenszufriedenheit ernsthaft ausgefüllt haben, da sie mit ihren Werten sehr weit außerhalb der Verteilung liegen. Allerdings liegen keine weiteren Informationen über die erhobene Gruppe vor (und es existiert eine reele Chance auf diese Daten), weshalb ein Ausschluss nicht sinnvoll zu Begründen wäre.</p>
</div>
</div>
<div id="einfluss-von-hebelwert-und-cooks-distanz" class="section level3">
<h3>Einfluss von Hebelwert und Cook’s Distanz</h3>
<p>Was wäre nun gewesen, wenn die Hebelwerte oder Cook’s Distanz extreme Werte angezeigt hätten? Um dieser Frage auf den Grund zu gehen, schauen wir uns für eine Kombination der beiden Koeffizienten den Effekt auf eine Regressionsgerade an. Die vier Grafiken zeigen jeweils die Regressionsgerade in schwarz ohne den jeweiligen Ausreißer, während die Gerade in blau die Regressionsanalyse (<code>Y ~ 1 + X</code>) inklusive des Ausreißers symbolisiert. Falls Sie die Grafik selbst bauen wollen, finden Sie sie in <a href="AppendixB">Appendix B</a>.</p>
<p><img src="/post/2021-10-11-Regression_Ausreisser_KliPPs_files/figure-html/unnamed-chunk-24-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>In <code>A)</code> ist die Regression ohne Ausreißer dargestellt. <code>B)</code> zeigt den Effekt, wenn nur der Hebelwert groß ist. Es ist kaum ein Einfluss auf die Regressionsgerade auszumachen. Der Mittelwert der Variable <code>X</code> wird stark nach rechts verschoben. Dies bedeutet, dass ein großer Hebelwert nur den Mittelwert dieser Variable in Richtung des Ausreißers “hebelt”, nicht aber zwangsweise die Regressionsgerade! <code>C)</code> zeigt eine große Cook’s Distanz bei gleichzeitig kleinem Hebelwert. die Gerade ist etwas nach oben verschoben und auch die Steigung hat sich leicht verändert. Insgesamt ist mit dem bloßen Auge allerdings noch kein extremer Effekt auf die Gerade auszumachen. Dieser Effekt wird nur in <code>D)</code> deutlich. Hier ist sowohl Cook’s Distanz als auch der Hebelwert extrem. Dadurch verändert sich die Regressionsgerade stark. Hier könnten wir davon sprechen, dass die Gerade durch den Ausreißer nach unten “gehebelt” wird. Die hier dargestellte Erhebung hat auch auch die größte Mahalanobisdistanz, da sie sowohl in <code>X</code> als auch in <code>Y</code> Richtung extrem ist (siehe dazu nächsten Abschnitt). Insgesamt zeigt diese Grafik, dass nicht ein Koeffizient alleine ausreicht, um einen Effekt auf eine Regressionsanalyse zu untersuchen und dass Werte besonders dann extreme Auswirkungen haben, wenn mehrere Koeffizienten groß sind!</p>
</div>
<div id="mahalanobisdistanz" class="section level3">
<h3>Mahalanobisdistanz</h3>
<p>Die Mahalanobisdistanz (siehe z.B. <a href="https://hds.hebis.de/ubffm/Record/HEB366849158">Eid et al., 2017,</a> ab Seite 707) ist ein Werkzeug, welches zur Identifikation von multidimensionalen Ausreißern verwendet werden kann. Mit Hilfe der Mahalanobisdistanz wird die Entfernung vom zentralen Zentroiden bestimmt und mit Hilfe der Kovarianzmatrix gewichtet. Im Grunde kann man sagen, dass die Entfernung vom gemeinsamen Mittelwert über alle Variablen an der Variation in den Daten relativiert wird. Im <strong>eindimensionalen Fall</strong> ist die Mahalanobisdistanz nichts anderes als der quadrierte <span class="math inline">\(z\)</span>-Wert, denn wir bestimmen dann die Mahalanobisdistanz einer Person <span class="math inline">\(i\)</span> via
<span class="math display">\[MD_i=\frac{(X_i-\bar{X})^2}{\sigma_X^2}=\left(\frac{X_i-\bar{X}}{\sigma_X}\right)^2=z^2.\]</span>
Wir erkennen, dass wir hier den Personenwert relativ zur Streuung in den Daten betrachten. Nutzen wir nun mehrere Variablen und wollen multivariate Ausreißer interpretieren, so ist die Mahalanobisdistanz folgendermaßen definiert:
<span class="math display">\[MD_i=(\mathbf{X}_i-\bar{\mathbf{X}})&#39;\Sigma^{-1}(\mathbf{X}_i-\bar{\mathbf{X}}).\]</span>
Der Vektor der Mittelwertsdifferenzen <span class="math inline">\(\mathbf{X}_i-\bar{\mathbf{X}}\)</span> wird durch die Kovarianzmatrix der Daten <span class="math inline">\(\Sigma\)</span> gewichtet. Sind zwei Variablen <span class="math inline">\(X_1\)</span> und <span class="math inline">\(X_2\)</span> positiv korreliert, so treten große Werte (und auch kleine Werte) auf beiden Variablen gemeinsam häufig auf, allerdings sind große <span class="math inline">\(X_1\)</span> und kleine <span class="math inline">\(X_2\)</span>-Werte (gleichzeitig und auch umgekehrt) unwahrscheinlich. Dies lässt sich anhand der Mahalanobisdistanz untersuchen. <em>Wann ist nun ein Mahalanobisdistanzwert extrem?</em> Dies können wir uns an einem zweidimensionalen Beispiel klarer machen. Dazu tragen wir in ein Diagramm die Ellipsen (Kurven) gleicher Mahalanobisdistanz ein, also jene Linien, welche laut Mahalanobisdistanz gleich weit vom Zentroiden entfernt liegen. Je dunkler die Kurven, desto weiter entfernt liegen diese Punkte vom Zentroiden (hier <span class="math inline">\((0,0)\)</span>) und desto unwahrscheinlicher sind diese Punkte in den Daten zu beobachten. In diesem Beispiel nehmen wir an, dass die Variablen positiv korreliert sind:</p>
<p><img src="/post/2021-10-11-Regression_Ausreisser_KliPPs_files/figure-html/unnamed-chunk-25-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Der Ellipsenplot zeigt zwei multivariat-normalverteilte Variablen. Die Normalverteilungsdichte können wir uns dort wie einen Hügel vorstellen, der aus dem Bildschirm wächst, wobei hellere Kurven für eine größere Höhe des Hügels sprechen.</p>
<p>Der Zentroid ist hier in Hellgrün dargestellt. Außerdem sind zwei Punkte (in schwarz) eingezeichnet, die die gleiche Mahalanobisdistanz haben. Allerdings sehen wir, repräsentiert durch die blaue Linie, ebenfalls die euklidische Distanz. Die euklidische Distanz ist jene, welche wir nutzen, wenn wir ein Maßband anlegen würden (um bspw. ein Zimmer zu vermessen oder eben die Distanz auf dem Bildschirm der beiden Punkte vom hellgrünen Zentroiden). Dies bedeutet, dass wenn wir in dieser Grafik (und damit in den Daten) die Kovariation der Variablen ignorieren würden, so würden wir den linken schwarzen Punkt und die blaue Linie als äquidistant (also gleich weit entfernt) annehmen. Berücksichtigen wir allerdings die positive Korrelation der Variablen, dann erkennen wir, dass die beiden schwarzen Punkte gleich wahrscheinlich sind und damit im Schnitt gleich häufig auftreten. Dies lässt sich folgendermaßen erklären: wenn zwei Variablen positiv korreliert sind, sind extreme positive und extreme negative Werte auf beiden Variablen gleichzeitig recht wahrscheinlich, während es sehr unwahrscheinlich ist, dass die eine Variable eine hohe und die andere gleichzeitig eine niedrige Ausprägung aufweist (und umgekehrt). Entsprechend haben Wertekonstellationen, die sehr unwahrscheinlich sind (gegeben der Struktur in den Daten) eine große Mahalanobisdistanz - in der Grafik wächst also die Mahalanobisdistanz je dunkler die Kurve.</p>
<p>Außerdem gilt, dass bei multivariater Normalverteilung der Daten die Mahalanobisdistanz <span class="math inline">\(\chi^2(df=p)\)</span> verteilt ist, wobei <span class="math inline">\(p\)</span> die Anzahl an Variablen ist. Der Vorteil hiervon ist, dass wir eine eindimensionale Verteilung untersuchen können, um ein Gefühl für multivariate Daten zu erhalten. Bspw. kann dann ein Histogramm oder ein Q-Q-Plot verwendet werden, um die Daten auf Normalverteilung zu untersuchen, bzw. es kann bspw. der Kolmogorov-Smirnov Test durchgeführt werden, um zu prüfen, ob die Mahalanobisdistanz <span class="math inline">\(\chi^2(df=p)\)</span> verteilt ist.</p>
<p>Der Befehl in <code>R</code> für die Mahalanobisdistanz ist <code>mahalanobis</code>. Wir nehmen einfach die Depressivitaet und die Lebenszufriedenheit via <code>Depression$...</code> als unsere zwei Variablen auf und fassen diese zusammen zu einer Datenmatrix <code>X</code> mit <code>cbind</code> (column-bind), welches die übergebenen Variablen als Spaltenvektoren zusammenfasst. <code>mahalanobis</code> braucht 3 Argumente: <code>x = X</code> die Daten, den gemeinsamen Mittelwert der Daten, den wir hier mit <code>colMeans</code> bestimmen (es wird jeweils der Mittelwert für die Spalten gebildet) sowie die Kovarianzmatrix der Daten mit <code>cov(X)</code> (<code>cor</code> gibt die Korrelationsmatrix aus; hier wird allerdings die Kovarianzmatrix gebraucht- anhand der Korrelationsmatrix lässt sich jedoch die Beziehung der Variablen besser einordnen), an welcher die Struktur relativiert werden soll:</p>
<pre class="r"><code>X &lt;- cbind(Depression$Depressivitaet, Depression$Lebenszufriedenheit) # Datenmatrix mit Depressivitaet in Spalte 1 und Lebenszufriedenheit in Spalte 2
colMeans(X)  # Spaltenmittelwerte (1. Zahl = Mittelwert der Depressivitaet, 2. Zahl = Mittelwert der Lebenszufriedenheit)</code></pre>
<pre><code>## [1] 5.655556 6.400000</code></pre>
<pre class="r"><code>cov(X) # Kovarianzmatrix von Depressivitaet und Lebenszufriedenheit</code></pre>
<pre><code>##           [,1]      [,2]
## [1,]  3.127216 -1.287640
## [2,] -1.287640  2.377528</code></pre>
<pre class="r"><code>cor(X) # zum Vergleich: die Korrelationsmatrix (die Variablen scheinen mäßig zu korrelieren, was unbedingt in die Ausreißerdiagnostik involviert werden muss)</code></pre>
<pre><code>##            [,1]       [,2]
## [1,]  1.0000000 -0.4722292
## [2,] -0.4722292  1.0000000</code></pre>
<pre class="r"><code>MD &lt;- mahalanobis(x = X, center = colMeans(X), cov = cov(X))</code></pre>
<p>Zum bestimmen der kritischen Distanz nehmen wir die <span class="math inline">\(\chi^2\)</span> Verteilung heran. Wir bestimmen mit <code>qchisq</code> den kritischen Wert, wobei als <span class="math inline">\(p\)</span>-Wert hier meist ein <span class="math inline">\(\alpha\)</span>-Niveau von .01 oder .001 herangezogen wird, damit wir nicht fälschlicherweise zu viele Werte aussortieren. Hierbei übergeben wir dem Argument <code>p</code> das <span class="math inline">\(\alpha\)</span>-Niveau, <code>lower.tail = F</code> besagt, dass wir damit die obere Grenze meinen (also mit <code>p</code> gerade die Wahrscheinlichkeit meinen, einen extremeren Wert zu finden), <code>df = 2</code> stellt die Freiheitsgrade ein (hier = 2, da 2 Variablen):</p>
<pre class="r"><code>qchisq(p = .01, lower.tail = F, df = 2)    # alpha = 1%</code></pre>
<pre><code>## [1] 9.21034</code></pre>
<pre class="r"><code>qchisq(p = .001, lower.tail = F, df = 2)   # alpha = 0.1%</code></pre>
<pre><code>## [1] 13.81551</code></pre>
<p>Nun können wir die Mahalanobisdistanz untersuchen:</p>
<pre class="r"><code>MD</code></pre>
<pre><code>##  [1]  1.29835776  1.85988286  1.68027868  0.38036881  1.93061376  5.31999173
##  [7]  1.29835776  0.89484803  5.31999173  0.07401952  1.85988286  0.96581665
## [13]  2.87179069  1.29835776  3.08398338  6.11730810  0.96581665  0.33582974
## [19]  1.93061376  3.99622396  0.96581665  1.85988286  1.93061376  3.57704668
## [25]  0.07401952  0.96581665  1.68027868  0.88006651  1.85988286  1.85988286
## [31]  0.96581665  0.19639932  0.07401952  2.79831527  0.19639932  1.64697702
## [37]  2.42350418  0.89484803  0.96581665  0.19639932  9.85316591  0.89484803
## [43]  6.01723026  4.27802381  0.88006651  0.19639932  0.07401952  0.38036881
## [49]  8.84194961  1.64697702  0.19639932  1.29835776  2.87179069  4.92199266
## [55]  7.18339325  0.19639932  0.33582974  0.38036881  3.46236019  0.07401952
## [61]  0.33582974  0.38036881  3.48368968  8.46422112  0.19639932  0.88006651
## [67]  0.07401952  0.88006651  1.09506856  1.50981570  0.38036881  0.07401952
## [73]  1.64697702  0.33582974  1.09506856  0.88006651  2.86848429  3.08398338
## [79]  2.86848429  1.09506856  2.39394112  1.50981570  1.09506856  2.42350418
## [85] 10.28690861  1.33295604  0.19639932  0.88006651  1.68027868  1.09506856</code></pre>
<p>Hier alle Werte durch zugehen ist etwas lästig. Natürlich können wir den Vergleich mit den kritischen Werten auch automatisieren und z.B. uns nur diejenigen Mahalanobisdistanzwerte ansehen, die größer als der kritische Wert zum <span class="math inline">\(\alpha\)</span>-Niveau von 1% sind. Wenn wir den <code>which</code> Befehl nutzen, so erhalten wir auch noch die Probandennummer der möglichen Ausreißer.</p>
<pre class="r"><code>MD[MD &gt; qchisq(p = .01, lower.tail = F, df = 2)]      # Mahalanobiswerte &gt; krit. Wert</code></pre>
<pre><code>## [1]  9.853166 10.286909</code></pre>
<pre class="r"><code>which(MD &gt; qchisq(p = .01, lower.tail = F, df = 2))   # Pbn-Nr. 1%</code></pre>
<pre><code>## [1] 41 85</code></pre>
<pre class="r"><code>which(MD &gt; qchisq(p = .001, lower.tail = F, df = 2))   # Pbn-Nr. 0.1%</code></pre>
<pre><code>## integer(0)</code></pre>
<p>Auf dem <span class="math inline">\(\alpha\)</span>-Niveau von 1% gäbe es 2 Ausreißer (Pbn-Nr = 41, 85), auf dem von 0.1% keinen.</p>
<p>Jetzt nochmal zurück zum zweiten Einsatz der Werte - der Prüfung der multivariaten Normalverteilung. Die Verteilung der Mahalanobisdistanz widerspricht nicht (zu sehr) der Annahme auf multivariate Normalverteilung, da das Histogramm einigermaßen zur Dichte der <span class="math inline">\(\chi^2(df=2)\)</span>-Verteilung passt:</p>
<pre class="r"><code>hist(MD, freq = F, breaks = 15)
xWerte &lt;- seq(from = min(MD), to = max(MD), by = 0.01)
lines(x = xWerte, y = dchisq(x = xWerte, df = 2), lwd = 3)</code></pre>
<p><img src="/post/2021-10-11-Regression_Ausreisser_KliPPs_files/figure-html/unnamed-chunk-30-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>qqPlot(x = MD,distribution =  &quot;chisq&quot;, df = 2, pch = 16)</code></pre>
<p><img src="/post/2021-10-11-Regression_Ausreisser_KliPPs_files/figure-html/unnamed-chunk-30-2.png" width="672" style="display: block; margin: auto;" /></p>
<pre><code>## [1] 85 41</code></pre>
<p>Gleiches gilt auch für den Q-Q-Plot, der hier ebenfalls gegen die <span class="math inline">\(\chi^2(df=2)\)</span>-Verteilung abgetragen wurde. Der Q-Q-Plot hat außerdem als Output die Fallnummer (Pbn-Nr) der extremeren Werte - hier ebenfalls 41 und 85. Insgesamt ist also zu sagen, dass die Mahalanobisdistanz nicht der multivariaten Normalverteilungsannahme widerspricht.</p>
<hr />
</div>
</div>
<div id="r-skript" class="section level2">
<h2>R-Skript</h2>
<p>Den gesamten <code>R</code>-Code, der in dieser Sitzung genutzt wird, können Sie <a href="/post/MSc1_R_Files/1_Regression_RCode.R"><svg aria-hidden="true" role="img" viewBox="0 0 512 512" style="height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;"><path d="M216 0h80c13.3 0 24 10.7 24 24v168h87.7c17.8 0 26.7 21.5 14.1 34.1L269.7 378.3c-7.5 7.5-19.8 7.5-27.3 0L90.1 226.1c-12.6-12.6-3.7-34.1 14.1-34.1H192V24c0-13.3 10.7-24 24-24zm296 376v112c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V376c0-13.3 10.7-24 24-24h146.7l49 49c20.1 20.1 52.5 20.1 72.6 0l49-49H488c13.3 0 24 10.7 24 24zm-124 88c0-11-9-20-20-20s-20 9-20 20 9 20 20 20 20-9 20-20zm64 0c0-11-9-20-20-20s-20 9-20 20 9 20 20 20 20-9 20-20z"/></svg> hier herunterladen</a>.</p>
<hr />
</div>
<div id="appendix" class="section level2">
<h2>Appendix</h2>
<div id="AppendixA" class="section level3">
<h3>Appendix A</h3>
<details>
<summary>
<strong>Regressionsmodell</strong>
</summary>
<p>Folgende Befehle führen zum gleichen Ergebnis wie:</p>
<pre class="r"><code>lm(Depressivitaet ~ 1 + Geschlecht + Lebenszufriedenheit, data = Depression)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Depressivitaet ~ 1 + Geschlecht + Lebenszufriedenheit, 
##     data = Depression)
## 
## Coefficients:
##         (Intercept)   Geschlechtweiblich  Lebenszufriedenheit  
##              7.2353               1.9117              -0.3663</code></pre>
<p>Das Interzept kann explizit mit angegeben werden (falls Sie <code>0 +</code> schreiben, setzen Sie das Interzept auf 0, was sich entsprechend auf die Parameterschätzungen auswirken wird, falls das Interzept von 0 verschieden ist!):</p>
<pre class="r"><code>lm(Depressivitaet ~ 0 + Geschlecht + Lebenszufriedenheit, data = Depression)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Depressivitaet ~ 0 + Geschlecht + Lebenszufriedenheit, 
##     data = Depression)
## 
## Coefficients:
## Geschlechtmaennlich   Geschlechtweiblich  Lebenszufriedenheit  
##              7.2353               9.1470              -0.3663</code></pre>
<p>Dem Output ist zu entnehmen, dass die Parameterschätzungen sich drastisch geändert haben!</p>
<p>Lassen wir das Interzept in der Schreibweise weg, so wird es per Default mitgeschätzt.</p>
<pre class="r"><code>lm(Depressivitaet ~ Geschlecht + Lebenszufriedenheit, data = Depression)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Depressivitaet ~ Geschlecht + Lebenszufriedenheit, 
##     data = Depression)
## 
## Coefficients:
##         (Intercept)   Geschlechtweiblich  Lebenszufriedenheit  
##              7.2353               1.9117              -0.3663</code></pre>
<p>Mit <code>formula</code> benutzen wir nicht die Position in der Funktion, sondern das Argument für die Formel:</p>
<pre class="r"><code>lm(formula = 1 + Depressivitaet ~ Geschlecht + Lebenszufriedenheit, data = Depression) </code></pre>
<pre><code>## 
## Call:
## lm(formula = 1 + Depressivitaet ~ Geschlecht + Lebenszufriedenheit, 
##     data = Depression)
## 
## Coefficients:
##         (Intercept)   Geschlechtweiblich  Lebenszufriedenheit  
##              8.2353               1.9117              -0.3663</code></pre>
<p>Wir können also auch einfach die Reihenfolge umdrehen, solange wir Argumente benutzen:</p>
<pre class="r"><code>lm(data = Depression, formula = 1 + Depressivitaet ~ Geschlecht + Lebenszufriedenheit) </code></pre>
<pre><code>## 
## Call:
## lm(formula = 1 + Depressivitaet ~ Geschlecht + Lebenszufriedenheit, 
##     data = Depression)
## 
## Coefficients:
##         (Intercept)   Geschlechtweiblich  Lebenszufriedenheit  
##              8.2353               1.9117              -0.3663</code></pre>
<p>Die Formel kann auch in Anführungszeichen geschrieben werden:</p>
<pre class="r"><code>lm(&quot;Depressivitaet ~ 1 + Geschlecht + Lebenszufriedenheit&quot;, data = Depression) </code></pre>
<pre><code>## 
## Call:
## lm(formula = &quot;Depressivitaet ~ 1 + Geschlecht + Lebenszufriedenheit&quot;, 
##     data = Depression)
## 
## Coefficients:
##         (Intercept)   Geschlechtweiblich  Lebenszufriedenheit  
##              7.2353               1.9117              -0.3663</code></pre>
<p>Wir können auf die Datensatzspezifizierung verzichten, indem wir die Variablen direkt ansprechen (es ändern sich entsprechend die Namen der Koeffizienten im Output):</p>
<pre class="r"><code>lm(Depression$Depressivitaet ~ 1 + Depression$Geschlecht + Depression$Lebenszufriedenheit) </code></pre>
<pre><code>## 
## Call:
## lm(formula = Depression$Depressivitaet ~ 1 + Depression$Geschlecht + 
##     Depression$Lebenszufriedenheit)
## 
## Coefficients:
##                    (Intercept)   Depression$Geschlechtweiblich  
##                         7.2353                          1.9117  
## Depression$Lebenszufriedenheit  
##                        -0.3663</code></pre>
<p>Wir können auch neue Variablen definieren, um diese dann direkt anzusprechen (es ändern sich entsprechend die Namen der Koeffizienten):</p>
<pre class="r"><code>AV &lt;- Depression$Depressivitaet
UV1 &lt;- Depression$Geschlecht
UV2 &lt;- Depression$Lebenszufriedenheit

lm(AV ~ 1 + UV1 + UV2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = AV ~ 1 + UV1 + UV2)
## 
## Coefficients:
## (Intercept)  UV1weiblich          UV2  
##      7.2353       1.9117      -0.3663</code></pre>
<p>Selbstverständlich gibt es auch noch weitere Befehle, die zum selben Ergebnis kommen. Sie sehen, dass Sie in <code>R</code> in vielen Bereichen mit leicht unterschiedlichem Code zum selben Ergebnis gelangen!</p>
</details>
</div>
<div id="AppendixB" class="section level3">
<h3>Appendix B</h3>
<details>
<summary>
<strong>Grafiken und ggplot2</strong>
</summary>
<p>Im folgenden Block sehen wir den Code für ein Histogramm in <code>ggplot2</code>-Notation (das Paket muss natürlich installiert sein: <code>install.packages(ggplot2)</code>). Hier sind einige Zusatzeinstellungen gewählt, die das Histogramm optisch aufbereiten.</p>
<div id="hat-values" class="section level4">
<h4>Hat-Values</h4>
<pre class="r"><code>n &lt;- length(residuals(model)) #Anzahl Personen
h &lt;- hatvalues(model) # Hebelwerte
library(ggplot2)</code></pre>
<pre><code>## Warning: Paket &#39;ggplot2&#39; wurde unter R Version 4.1.1 erstellt</code></pre>
<pre class="r"><code>df_h &lt;- data.frame(h) # als Data.Frame für ggplot
ggplot(data = df_h, aes(x = h)) + 
     geom_histogram(aes(y =..density..),  
                    bins = 15,             # Wie viele Balken sollen gezeichnet werden?
                     colour = &quot;blue&quot;,              # Welche Farbe sollen die Linien der Balken haben?
                    fill = &quot;skyblue&quot;) +           # Wie sollen die Balken gefüllt sein?
  geom_vline(xintercept = 4/n, col = &quot;red&quot;)+ # Cut-off bei 4/n
  labs(title = &quot;Histogramm der Hebelwerte&quot;, x = &quot;Hebelwerte&quot;) # Füge eigenen Titel und Achsenbeschriftung hinzu</code></pre>
<p><img src="/post/2021-10-11-Regression_Ausreisser_KliPPs_files/figure-html/unnamed-chunk-39-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="cooks-distanz-1" class="section level4">
<h4>Cooks-Distanz:</h4>
<p>Hier nochmal der Code für das Histogramm der Cooks-Distanz ohne die Styling-Elemente.</p>
<pre class="r"><code># Cooks Distanz
CD &lt;- cooks.distance(model) # Cooks Distanz
df_CD &lt;- data.frame(CD) # als Data.Frame für ggplot
ggplot(data = df_CD, aes(x = CD)) + 
     geom_histogram(aes(y =..density..),  bins = 15)+
  geom_vline(xintercept = 1, col = &quot;red&quot;) # Cut-Off bei 1</code></pre>
<p><img src="/post/2021-10-11-Regression_Ausreisser_KliPPs_files/figure-html/unnamed-chunk-40-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Hier finden Sie außerdem den Code zu den vier Grafiken, die den Einfluss von Hebelwerten und der Cooks’s Distanz dargestellt haben.</p>
<pre class="r"><code>par(mfrow=c(2,2),cex.axis = 1.1, cex.lab= 1.2, cex.main = 1.3, mar = c(5, 5, 2, 1),
    bty=&quot;n&quot;,bg=&quot;white&quot;, mgp=c(2, 0.8, 0))

library(car)
X &lt;- sort(rnorm(25))
y &lt;- X + rnorm(25, sd = 0.3) 
reg &lt;- lm(y~X)


X_ &lt;- c(X, 0)
y_ &lt;- c(y, 0 + 0 + rnorm(1, sd = 0.3))
reg1 &lt;- lm(y_ ~ X_)

plot(X_,y_, pch = 16, main = &quot;A)  kleine CD, kleiner Hebelwert&quot;, xlab = &quot;X&quot;, ylab = &quot;Y&quot;, xlim = c(-2,4), ylim = c(-2, 4))
abline(reg = reg, lwd = 3)
legend(x=&quot;topleft&quot;, legend = c(&quot;normal&quot;, &quot;outlier&quot;), col = c(&quot;black&quot;, &quot;darkblue&quot;), pch = 16, cex = 1.1, box.col = &quot;grey&quot;)



X_ &lt;- c(X, 4)
y_ &lt;- c(y, 3.7 + rnorm(1, sd = 0.3))
reg1 &lt;- lm(y_ ~ X_)

plot(X_,y_, pch = 16, main = &quot;B)  kleine CD, großer Hebelwert&quot;, xlab = &quot;X&quot;, ylab = &quot;Y&quot;, xlim = c(-2,4), ylim = c(-2, 4))
abline(reg = reg, lwd = 3)
abline(reg = reg1, lwd = 5, col = &quot;blue&quot;)
points(X_[length(X)+1], y_[length(X)+1], pch = 15, cex = 2.8, col = &quot;gold&quot;)
points(X_[length(X)+1], y_[length(X)+1], pch = 16, cex = 2, col = &quot;darkblue&quot;)

legend(x=&quot;topleft&quot;, legend = c(&quot;normal&quot;, &quot;outlier&quot;), col = c(&quot;black&quot;, &quot;darkblue&quot;), pch = 16, cex = 1.1, box.col = &quot;grey&quot;)


X_ &lt;- c(X, 0)
y_ &lt;- c(y, 0 + 3.7 + rnorm(1, sd = 0.3))
reg1 &lt;- lm(y_ ~ X_)

plot(X_,y_, pch = 16, main = &quot;C)  große CD, kleiner Hebelwert&quot;, xlab = &quot;X&quot;, ylab = &quot;Y&quot;, xlim = c(-2,4), ylim = c(-2, 4))
abline(reg = reg, lwd = 3)
abline(reg = reg1, lwd = 5, col = &quot;blue&quot;)
legend(x=&quot;topleft&quot;, legend = c(&quot;normal&quot;, &quot;outlier&quot;), col = c(&quot;black&quot;, &quot;darkblue&quot;), pch = 16, cex = 1.1, box.col = &quot;grey&quot;)
points(X_[length(X)+1], y_[length(X)+1], pch = 15, cex = 2.8, col = &quot;gold&quot;)
points(X_[length(X)+1], y_[length(X)+1], pch = 16, cex = 2, col = &quot;darkblue&quot;)


X_ &lt;- c(X, 4)
y_ &lt;- c(y, 0  + rnorm(1, sd = 0.3))
reg1 &lt;- lm(y_ ~ X_)

plot(X_,y_, pch = 16, main = &quot;D)  große CD, großer Hebelwert&quot;, xlab = &quot;X&quot;, ylab = &quot;Y&quot;, xlim = c(-2,4), ylim = c(-2, 4))
abline(reg = reg, lwd = 3)
abline(reg = reg1, lwd = 5, col = &quot;blue&quot;)
legend(x=&quot;topleft&quot;, legend = c(&quot;normal&quot;, &quot;outlier&quot;), col = c(&quot;black&quot;, &quot;darkblue&quot;), pch = 16, cex = 1.1, box.col = &quot;grey&quot;)
points(X_[length(X)+1], y_[length(X)+1], pch = 15, cex = 2.8, col = &quot;gold&quot;)
points(X_[length(X)+1], y_[length(X)+1], pch = 16, cex = 2, col = &quot;darkblue&quot;)</code></pre>
<p><img src="/post/2021-10-11-Regression_Ausreisser_KliPPs_files/figure-html/unnamed-chunk-41-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="ellipse" class="section level4">
<h4>Ellipse</h4>
<p>Auch der Code für die Grafik, die zur Veranschaulichung der Mahalanobisdistanz verwendet wurde, ist hier nochmal einsehbar:</p>
<pre class="r"><code>library(ellipse)
mu1 &lt;- c(0,0)
mu2 &lt;- c(1,0)
S1 &lt;- matrix(c(1,1,1,4),2,2)
#S2 &lt;- matrix(c(4,0,0,1),2,2)
plot(0, col = &quot;white&quot;, xlim = c(-3,3.5),ylim = c(-6,6), xlab = expression(X[1]), ylab = expression(X[2]), 
     main = &quot;Kurven gleicher Wahrscheinlichkeit/\n Kurven gleicher Mahalanobisdistanz&quot;)

points(mu1[1],mu1[2],pch=19,col=&quot;green&quot;, cex = 3)
#points(mu2[1],mu2[2],pch=19, col = &quot;blue&quot;, cex = 3)
# plotte einzelne Kovarianzmatrizen
color &lt;- c(&quot;yellow&quot;,&quot;gold3&quot;, &quot;gold3&quot;, &quot;red&quot;)
i &lt;- 1
for (q in c(0.5, 0.8,0.95,.99))
{
  lines(ellipse(S1,level=q)[,1]+mu1[1],ellipse(S1,level=q)[,2]+mu1[2], col = color[i], lwd = 4) 
  #lines(ellipse(S2,level=q)[,1]+mu2[1],ellipse(S2,level=q)[,2]+mu2[2],col= color [i], lwd = 4)
  i &lt;- i +1 
}

X &lt;- ellipse(S1,level=0.8)[,1]+mu1[1]
Y &lt;- ellipse(S1,level=0.8)[,2]+mu1[2]

i &lt;- 25
lines(c(X[i], 0), c(Y[i], 0), lwd = 3, col = &quot;blue&quot;)
points(X[i],Y[i], cex = 2, pch = 16)

l &lt;- sqrt(X[i]^2 + Y[i]^2)

i &lt;- 1
l2 &lt;-  sqrt(X[i]^2 + Y[i]^2)
lines(c(0, X[i]), c(0, Y[i]), lwd = 3)

arrows(x0=0,x1= X[i]*l/l2, y0=0,y1= Y[i]*l/l2, lwd = 3, col = &quot;blue&quot;, code = 3, angle = 90, length = 0.1)

points(X[i],Y[i], cex = 2, pch = 16)
points(mu1[1],mu1[2],pch=19,col=&quot;green&quot;, cex = 3)</code></pre>
<p><img src="/post/2021-10-11-Regression_Ausreisser_KliPPs_files/figure-html/unnamed-chunk-42-1.png" width="672" style="display: block; margin: auto;" /></p>
</details>
<hr />
</div>
</div>
</div>
<div id="literatur" class="section level2">
<h2>Literatur</h2>
<p><a href="https://hds.hebis.de/ubffm/Record/HEB369761391">Agresti, A, &amp; Finlay, B. (2013).</a> <em>Statistical methods for the social sciences.</em> (Pearson new international edition, 4th edition). Harlow, Essex : Pearson Education Limited.</p>
<p><a href="https://hds.hebis.de/ubffm/Record/HEB366849158">Eid, M., Gollwitzer, M., &amp; Schmitt, M. (2017).</a> <em>Statistik und Forschungsmethoden</em> (5. Auflage, 1. Auflage: 2010). Weinheim: Beltz.</p>
<p><a href="https://hds.hebis.de/ubffm/Record/HEB371183324">Pituch, K. A. &amp; Stevens, J. P. (2016).</a> <em>Applied Multivariate Statistics for the Social Sciences</em> (6th ed.). New York: Taylor &amp; Francis.</p>
<ul>
<li><small> <em>Blau hinterlegte Autorenangaben führen Sie direkt zur universitätsinternen Ressource.</em> </small></li>
</ul>
</div>
