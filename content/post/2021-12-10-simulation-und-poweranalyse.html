---
title: Simulation und Poweranalyse
author: 
date: '2021-09-21'
slug: simulation
categories:
  - BSc2
tags:
  - Simulation
  - Poweranalyse
  - Type I-Error
  - Alpha-Fehler
  - Fehler erster Art
  - Testmacht
  - Teststärke
subtitle: ''
summary: ''
authors: [irmer, nehler]
lastmod: '2023-02-02T13:10:00+01:00'
featured: no
header:
  image: "/header/BSc2_Sim_Power.jpg"
  caption: "[Courtesy of pxhere](https://pxhere.com/en/photo/674621)"
projects: []

---



<details>
<summary>
Kernfragen dieser Lehreinheit
</summary>
<ul>
<li>Wie können Variablen und ganze Modelle simuliert werden?</li>
<li>Wie lassen sich der <span class="math inline">\(\alpha\)</span>-Fehler (Type I-Error, Fehler erster Art) und die Power (Testmacht, Teststärke) empirisch bestimmen?</li>
<li>Welche anderen Möglichkeiten, den <span class="math inline">\(\alpha\)</span>-Fehler und die Power zu bestimmen, gibt es?</li>
<li>Wie lassen sich Power-Plots erstellen und was bedeuten sie?</li>
</ul>
</details>
<hr />
<div id="Einleitung" class="section level2">
<h2>Einleitung</h2>
<p>In den vergangenen Sitzungen haben wir verschiedene Tests für unterschiedliche Fragestellungen kennengelernt: <span class="math inline">\(t\)</span>-Test und Wilcoxon-Test für Mittelwertsvergleiche zweier Gruppen sowie den Korrelationstest, um die Beziehung zweier Variablen zu untersuchen. Wie fast alle statistischen Tests folgen die erwähnten Tests einer gewissen Logik. Unter der Annahme der Nullhypothese <span class="math inline">\(H_0\)</span> und einiger weiterer Zusatzannahmen (z.B. bezüglich der Verteilung des Merkmals in der Population) folgt die jeweilige Teststatistik einer (mathematisch) herleitbaren Verteilung. Für den <span class="math inline">\(t\)</span>-Test für unabhängige Stichproben ist es z.B. die <span class="math inline">\(t\)</span>-Verteilung mit <span class="math inline">\(n - 2\)</span> Freiheitsgraden. Anhand der entsprechenden Verteilungsfunktion können wir die Wahrscheinlichkeit der empirischen Daten gegeben der Annahmen bestimmen und so darüber entscheiden, ob die Nullhypothese verworfen wird.</p>
<p>Dazu können wir ein <span class="math inline">\(\alpha\)</span>-Fehlerniveau festlegen und die Nullhypothese verwerfen, wenn das Zustandekommen unserer Daten unter diesen Annahmen unwahrscheinlicher ist, als diese “akzeptable Irrturmswahrscheinlichkeit”. Wenn dem nicht so ist, behalten wir die Nullhypothese bei. Etwas formaler:</p>
<p><span class="math display">\[\begin{align}
p &amp;&lt; \alpha\ (=5\%) \Longrightarrow \neg H_0 \implies H_1 \\
p &amp;\ge \alpha\ (=5\%) \Longrightarrow H_0
\end{align}\]</span></p>
<p>Äquivalent dazu können wir das Quantil der Verteilung bestimmen, ab dem Werte eine Wahrscheinlichkeit kleiner als <span class="math inline">\(\alpha\)</span> haben (den kritischen Wert) und unsere Teststatistik (den empirischen Wert) damit:</p>
<p><span class="math display">\[\begin{align}
|t_\text{emp.}| &amp;&gt; t_\text{krit.} \Longrightarrow \neg H_0 \implies H_1 \\
|t_\text{emp.}| &amp;\le t_\text{krit.} \Longrightarrow H_0
\end{align}\]</span></p>
<p>Beide Ansätze kommen zum identischen Ergebnis, weil Quantil und <span class="math inline">\(p\)</span>-Wert einfache Übersetzungen voneinander sind. In R können wir, wie <a href="/post/verteilungen">bereits gesehen</a>, mit <code>p*</code> und <code>q*</code> für verschiedene Verteilungen diese Übersetzung vornehmen.</p>
<p>Gilt die Null-Hypothese, so sollte der Test in nur höchstens <span class="math inline">\(\alpha=5\%\)</span> der Fälle ein signifikantes Ergebnis anzeigen. Die Aussage beschreibt eigentlich ein Gedankenexperiment:</p>
<blockquote>
<p>Wenn wir das gleiche Experiment unendlich häufig und unabhängig voneinander wiederholen könnten und dabei unendlich häufig aus einer Population ziehen würden, in welcher es <em>keinen</em> Effekt gibt, dann sollte in höchstens <span class="math inline">\(5\%\)</span> der Wiederholungen (auch Replikationen genannt) rein durch Zufall ein signifikantes Ergebnis beim Durchführen des inferenzstatistischen Test herauskommen.</p>
</blockquote>
<p>Andersherum sollte ein inferenzstatistischer Test die <span class="math inline">\(H_0\)</span> möglichst häufig verwerfen, wenn die <span class="math inline">\(H_0\)</span> tatsächlich nicht in der Population gilt und es somit einen bedeutsamen Effekt gibt. Die Wahrscheinlichkeit, dass wird die <span class="math inline">\(H_0\)</span> nicht verwerfen, obwohl sie falsch ist, wird <span class="math inline">\(\beta\)</span>-Fehler genannt. Die Gegenwahrscheinlichkeit <span class="math inline">\(1-\beta\)</span> (also die Wahrscheinlichkeit, die <span class="math inline">\(H_0\)</span> korrekterweise zu verwerfen, weil in Wahrheit <span class="math inline">\(H_1\)</span> gilt) nennen wir die Power oder Teststärke. Sie sollte möglichst hoch sein. Es gibt verschiedene Richtlinien dazu, was “ausreichend” Power darstellt - immer in Abhängigkeit davon, welche Art von Fehlern (<span class="math inline">\(\alpha\)</span> oder <span class="math inline">\(\beta\)</span>) schlimmer ist. In vielen psychologischen Studien wird eine Power von <span class="math inline">\(80\%\)</span> als Richtwert genutzt.</p>
<p>Zurück zu unserem Gedankenexperiment:</p>
<blockquote>
<p>Eine Power von <span class="math inline">\(1-\beta = 80\%\)</span> bedeutet, wenn wir das gleiche Experiment unendlich häufig und unabhängig von einander wiederholen könnten und dabei unendlich häufig aus einer Population ziehen würden, in welcher es einen Effekt gibt, dann sollte in mindestens <span class="math inline">\(80\%\)</span> der Fälle ein signifikantes Ergebnis beim Durchführen des inferenzstatistischen Test herauskommen.</p>
</blockquote>
<p>Im Folgenden wollen wir dieses Gedankenexperiment in die Tat umsetzen und unsere Kenntnisse über das Simulieren von Zufallszahlen verwenden, um die Power und den Fehler 1. Art (<span class="math inline">\(\alpha\)</span>-Fehler) empirisch zu prüfen. Hierbei beschränken wir uns auf den <span class="math inline">\(t\)</span>-Test (mit der Funktion <code>t.test()</code>) und den Korrelationstest (mit der Funktion <code>cor.test()</code>). Da wir nun also Daten simulieren brauchen wir in diesem Tutorial den Datensatz nicht.</p>
</div>
<div id="simulation-und-alpha-fehler" class="section level2">
<h2>Simulation und <span class="math inline">\(\alpha\)</span>-Fehler</h2>
<p>Sie haben nun die Möglichkeit, in einen möglichen Forschungsbereich von Methodiker:innen hineinzublicken: Simulationsstudien. Um das beschriebene Gedankenexperiment umzusetzen, müssen wir sehr häufig Stichproben aus der Population ziehen und unseren Test (z.B. einen Mittelwertsvergleich) durchführen. Die Realität hat dabei zwei grundlegende Probleme:</p>
<ol style="list-style-type: decimal">
<li>Wir wissen nicht, ob in der Realität die <span class="math inline">\(H_0\)</span> oder <span class="math inline">\(H_1\)</span> gilt.</li>
<li>Um zuverlässige Wahrscheinlichkeitsaussagen zu machen müssen wir die Untersuchung <em>sehr häufig</em> wiederholen, was in den allermeisten Fällen (in echten empirischen) nicht praktikabel ist.</li>
</ol>
<p>Daher behelfen wir uns in methodologischer Forschung mit Simulationsstudien.</p>
<div id="mittelwertsvergleiche-t-test-unter-h_0" class="section level3">
<h3>Mittelwertsvergleiche: <span class="math inline">\(t\)</span>-Test unter <span class="math inline">\(H_0\)</span></h3>
<p>Der (klassische) <span class="math inline">\(t\)</span>-Test hat folgende Voraussetzungen:</p>
<ul>
<li>die Erhebungen sind voneinander unabhängig</li>
<li>die Varianzen in den beiden Gruppen sind gleich groß</li>
<li>die Variable ist in den Populationen normalverteilt</li>
</ul>
<p>Wir simulieren nun eine Variable <span class="math inline">\(X\)</span> für zwei Gruppen mit jeweils <span class="math inline">\(N=20\)</span>. Dabei nehmen wir an, dass in beiden Gruppen die Werte standardnormalverteilt sind (also einer Normalverteilung mit einem Mittelwert von 0 und einer Standardabweichung von 1 folgen). Für die Simulation von normalverteilten Werten haben wir die Funktion <code>rnorm()</code> kennengelernt. Wie im <a href="/post/verteilungen">Beitrag zu Verteilungen</a> besprochen, können wir mit <code>set.seed()</code> die Analysen replizierbar machen. Das bedeutet, dass eure Werte mit den hier stehenden Werten übereinstimmen sollten.</p>
<pre class="r"><code>N &lt;- 20
set.seed(1234)
X_1 &lt;- rnorm(N)
X_2 &lt;- rnorm(N)</code></pre>
<p>Die Werte der ersten Gruppe legen wir in dem Objekt <code>X_1</code> ab, die Werte der zweiten Gruppe im Objekt <code>X_2</code>. Dadurch dass wir die Werte für beide Gruppen getrennt simuliert haben, sind die Werte voneinander unabhängig (erste Voraussetzung erfüllt). Dadurch, dass wir mit <code>rnorm()</code> arbeiten, machen wir außerdem explizit, dass wir Werte erzeugen, die (in der Population) der Normalverteilung folgen (dritte Voraussetzung erfüllt). Dadurch, dass wir in <code>rnorm()</code> außerdem für beide Variablen die Voreinstellung <code>sd = 1</code> für die Standardabweichung nutzen, ist auch die zweite Voraussetzung (Homoskedastizität) erfüllt.</p>
<p>Zusätzlich haben wir in beiden Fällen auch die Voreinstellung <code>mean = 0</code> für den Mittelwert benutzt. Dadurch gilt <span class="math inline">\(\mu_{X1}=\mu_{X2} = 0\)</span> für die Verteilungen aus denen beide Stichproben gezogen wurden. Da es sich hier allerdings um eine Zufallsziehung handelt, sind die Mittelwerte der beiden Gruppen natürlich nicht exakt 0:</p>
<pre class="r"><code>mean(X_1)</code></pre>
<pre><code>## [1] -0.2506641</code></pre>
<pre class="r"><code>mean(X_2)</code></pre>
<pre><code>## [1] -0.5770699</code></pre>
<p>Sie weichen (zufällig) von der 0 ab. Diese Abweichung wird auch Samplevariation (Stichprobenschwankung) genannt.</p>
<p>Wir können nun mithilfe des <span class="math inline">\(t\)</span>-Tests untersuchen, ob die beiden Variablen den gleichen Mittelwert haben (da wir den <span class="math inline">\(t\)</span>-Test unter Annahme der Varianzhomogenität durchführen wollen, müssen wir <code>var.equal = TRUE</code> wählen, da sonst die Variante mit Welch-Korrektur gerechnet wird):</p>
<pre class="r"><code>ttestH0 &lt;- t.test(X_1, X_2, var.equal = TRUE)
ttestH0</code></pre>
<pre><code>## 
## 	Two Sample t-test
## 
## data:  X_1 and X_2
## t = 1.1349, df = 38, p-value = 0.2635
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -0.2558241  0.9086358
## sample estimates:
##  mean of x  mean of y 
## -0.2506641 -0.5770699</code></pre>
<pre class="r"><code>ttestH0$statistic # t-Wert</code></pre>
<pre><code>##        t 
## 1.134902</code></pre>
<pre class="r"><code>ttestH0$p.value   # zugehöriger p-Wert</code></pre>
<pre><code>## [1] 0.2635244</code></pre>
<p>Die Mittelwertsdifferenz liegt bei 0.3264, was einen <span class="math inline">\(t_\text{emp}\)</span>-Wert von $t_ =$1.1349 ergibt. Der zugehörige <span class="math inline">\(p\)</span>-Wert liegt bei $p=$0.2635. Somit ist diese Mittelwertsdifferenz auf einem <span class="math inline">\(\alpha\)</span>-Niveau von <span class="math inline">\(5\%\)</span> nicht statistisch bedeutsam. Die Nullyhpothese wird also nicht verworfen, was in diesem Fall die richtige Entscheidung ist, da in der Population ja die Gleichheit der Mittelwerte der beiden Gruppen gilt.</p>
<p>Wir sind an dieser Stelle aber nicht daran interessiert, wie der <span class="math inline">\(p\)</span>-Wert in diesem spezifischen Experiment (mit Seed = 1234) ausfällt, sondern wir möchten wissen, ob die Nullhypothese in ca. <span class="math inline">\(5\%\)</span> der Fälle verworfen wird, wenn wir das Experiment häufig wiederholen. Dafür könnten wir den oben gezeigten Code immer wieder ausführen und den <span class="math inline">\(p\)</span>-Wert notieren. Das erscheint sehr lästig. Wir können hier bspw. die <code>replicate()</code>-Funktion verwenden. Mit dieser Funktion wird irgendein Stück <code>R</code>-Code (eine “Expression”, daher der Namen des Arguments: <code>expr</code>) <code>n</code>-mal wiederholt. Wenn wir z.B. 5 mal drei Werte aus einer Normalverteilung ziehen wollen, kann das Ganze so aussehen:</p>
<pre class="r"><code>replicate(n = 5, expr = rnorm(3))</code></pre>
<pre><code>##            [,1]       [,2]       [,3]       [,4]       [,5]
## [1,]  1.4494963 -0.2806230 -1.1073182 -0.4968500 -1.1088896
## [2,] -1.0686427 -0.9943401 -1.2519859 -1.8060313 -1.0149620
## [3,] -0.8553646 -0.9685143 -0.5238281 -0.5820759 -0.1623095</code></pre>
<p><code>replicate()</code> kann aber nicht nur einzelne Funktionen, sondern auch mehrere Zeilen <code>R</code>-Code entgegennehmen, solange die in geschwungenen Klammern <code>{ ... }</code> angegeben werden. Wenn wir zwei unabhängige Variablen erstellen, mit einem <span class="math inline">\(t\)</span>-Test auf Mittelwertsgleichheit prüfen und uns den <span class="math inline">\(p\)</span>-Wert ausgeben lassen wollen, brauchen wir die folgenden vier Zeilen:</p>
<pre class="r"><code>X_1 &lt;- rnorm(N)
X_2 &lt;- rnorm(N)
ttestH0 &lt;- t.test(X_1, X_2, var.equal = TRUE)
ttestH0$p.value</code></pre>
<p><code>N</code> hatten wir oben bereits als 20 festegelegt. Mit <code>replicate()</code> können wir den Code beliebig häufig durchführen. Z.B. zunächst 10 mal:</p>
<pre class="r"><code>set.seed(1234)
replicate(n = 10, expr = {X_1 &lt;- rnorm(N)
                          X_2 &lt;- rnorm(N)
                          ttestH0 &lt;- t.test(X_1, X_2, var.equal = TRUE)
                          ttestH0$p.value})</code></pre>
<pre><code>##  [1] 0.26352442 0.03081077 0.21285027 0.27429670 0.53201656 0.79232864
##  [7] 0.93976306 0.43862992 0.96766599 0.68865560</code></pre>
<p>Uns werden insgesamt 10 <span class="math inline">\(p\)</span>-Werte übergeben. Wenn wir genau hinsehen, dann erkennen wir den ersten <span class="math inline">\(p\)</span>-Wert wieder. Dies ist der <span class="math inline">\(p\)</span>-Wert unseres Experiments weiter oben. Wiederholen wir nun das Experiment nicht nur 10 Mal, sondern 10000 Mal, dann erhalten wir eine gute Übersicht über das Verhalten der <span class="math inline">\(p\)</span>-Werte unter den Bedingungen, die wir vorgegeben haben: Gültigkeit der Nullhypothese und Standardnormalverteilung der beiden von einander unabhängigen Variablen. Damit uns die 10000 Werte nicht einfach in die Konsole gedruckt werden, legen wir sie im Objekt <code>pt_H0</code> ab (für <span class="math inline">\(p\)</span>-Werte für den <span class="math inline">\(t\)</span>-Test unter der <span class="math inline">\(H_0\)</span>-Hypothese):</p>
<pre class="r"><code>set.seed(1234)
pt_H0 &lt;- replicate(n = 10000, expr = {X_1 &lt;- rnorm(N)
                                      X_2 &lt;- rnorm(N)
                                      ttestH0 &lt;- t.test(X_1, X_2, var.equal = TRUE)
                                      ttestH0$p.value})</code></pre>
<p>Schauen wir uns doch mal die Verteilung der <span class="math inline">\(p\)</span>-Werte an:</p>
<pre class="r"><code>hist(pt_H0, breaks = 20) </code></pre>
<p><img src="/post/2021-12-10-simulation-und-poweranalyse_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>Die <span class="math inline">\(p\)</span>-Werte erscheinen einigermaßen gleichverteilt auf dem Intervall [0,1]. Dies ist auch genau gewünscht. Unter der <span class="math inline">\(H_0\)</span>-Hypothese ist der <span class="math inline">\(p\)</span>-Wert uniform (gleichverteilt) auf dem Intervall [0,1]. Somit tritt jeder Wert mit der selben Wahrscheinlichkeit auf. Dies bedeutet gleichzeitig, dass in dem Intervall [0, 0.05] gerade <span class="math inline">\(5\%\)</span> der Fälle landen sollten. Dies ist gerade das Intervall, in dem die signifikanten Ergebnisse landen. Somit erkennen wir, dass unter <span class="math inline">\(H_0\)</span> die Nullhypothese nur in <span class="math inline">\(5\%\)</span> verworfen werden sollte, wenn wir uns ein <span class="math inline">\(\alpha\)</span>-Niveau von <span class="math inline">\(5\%\)</span> vorgeben. Wir können prüfen, ob dem so ist, indem wir den relativen Anteil bestimmen, in welchem die <span class="math inline">\(H_0\)</span> verworfen wird. Dies ist genau dann der Fall, wenn der <span class="math inline">\(p\)</span>-Wert kleiner als <span class="math inline">\(0.05\)</span> ist (siehe <a href="#Einleitung">Einleitung</a>). Die relative Häufigkeit bestimmen wir so:</p>
<pre class="r"><code>mean(pt_H0 &lt; 0.05)</code></pre>
<pre><code>## [1] 0.0484</code></pre>
<p>Somit wird die Nullhypothese hier in 4.84% der Fälle verworfen. Dies zeigt uns, dass der <span class="math inline">\(t\)</span>-Test unter <span class="math inline">\(H_0\)</span> für <span class="math inline">\(N=20\)</span> gut funktioniert, da die empirische Rate des Fehlers 1. Art bei ungefähr <span class="math inline">\(5\%\)</span> liegt. Wir könnten auch untersuchen, wie robust ein Test ist, indem wir eine Annahme verletzen (z.B. Varianzhomogenität) und untersuchen, wie sich das auf das empirische <span class="math inline">\(\alpha\)</span>-Niveau und die Verteilung der Teststatistik oder der <span class="math inline">\(p\)</span>-Werte ausübt.</p>
<p>Im Übrigen hätten wir auch die ganze Prozedur mithilfe der empirischen <span class="math inline">\(t\)</span>-Werte durchführen können (diese sind als <code>statistic</code> im Test-Objekt angelegt):</p>
<pre class="r"><code>set.seed(1234)
tt_H0 &lt;- replicate(n = 10000, expr = {X_1 &lt;- rnorm(N)
                                      X_2 &lt;- rnorm(N)
                                      ttestH0 &lt;- t.test(X_1, X_2, var.equal = TRUE)
                                      ttestH0$statistic})</code></pre>
<p>Wir können uns die Verteilung dieser Werte ansehen, um zu prüfen, ob die Werte annähernd <span class="math inline">\(t\)</span>-verteilt sind. Die Dichte erhalten wir mit <code>dt(x = x, df = 38)</code>, wobei <code>x</code> die gewünschte x-Koordinate ist:</p>
<pre class="r"><code>hist(tt_H0, breaks = 50, freq = FALSE) # freq = FALSE, damit relative Häufigkeiten eingetragen werden!
x &lt;- seq(-4, 4, 0.01) # Sequenz von -4 bis 4 in 0.01 Schritten
lines(x = x, y = dt(x = x, df = 38), lwd = 2) # lwd = Liniendicke</code></pre>
<p><img src="/post/2021-12-10-simulation-und-poweranalyse_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>Die empirischen <span class="math inline">\(t\)</span>-Werte können wir auch mit dem kritischen Wert abgleichen. Weil die <span class="math inline">\(t\)</span>-Verteilung symmetrisch ist, können wir hier zweiseitig testen, indem wir einfach die Beträge (<code>abs()</code>) der empirischen <span class="math inline">\(t\)</span>-Werte nutzen.</p>
<pre class="r"><code>t_krit &lt;- qt(p = .975, df = 38)
mean(abs(tt_H0) &gt; t_krit) # empirischer Alpha-Fehler</code></pre>
<pre><code>## [1] 0.0484</code></pre>
<p>Die Analyse kommt zum exakt gleichen Ergebnis. Das liegt daran, dass - wie oben bereits festgehalten - der <span class="math inline">\(p\)</span>-Wert und der <span class="math inline">\(t\)</span>-Wert ineinander überführbar sind. Das Histogramm zeigt uns außerdem die <span class="math inline">\(t\)</span>-Verteilung unter der Nullhypothese mit 38 Freiheitsgraden (<span class="math inline">\(N-2\)</span>). Die theoretische Kurve (mit <code>dt()</code>) passt sehr gut zum Histogramm!</p>
</div>
<div id="lineare-beziehungen-zwischen-variablen-korrelationstest-unter-h_0" class="section level3">
<h3>Lineare Beziehungen zwischen Variablen: Korrelationstest unter <span class="math inline">\(H_0\)</span></h3>
<p>Der (klassische) Korrelationstest hat fast die identischen Voraussetzungen wie der <span class="math inline">\(t\)</span>-Test:</p>
<ul>
<li>die Erhebungen sind voneinander unabhängig</li>
<li>die Varianzen in den beiden Gruppen sind gleich groß</li>
<li>die Variablen sind in der Population normalverteilt</li>
<li>die Variablen hängen linear zusammen</li>
</ul>
<p>Wie bei den vorherigen Berechnungen des t-Tests simulieren wirs uns wieder Daten. Die beiden Variablen, die wir gleich korrelieren wollen, nennen wir <code>Y</code> und <code>Z</code>.</p>
<pre class="r"><code>set.seed(1234)
Y &lt;- rnorm(N)
Z &lt;- rnorm(N)
cor(Y, Z) # empirische Korrelation</code></pre>
<pre><code>## [1] -0.2765719</code></pre>
<pre class="r"><code>cortestH0 &lt;- cor.test(Y, Z)
cortestH0$p.value # empirischer p-Wert</code></pre>
<pre><code>## [1] 0.2378304</code></pre>
<p>Die empirische Korrelation liegt bei -0.28. Die wahre Korrelation liegt bei 0, da <code>R</code> Zufallsvektoren unabhängig voneinander simuliert. Der <span class="math inline">\(p\)</span>-Wert des Korrelationstests liegt bei 0.2378. Damit ist das Ergebnis nicht statistisch bedeutsam. Die Korrelation von -0.28 ist zufällig aufgetreten. Wir wiederholen nun auch dieses Experiment:</p>
<pre class="r"><code>set.seed(1234)
pcor_H0 &lt;- replicate(n = 10000, expr = {Y &lt;- rnorm(N)
                                        Z &lt;- rnorm(N)
                                        cortestH0 &lt;- cor.test(Y, Z)
                                        cortestH0$p.value})</code></pre>
<p>Die <span class="math inline">\(p\)</span>-Werte sind wieder einigermaßen uniform auf [0,1] verteilt:</p>
<pre class="r"><code>hist(pcor_H0, breaks = 20) </code></pre>
<p><img src="/post/2021-12-10-simulation-und-poweranalyse_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>Das empirische <span class="math inline">\(\alpha\)</span>-Niveau liegt bei:</p>
<pre class="r"><code>mean(pcor_H0 &lt; 0.05)</code></pre>
<pre><code>## [1] 0.0481</code></pre>
<p>Somit wird die Nullhypothese hier in 4.81% der Fälle verworfen. Dies zeigt uns, dass auch der Korrelationstest unter <span class="math inline">\(H_0\)</span> für <span class="math inline">\(N=20\)</span> gut funktioniert, da die empirische Rate des Fehlers 1. Art bei ungefähr <span class="math inline">\(5\%\)</span> liegt.</p>
</div>
</div>
<div id="poweranalysen" class="section level2">
<h2>Poweranalysen</h2>
<p>Mit einer Power-Analyse untersuchen wir im Grunde, wie sich die Wahrscheinlichkeit die Nullhypothese zu verwerfen, verändert, je nachdem wie groß der Effekt ist. Dem wollen wir nun nachgehen und entsprechend unsere Daten unter der Alternativhypothese simulieren.</p>
<div id="mittelwertsvergleiche-t-test-unter-h_1" class="section level3">
<h3>Mittelwertsvergleiche: <span class="math inline">\(t\)</span>-Test unter <span class="math inline">\(H_1\)</span></h3>
<p>In der Inferenzstatistik gibt es im Grunde nicht “die” Alternativhypothese, sondern eine ganze Batterie an Alternativhypothesen. Beim ungerichteten Mittelwertsvergleich sieht die Alternativhypothese so aus:</p>
<p><span class="math display">\[H_1: \mu_{X1} \neq \mu_{X2},\]</span>
was natürlich äquivalent zur folgenden Aussage zur Differenz der beiden Mittelwerte ist: <span class="math inline">\(d=\mu_{X1}-\mu_{X2}:\)</span></p>
<p><span class="math display">\[H_1: d =  \mu_{X1}-\mu_{X2} \neq 0.\]</span></p>
<p>Hier wird <span class="math inline">\(d\)</span> als eine feste Zahl angenommen (z.B. 0.5, <span class="math inline">\(-\sqrt{2}\)</span>, <span class="math inline">\(\pi\)</span>, 123.456). Je größer der Effekt, desto größer ist die Wahrscheinlichkeit, dass dieser auch identifiziert wird. Hierbei wird die Größe des Effekts relativ zur zufälligen Streuung genommen. Da wir standardisierte (standardnormalverteilte) Variablen verwendet hatten, ist es so, dass die Mittelwertsdifferenz <span class="math inline">\(d\)</span> gerade in Vielfachen der Standardabweichung zu interpretieren ist (also Cohen’s <span class="math inline">\(d&#39;_2\)</span> darstellt). Bspw. wäre <span class="math inline">\(d=0.5\)</span> eine halbe Standardabweichung. Wir erhalten eine Mittelwertsdifferenz von 0.5, indem wir zu dem zuvorigen Code einfach 0.5 zur Y-Gleichung dazuaddieren. “In der Population unterscheiden sich nun <code>_X_1</code> und <code>X_2</code> um 0.5 im Mittelwert, da <code>X_1</code> einen Mittelwert von 0 hat und <code>X_2</code> einen Mittelwert von 0 + 0.5 = 0.5.”</p>
<pre class="r"><code>set.seed(12345)
X_1 &lt;- rnorm(N)
X_2 &lt;- rnorm(N) + 0.5 
ttestH1 &lt;- t.test(X_1, X_2, var.equal = TRUE)
ttestH1$p.value</code></pre>
<pre><code>## [1] 0.0160865</code></pre>
<p>Der empirische <span class="math inline">\(p\)</span>-Wert ist diesmal kleiner als <span class="math inline">\(0.05\)</span>. Die Frage ist nun, wie häufig das für eine Stichprobengröße von <span class="math inline">\(N=20\)</span> pro Gruppe vorkommt. Wir führen wieder eine Simulation dazu durch:</p>
<pre class="r"><code>set.seed(12345)
pt_H1 &lt;- replicate(n = 10000, expr = {X_1 &lt;- rnorm(N)
                                      X_2 &lt;- rnorm(N) + 0.5 
                                      ttestH1 &lt;- t.test(X_1, X_2, var.equal = TRUE)
                                      ttestH1$p.value})
mean(pt_H1 &lt; 0.05) # empirische Power</code></pre>
<pre><code>## [1] 0.335</code></pre>
<pre class="r"><code>hist(pt_H1, breaks = 20)</code></pre>
<p><img src="/post/2021-12-10-simulation-und-poweranalyse_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>Die empirische Power (die Wahrscheinlichkeit in unserer Simulation, dass die <span class="math inline">\(H_0\)</span> verworfen wird) liegt bei 0.335. Das Histogramm ist nun alles andere als gleichverteilt. Kleine <span class="math inline">\(p\)</span>-Werte nahe Null kommen wesentlich häufiger vor als große <span class="math inline">\(p\)</span>-Werte nahe 1.</p>
<p>Woran liegt nun dieses schiefe Histogramm? Wir schauen uns dazu noch schnell die <span class="math inline">\(t\)</span>-Werte an:</p>
<pre class="r"><code>set.seed(12345)
tt_H1 &lt;- replicate(n = 10000, expr = {X_1 &lt;- rnorm(N)
                                      X_2 &lt;- rnorm(N) + 0.5 
                                      ttestH1 &lt;- t.test(X_1, X_2, var.equal = TRUE)
                                      ttestH1$statistic})
hist(tt_H1, breaks = 50, freq = FALSE) # freq = FALSE, damit relative Häufigkeiten eingetragen werden!
x &lt;- seq(-4, 4, 0.01) # Sequenz von -4 bis 4 in 0.01 Schritten
lines(x = x, y = dt(x = x, df = 38), lwd = 2) # lwd = Liniendicke</code></pre>
<p><img src="/post/2021-12-10-simulation-und-poweranalyse_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p>Der Grafik entnehmen wir, dass die gesamte Verteilung der empirischen t-Werte verschoben ist im Vergleich zur theoretischen Verteilung (Linie). Somit ist klar, dass die Hypothese häufiger verworfen werden muss, da die kritischen <span class="math inline">\(t\)</span>-Werte unter der Alternativhypothese nicht mehr sinnvoll sind!</p>
<p>Nun zurück zur geschätzten Power: Eine Power von 0.335 ist allerdings nicht besonders hoch. Nur in etwas mehr als einem Drittel der Replikationen wurde die Mittelwertsdifferenz als statistisch bedeutsam betitelt. Wir wissen aber, weil wir das Modell vorgegeben haben, dass die <span class="math inline">\(H_0\)</span> tatsächlich nicht gilt. Somit wird in 2/3 der Fälle fälschlicherweise die <span class="math inline">\(H_0\)</span> nicht verworfen, obwohl sie nicht gilt. Das ist der Fehler 2. Art, auch <span class="math inline">\(\beta\)</span>-Fehler genannt. Somit ist ersichtlich, dass die Power sich berechnet als <span class="math inline">\(1-\beta\)</span>.</p>
<p>Mit größerer Stichprobe wird die Power steigen (was wir im Abschnitt <a href="#PowerPlots">Power-Plots</a> noch deutlicher sehen werden).</p>
<p>Die Power für den Korrelationstest zu bestimmen, ist für diese Sitzung als Übungsaufgabe geplant!</p>
</div>
</div>
<div id="PowerPlots" class="section level2">
<h2>Power-Plots</h2>
<p>Mit einem einzelnen Power-Wert lässt sich in der Regel nicht so viel anfangen. Aus diesem Grund werden Power-Plots erstellt, welche darstellen, wie sich die Power bspw. über unterschiedliche Stichprobengrößen (um die Asymptotik des Tests zu prüfen) oder über unterschiedliche Effektgrößen verändert.</p>
<div id="power-plots-für-mittelwertsunterschiede" class="section level3">
<h3>Power-Plots für Mittelwertsunterschiede</h3>
<p>Wir schauen uns die Power-Plots diesmal nur für die Mittelwertsunterschiede an. Zunächst beginnen wir mit der Asymptotik. Wir wiederholen im einfachsten Fall das Experiment von oben für 5 Stichprobengrößen: <span class="math inline">\(N=20, 40, 60, 80, 100\)</span> (wobei wir das Ergebnis für <span class="math inline">\(N=20\)</span> bereits bestimmt haben). Dazu kopieren wir jeweils den Code von oben und ändern die Stichprobengröße ab:</p>
<pre class="r"><code>set.seed(12345)
pt_H1_20 &lt;- pt_H1
pt_H1_40 &lt;- replicate(n = 10000, expr = {X_1 &lt;- rnorm(40)
                                         X_2 &lt;- rnorm(40) + 0.5 
                                         ttestH1 &lt;- t.test(X_1, X_2, var.equal = TRUE)
                                         ttestH1$p.value})
pt_H1_60 &lt;- replicate(n = 10000, expr = {X_1 &lt;- rnorm(60)
                                         X_2 &lt;- rnorm(60) + 0.5 
                                         ttestH1 &lt;- t.test(X_1, X_2, var.equal = TRUE)
                                         ttestH1$p.value})
pt_H1_80 &lt;- replicate(n = 10000, expr = {X_1 &lt;- rnorm(80)
                                         X_2 &lt;- rnorm(80) + 0.5 
                                         ttestH1 &lt;- t.test(X_1, X_2, var.equal = TRUE)
                                         ttestH1$p.value})
pt_H1_100 &lt;- replicate(n = 10000, expr = {X_1 &lt;- rnorm(100)
                                          X_2 &lt;- rnorm(100) + 0.5 
                                          ttestH1 &lt;- t.test(X_1, X_2, var.equal = TRUE)
                                          ttestH1$p.value})</code></pre>
<p>Nun haben wir eine ganze Menge an <span class="math inline">\(p\)</span>-Werten abgespeichert. Jetzt müssen wir nur noch die Power für jede Bedingung bestimmen. Diese schreiben wir direkt in einen Vektor:</p>
<pre class="r"><code>t_power &lt;- c(mean(pt_H1_20 &lt; 0.05),
             mean(pt_H1_40 &lt; 0.05),
             mean(pt_H1_60 &lt; 0.05),
             mean(pt_H1_80 &lt; 0.05),
             mean(pt_H1_100 &lt; 0.05))
t_power</code></pre>
<pre><code>## [1] 0.3350 0.5991 0.7700 0.8809 0.9369</code></pre>
<p>Wir sehen sehr gut, dass die Power ansteigt. Der zugehörige Power-Plot sieht nun so aus (zunächst legen wir die Stichproben in <code>Ns</code> ab):</p>
<pre class="r"><code>Ns &lt;- seq(20, 100, 20)
plot(x = Ns, y = t_power, type = &quot;b&quot;, main = &quot;Power vs. N&quot;)</code></pre>
<p><img src="/post/2021-12-10-simulation-und-poweranalyse_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p>Dem Plot entnehmen wir, dass ab etwas über <span class="math inline">\(N=60\)</span> die Power oberhalb der gewünschten <span class="math inline">\(80\%\)</span>-Marke liegt. Wir erkennen also, dass die Wahrscheinlichkeit einen Effekt zu finden, wenn dieser da ist, mit steigender Stichprobengröße wächst. Auf diesem Weg kann ein Experiment auch hinsichtlich der nötigen Stichprobengröße geplant werden. Wenn aus Voruntersuchungen oder der Literatur bekannt ist, wie groß ein Effekt zu erwarten ist, dann kann über Poweranalysen untersucht werden, wie groß eine Stichprobe sein muss, um einen Effekt mit hinreichend großer Wahrscheinlichkeit zu identifizieren.</p>
<p>Genauso könnten wir uns fragen, wie groß ein Effekt sein muss, damit mit der vorliegenden Stichprobengröße und mit hinreichend großer Wahrscheinlichkeit ein signifikantes Ergebnis gefunden wird. In diesem Fall sprechen wir von einer Sensitivitätsanalyse. Dies schauen Sie sich ebenfalls als Übung an!</p>
<p>Wenn man dies auf die Spitze treibt, dann landet man vielleicht bei diesem schönen Plot:</p>
<p><img src="/post/2021-12-10-simulation-und-poweranalyse_files/figure-html/unnamed-chunk-23-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>Auf der x-Achse ist die Mittelwertsdifferenz dargestellt, <span class="math inline">\(N\)</span> ist farblich kodiert. Dieser Plot enthält also sowohl Informationen über die Asymptotik (Verhalten mit steigender Stichprobengröße) und über die Auswirkung der Effektstärke. Die gestrichelte Linien symbolisiert die gewünschten <span class="math inline">\(80\%\)</span>, die gepunktete Linie zeigt das <span class="math inline">\(\alpha\)</span>-Fehlerniveau von <span class="math inline">\(5\%\)</span>. Hier wurden allerdings keine Simulationen durchgeführt (sonst wären die Linien nicht so “smooth”), denn für den <span class="math inline">\(t\)</span>-Test lässt sich die Power auch noch leicht über Formeln bestimmen. Diese Formeln sind bspw. in dem <code>R</code>-Paket <code>pwr</code> hinterlegt. Für Interessierte ist in <a href="#AppendixA">Appendix A</a> ein kleiner Exkurs in <code>pwr</code> dargestellt.</p>
</div>
<div id="AppendixA" class="section level3">
<h3>Appendix A</h3>
<details>
<summary>
<strong>Poweranalysen: geschlossene Formeln</strong>
</summary>
<p>Für den <span class="math inline">\(t\)</span>-Test lässt sich die Power auch über Formeln finden. Diese sind im <code>pwr</code>-Paket implementiert. Die Funktion <code>pwr.t.test()</code> ist die Richtige. Sie nimmt zwei wichtige Argumente entgegen: <code>n</code> und <code>d</code>. Hierbei ist <span class="math inline">\(n\)</span> die Stichprobengröße und <span class="math inline">\(d\)</span> ist die Effektstärke nach Cohen:</p>
<p><span class="math display">\[d:=\frac{|\mu_1-\mu_2|}{\sigma},\]</span>
wobei <span class="math inline">\(\mu_1\)</span> und <span class="math inline">\(\mu_2\)</span> die beiden Mittelwerte in den Gruppen sind und <span class="math inline">\(\sigma\)</span> ist die wahre Standardabweichung über die beiden Gruppen hinweg. Dadurch, dass bei uns die Varianz jeweils 1 in den Gruppen war, hatten wir durch Zufall auch oben die Effektstärke nach Cohen gewählt! Die Werte sind also durchaus vergleichbar. Wir schauen uns die Power für unsere erste Effektstärke von 0.5 bei einer Stichprobengröße von 20 an:</p>
<pre class="r"><code># falls noch nicht installiert: &quot;install.packages(&quot;pwr&quot;)&quot;
 library(pwr)
 pwr.t.test(n = 20, d = 0.5)</code></pre>
<pre><code>## 
##      Two-sample t test power calculation 
## 
##               n = 20
##               d = 0.5
##       sig.level = 0.05
##           power = 0.337939
##     alternative = two.sided
## 
## NOTE: n is number in *each* group</code></pre>
<p>Die Power liegt bei 0.3379, also bei 33.79%. Sie ist damit also gar nicht so verschieden von unserer geschätzten Power von 33.5%. Mehr Informationen zum <code>pwr</code>-Paket finden sie <a href="https://www.statmethods.net/stats/power.html">hier</a>.</p>
</details>
</div>
</div>
