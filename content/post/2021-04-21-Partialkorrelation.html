---
title: "Partial- & Semipartialkorrelation"
date: '2021-04-22'
slug: partial
categories:
     - BSc7
tags:
- Partialkorrelation
- Semipartialkorrelation
- geteilte Varianz
- Zusammenhangsanalyse
subtitle: ''
summary: ''
authors: [schroeder, gruetzmacher, nehler, irmer]
lastmod: '2023-05-12 12:00:12 CEST'
featured: no
header:
     image: "/header/PsyBSc7_Partial.jpg"
     caption: "[Courtesy of pexels](https://www.pexels.com/photo/optical-glass-triangular-prism-3845162/)"
projects: []
---



<div id="einleitung" class="section level2">
<h2>Einleitung</h2>
<p>Sicher haben Sie in der Welt der Verschwörungstheorien mal gehört, dass die Anzahl der COVID-Erkrankungen mit der Anzahl der 5G-Tower zusammenhängt. Aber wussten Sie, dass auch der Konsum von Eiscreme und die Anzahl der Morde in New York oder die Anzahl von Nicolas-Cage-Filmauftritten mit der Anzahl weiblicher Redakteure beim Harvard Law Review positiv korreliert sind?<sup>1</sup></p>
<p>Die Frage ist jedoch, ob mit den korrelativen Zusammenhängen der Beweis erbracht wurde, dass 5G-Strahlungen für COVID-Erkrankungen verantwortlich sind, der Eiskonsum zu einer erhöhten Mordrate führt oder die Anzahl der Filme, in denen Nicolas Cage mitspielt, einen Effekt auf die Frauenquote bei der Harvard Law Review hat. Die Antwort ist, wie Sie in Statistik I und Ihrer Einführung in die Versuchsplanung bereits wissen: <strong><em>Nein!</em></strong></p>
<p>Korrelationen liefern keine Belege für Kausalität. Zum einen gibt eine Korrelation keine Auskunft darüber, ob eine Variable <em>x</em> eine Variable <em>y</em> beeinflusst oder umgekehrt. Die meisten Korrelationsanalysen werden in der Psychologie für (relativ) gleichzeitig erhobene Konstrukte durchgeführt. Dies liegt besonders an der hohen Prävalenz von Fragebogenstudien. Den Einfluss des zeitlichen Aspekt auf die Kausalität werden wir jetzt nicht genauer betrachten, sondern eine andere notwendige Bedingung in Frage stellen.</p>
<p>Der Zusammenhang zwischen zwei Variablen kann nämlich durch eine Drittvariable beeinflusst sein, was wir für Kausalität ausschließen müssten. So wird z.B. die Korrelation zwischen 5G-Towern mit den COVID-Erkrankungen durch die Ballungsgebiete erklärt, in denen die Leute enger beieinander leben. In dieser Sitzung beschäftigen wir uns daher mit der Partial- und der Semipartialkorrelation, d.h. Methoden mit denen der Einfluss einer oder mehrerer Drittvariablen kontrolliert werden kann, um hierdurch Scheinkorrelationen, redundante oder maskierte Zusammenhänge aufzudecken.</p>
<p><em>Anmerkungen:</em></p>
<p><sup>1</sup> Es gibt einen ganzen Blog, der sich mit solchen Scheinkorrelationen (bzw. <a href="http://tylervigen.com/spurious-correlations"><em>spurious Correlations</em></a>) befasst.</p>
</div>
<div id="wiederholung-korrelationen" class="section level2">
<h2>Wiederholung Korrelationen</h2>
<p>In der Psychologie werden häufig statistische Zusammenhänge (bzw. stochastische Abhängigkeiten) zwischen Variablen ermittelt. Der statistische Zusammenhang kann mithilfe verschiedener Zusammenhangsmaße gemessen werden, z.B. mit der bivariaten Produkt-Moment-Korrelation, die die Beziehung zwischen zwei metrischen Variablen (bzw. einer metrischen und einer dichotomen Variable) berechnet.</p>
<p><span class="math display">\[r_{xy} = corr(X,Y) = \dfrac {\sum\limits_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y})}{\sqrt{\sum\limits_{i=1}^n (X_i - \bar{X})^2 \cdot \sum\limits_{i=1}^n (Y_i - \bar{Y})^2}}\hat{=}\frac{\mathbb{C}ov[X,Y]}{\sqrt{\mathbb{V}ar[X]\mathbb{V}ar[Y]}}\]</span></p>
<p>Der Korrelationskoeffizient r<sub>xy</sub> misst die Stärke und Richtung einer linearen Beziehung zwischen zwei Variablen <em>x</em> und <em>y</em>. Der Wert von r<sub>xy</sub> liegt dabei immer im Wertebereich zwischen +1 und -1. Man kann auch sagen, dass die Kovarianz “skaliert” wird, um diese besser interpretieren zu können, deshalb steht in obiger Formel auch, <span class="math inline">\(\mathbb{C}ov[X,Y]\)</span> (Kovarianz zwischen <span class="math inline">\(X\)</span> und <span class="math inline">\(Y\)</span>) geteilt durch das Produkt aus der Wurzel der Varianzen <span class="math inline">\(\mathbb{V}ar[X]\)</span> und <span class="math inline">\(\mathbb{V}ar[Y]\)</span>. Eine Korrelation von 1 bedeutet ein perfekter positiver Zusammenhang, d.h. mit der Zunahme der eine Variablen, nimmt auch die anderen Variable zu und umgekehrt. Eine Korrelation von -1 bedeutet ein perfekter negativer Zusammenhang bei dem die Zunahme der einen Variablen mit der Abnahme der anderen Variablen einhergeht. Eine Korrelation von 0 hingegen bedeutet, dass es keinen Zusammenhang zwischen den Variablen gibt. Je höher der absolute Wert einer Korrelation zweier Variablen ist, desto mehr Varianz teilen die beiden Variablen miteinander.</p>
<p><img src="/post/VisualisierungderKorrelation.png" style="width:90.0%" /></p>
<p>Der Zusammenhang zwischen zwei Variablen <em>x</em> und <em>y</em> kann aber auch durch eine Drittvariable <em>z</em> beeinflusst werden. Methoden zur Kontrolle von Drittvariablen und zur Aufdeckung von Scheinkorrelationen, redundanten oder maskierten Zusammenhängen, sind die Partial- und Semipartialkorrelation.</p>
<p><img src="/post/Partial1.png" style="width:50.0%" /></p>
</div>
<div id="partialkorrelation" class="section level2">
<h2>Partialkorrelation</h2>
<p>Die Partialkorrelation ist die bivariate Korrelation zweier Variablen <em>x</em> und <em>y</em>, die bestehen würde, wenn zuvor der Einfluss einer weiteren Variable <em>z</em> statistisch kontrolliert (d.h. “auspartialisiert” oder “herausgerechnet”) wird. Konzeptionell kann die Partialkorrelation r<sub>xy.z</sub> gebildet werden als Korrelation der Regressionsresiduen von <em>x</em> bei Vorhersage durch <em>z</em> und <em>y</em> bei Vorhersage durch <em>z</em>. Weil das Regressionsresiduum nichts mit den Prädiktoren gemein hat, bleiben nach Regression von <em>x</em> und <em>y</em> auf <em>z</em> jeweils nur der Anteil an <em>x</em> und <em>y</em> übrig, der nichts mit <em>z</em> zu tun hat.
<img src="/post/Partial2.png" style="width:50.0%" /></p>
<div id="anwendungsbeispiel-und-vorbereitung" class="section level3">
<h3>Anwendungsbeispiel und Vorbereitung</h3>
<p>Sie arbeiten an einer Schule und sind dafür zuständig, das Lernkonzept der Schule mit psychologischen Erkenntnissen zu unterstützen und zu verbessern. Die Schulleitung hat die erfahrungsbasierte Meinung, dass die Schüler:innen, die gut in Mathematik sind, auch gut in Lesetests abschneiden. Die Schulleitung möchte daher die Didaktik der beiden Fächer vereinen, um mehr von dieser Synergie zu profitieren. Sie als Psycholog:in vermuten jedoch, dass der Zusamenhang nur besteht, da Schüler:innen mit einem hohen IQ gut in beiden Bereichen sind.</p>
<p>Um die Fragestellung zu klären, bevor ein neues Didaktikkonzept entwickelt werden muss, hat eine Stichprobe von 100 Schüler:innen einen Lesetest (<code>reading</code>, <em>x</em>), Mathematiktest (<code>math</code>, <em>y</em>) und allgemeinen Intelligenztest (<code>IQ</code>, <em>z</em>) beantwortet. Der resultierende Datensatz ist direkt von PandaR einlesbar. Mit <code>head()</code> schauen wir uns die obersten sechs Zeilen direkt an, um eine Übersicht zu erhalten.</p>
<pre class="r"><code>#Daten abrufen
load(url(&quot;https://pandar.netlify.app/post/Schulleistungen.rda&quot;))
head(Schulleistungen)</code></pre>
<pre><code>##   female        IQ  reading     math
## 1      1  81.77950 449.5884 451.9832
## 2      1 106.75898 544.8495 589.6540
## 3      0  99.14033 331.3466 509.3267
## 4      1 111.91499 531.5384 560.4300
## 5      1 116.12682 604.3759 659.4524
## 6      0 106.14127 308.7457 602.8577</code></pre>
<p>Wir sehen, dass in jeder Zeile eine Schülerin oder ein Schüler der aufgeführt ist. Neben den 3 schon beschriebenen Variablen wurde nur noch eine vierte erhoben: <code>female</code> als Angabe des Geschlechts (hier nur Männer und Frauen im Datensatz).</p>
<p>Zur Vorbereitung müssen wir außerdem noch <code>ggplot2</code> aktivieren, das wir im Verlauf des Tutorials benutzen werden.</p>
<pre class="r"><code>#Pakete laden
library(ggplot2)       # für Graphiken</code></pre>
<p>Sie möchten in einem ersten Schritt wissen, ob die Leistung im Lesetest generell mit der Leistung im Mathematiktest zusammenhängt, um die erfahrungsbedingte Meinung der Schulleitung zu überprüfen</p>
<div id="korrelation-zwischen-lese--und-mathematikleistung" class="section level4">
<h4>Korrelation zwischen Lese- und Mathematikleistung</h4>
<p>Zur Berechnung und gleichzeitig inferenzstatistischen Absicherung einer einfachen bivariaten Korrelation, kann der <code>cor.test()</code>-Befehl genutzt werden, den wir im letzten Semester kennen gelernt haben. Um die reinen Zahlen noch zu unterstützen, zeichnen wir mit der Funktion <code>ggplot()</code> einen Scatterplot (<code>geom_point()</code>).</p>
<pre class="r"><code># grafische Darstellung mittels Scatterplot
ggplot(Schulleistungen, aes(x=reading, y=math)) + 
  geom_point() + 
  labs(x= &quot;Leseleistung&quot;, y= &quot;Mathematikleistung&quot;)</code></pre>
<p><img src="/post/2021-04-21-Partialkorrelation_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<pre class="r"><code># Korrelationstest
cor.test(Schulleistungen$reading, Schulleistungen$math)</code></pre>
<pre><code>## 
## 	Pearson&#39;s product-moment correlation
## 
## data:  Schulleistungen$reading and Schulleistungen$math
## t = 3.9959, df = 98, p-value = 0.0001248
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  0.1920040 0.5316377
## sample estimates:
##       cor 
## 0.3743059</code></pre>
<p>Der Output zeigt einen Korrelationskoeffiziert von 0.374, was bedeutet dass die beiden fachspezifischen Tests für Lesen und Mathematik positiv miteinander korrelieren (was wiederum bedeutet, dass wenn die Leseleistung überdurchschnittlich ausgeprägt ist, so ist im Mittel auch die Matheleistung überdurchschnittlich ausgeprägt - und umgekehrt). Der p-Wert beträgt 0.000. Der Zusammenhang, den die Schulleitung beobachtet hat, nehmen wir also mit einer Irrtumswahrscheinlichkeit von 5% an. Schüler:innen, die gute Mathematikleistungen erbringen, zeigen eine bessere Leseleistung.</p>
<p>Nun heißt es näher zu betrachten, wie der IQ mit den einzelnen Leistungsbereichen zusammenhängt.</p>
</div>
<div id="untersuchung-zum-zusammenhang-der-allgemeinen-intelligenz-zu-der-lese--und-mathematikleistung" class="section level4">
<h4>Untersuchung zum Zusammenhang der allgemeinen Intelligenz zu der Lese- und Mathematikleistung</h4>
<p>Als nächstes sollten wir uns eine Übersicht verschaffen, ob unsere Drittvariable, wie wir es annehmen, überhaupt einen Zusammenhang zu den beiden ursprünglichen variablen hat. Für einen ersten Eindruck berechnen wir den Zusammenhang der allgemeinen Intelligenz zu der Lese- und Mathematikleistung durch die Korrelation und sichern diese auch inferenzstatistisch ab.</p>
<pre class="r"><code># Korrelation der Drittvariablen mit den beiden ursprünglichen Variablen
cor.test(Schulleistungen$IQ, Schulleistungen$reading)</code></pre>
<pre><code>## 
## 	Pearson&#39;s product-moment correlation
## 
## data:  Schulleistungen$IQ and Schulleistungen$reading
## t = 6.8372, df = 98, p-value = 6.954e-10
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  0.4185987 0.6879220
## sample estimates:
##       cor 
## 0.5682917</code></pre>
<pre class="r"><code>cor.test(Schulleistungen$IQ, Schulleistungen$math)</code></pre>
<pre><code>## 
## 	Pearson&#39;s product-moment correlation
## 
## data:  Schulleistungen$IQ and Schulleistungen$math
## t = 9.638, df = 98, p-value = 7.392e-16
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  0.5807324 0.7862661
## sample estimates:
##       cor 
## 0.6975801</code></pre>
<p>Die Ergebnisse zeigen, dass die allgemeine Intelligenz sowohl mit der Lese-, als auch mit der Mathematikleistung signifikant zusammenhängt. Sie stellt daher eine mögliche konfundierende Drittvariable dar. Diese Vermutung können wir nun mit einer Partialkorrelation überprüfen, für die wir die Residuen brauchen. Mehr Infos gibt es in <a href="#AppendixA">Appendix A</a>.</p>
<p>Um die Residuen zu erhalten bestimmen wir die bivariate Regression zwischen der dritten Variable und der ursprünglichen beiden Variablen. Die Residuen repräsentieren den Teil der Variablen, der von der Drittvariable unkorreliert ist. Das kann man also äquivalent dazu sehen, dass dieser Anteil von der Drittvariablen nicht erklärt werden kann (also dem Fehler in der Regression).</p>
<pre class="r"><code># Regression 
reg_math_IQ &lt;- lm(math ~ IQ, data = Schulleistungen)
reg_reading_IQ &lt;- lm(reading ~ IQ, data = Schulleistungen)</code></pre>
<pre class="r"><code># Residuen in Objekt ablegen (Residuen x)
res_reading_IQ &lt;- residuals(reg_reading_IQ)

# Residuen in Objekt ablegen (Residuen y)
res_math_IQ &lt;- residuals(reg_math_IQ)</code></pre>
<p>Wir haben die Residuen nun in Objekten abgelegt und können die Partialkorrelation bestimmen.</p>
</div>
<div id="partialkorrelation-als-korrelation-zwischen-den-residuen" class="section level4">
<h4>Partialkorrelation (als Korrelation zwischen den Residuen)</h4>
<p>Die bivariate Korrelation kann durch die Funktion <code>cor()</code> bestimmt werden. Als Argumente brauchen wir die beiden Objekte mit den Residuen. Beachten Sie, dass wir hier NICHT <code>cor.test()</code> verwenden, da die inferenzstatistische Absicherung die falschen Freiheitsgrade nutzen würde. Für interessierte Lesende findet sich in <a href="#AppendixA">Appendix A</a> eine genauere Erläuterung der Problematik.</p>
<pre class="r"><code># Partialkorrelation durch Residuen
cor(res_reading_IQ, res_math_IQ)</code></pre>
<pre><code>## [1] -0.0375247</code></pre>
<p>Es zeigt sich also, dass der ursprüngliche Zusammenhang zwischen der Lese- und Mathematikleistung (<em>r<sub>xy</sub></em>= 0.37) unter Kontrolle der allgemeinen Intelligenz verschwindet.</p>
</div>
<div id="paket-nutzung" class="section level4">
<h4>Paket-Nutzung</h4>
<p>Neben diesem Umweg über die Bestimmung der Residuen, die uns die Logik der Partialkorrelation nochmal näher bringen sollte, gibt es natürlich auch eine Funktion zur direkten Bestimmung. Diese ist aber nicht in den Basis-Paketen erhalten, weshalb wir erstmal <code>ppcor</code> installieren müssen.</p>
<pre class="r"><code># Paket für Partial- und Semipartialkorrelation
install.packages(&quot;ppcor&quot;)</code></pre>
<p>Anschließends muss das Paket natürlich noch aktiviert werden.</p>
<pre class="r"><code>library(ppcor)</code></pre>
<pre><code>## Loading required package: MASS</code></pre>
<p>Mit der Funktion <code>pcor.test()</code> lässt sich die Partialkorrelation direkt ermitteln:</p>
<pre class="r"><code># Partialkorrelation mit Funktion
pcor.test(x=Schulleistungen$reading, y=Schulleistungen$math, z=Schulleistungen$IQ)</code></pre>
<pre><code>##     estimate  p.value  statistic   n gp  Method
## 1 -0.0375247 0.712311 -0.3698359 100  1 pearson</code></pre>
<p>Die Partialkorrelation (r<sub>xy.z</sub>) beträgt -0.04 und ist nicht signifikant von 0 verschieden (p= 0.71). Es zeigt sich also, dass der ursprüngliche Zusammenhang zwischen der Lese- und Mathematikleistung (<em>r<sub>xy</sub></em>=0.37) unter Kontrolle der allgemeinen Intelligenz verschwindet. Es handelt sich also um eine Scheinkorrelation zwischen der Lese- und der Matheleistung. Anders ausgedrückt lassen sich gemeinsame Unterschiede auf Lese und Matheleistung zwischen zwei Kindern allein druch den Unterschied in der Intelligenz dieser beiden Kinder erklären. Angenommen Kind <em>A</em> hat überdurchschnittliche Lese- und und Matheleistungen in den beiden Tests, während Kind <em>B</em> durchschnittliche Werte auf beiden Tests aufweist, dann lässt sich dieser Unterschied in beiden Tests zwischen Kind <em>A</em> und Kind <em>B</em> allein durch Unterschiede der beiden Kinder in der Intelligenz erklären. Auf Grund der Vorzeichen müsste Kind <em>A</em> bspw. eine überdurchschnittliche Intelligenz aufweisen, während Kind <em>B</em> eine durchschnittliche Intelligenz zeigt.</p>
</div>
</div>
<div id="mögliche-veränderung-der-ursprünglichen-korrelation-bei-bestimmung-der-partialkorrelation" class="section level3">
<h3>Mögliche Veränderung der ursprünglichen Korrelation bei Bestimmung der Partialkorrelation</h3>
<p>Wird eine Partialkorrelation berechnet, kann die ursprüngliche Korrelation sich auf drei Arten verhalten (betraglich bedeutet immer “ohne Vorzeichen”: z.B. |-1| = 1):</p>
<ol style="list-style-type: decimal">
<li>Partialkorrelation ist (betraglich) kleiner als die ursprüngliche Korrelation (|r<sub>xy.z</sub>|&lt;|r<sub>xy</sub>|)</li>
</ol>
<p>Wie in unserem Bespiel teilen alle drei Variablen miteinander Varianz. Partialisiert man nun eine Variable aus dem Zusammenhang der beiden anderen Variablen heraus, wird die geteilte Varianz weniger, womit die Korrelation betraglich sinkt (es ist wichtig diese Aussage für den Betrag zu formulieren, denn auch wenn eine Korrelation negativ ausfällt, bspw. r<sub>xy</sub> = -.20, und eine dritte Variable wird auspartialisiert womit dann die gemeinsame Varianz zwischen <em>x</em> und <em>y</em> sinkt, so verringert sich die Partialkorrelation bspw. auf r<sub>xy.z</sub>=-.10). Dieser Fall ist der am häufigsten eintretende, da in der Forschung oft Variablen auspartialisiert werden, weil es theoretische Annahme gibt, warum die Variablen Varianz teilen sollten, man aber eine isolierte Assoziation (Beziehung) zwischen <em>x</em> auf <em>y</em> betrachten möchte. Der im Tutorial dargestellte Effekt ist die extremste Form dieser Klasse an Veränderung, bei der die Partialkorrelation dann fast bei 0 liegt.</p>
<ol start="2" style="list-style-type: decimal">
<li>Partialkorrelation ist gleich der ursprünglichen Korrelation (r<sub>xy.z</sub>=r<sub>xy</sub>)</li>
</ol>
<p>Ist die Partialkorrelation r<sub>xy.z</sub> genauso groß (nicht signifikant unterschiedlich) wie die Ausgangskorrelation r<sub>xy</sub>, ist <em>z</em> mit <em>x</em> und <em>y</em> unkorreliert. Die Drittvariable <em>z</em> würde also keinen Zusammenhang und damit keine geteilte Varianz mit <em>x</em> und <em>y</em> haben (und auch nicht erklären).</p>
<ol start="3" style="list-style-type: decimal">
<li>Partialkorrelation ist (betraglich) größer als die ursprüngliche Korrelation (|r<sub>xy.z</sub>|&gt;|r<sub>xy</sub>|)</li>
</ol>
<p>In einem solchen Fall liegt meist ein Suppressoreffekt vor (ein Teil der Varianz in <em>x</em> wird durch die Drittvariable unterdrückt bzw. supprimiert, der für den Zusammenhang mit <em>y</em> irrelevant ist). Der klassische Suppressoreffekt tritt dann auf, wenn <em>z</em> mit <em>y</em> zu 0 korreliert (was nicht immer der Fall sein muss), mit <em>x</em> aber eine bedeutende Korrelation aufweist (Sonderformen eines Suppressoreffekts finden Interessierte in Eid &amp; Gollwitzer 2017, Kap.18,19). In solch einem Fall wird der für <em>y</em> irrelevante Varianzanteil in <em>x</em> durch den Supressor <em>z</em> gebunden, wodurch der relative Anteil an geteilter Varianz zwischen <em>x</em> und <em>y</em> größer wird. Ein Beispiel: Sie untersuchen den Zusammenhang von Sport (x), Kalorienzufuhr(z) und Gewichtsverlust (y). Sporttreiben korreliert positiv mit Gewichtsverlust und Kalorienzufuhr, Kalorienzufuhr aber nicht mit Gewichtsverlust. In einer Partialkorrelation wird die Korrelation von Sporttreiben mit Gewichtsverlust unter der Kontrolle von Kalorienzufuhr größer. Sie können daraus schließen, dass die Kalorienzuführ in diesem Beispiel als Suppressor agiert. Die Inhaltliche Begründung dafür wäre, dass mit einer erhöhten sportlichen Aktivität eine erhöhte Kalorienzufuhr einhergeht. Dieser Zusammenhang hat den positiven Effekt von Sport supprimiert.</p>
</div>
</div>
<div id="semipartialkorrelation" class="section level2">
<h2>Semipartialkorrelation</h2>
<p>Wird aus inhaltlichen Gründen angenommen, dass die Drittvariable nur eine der Variablen <em>x</em> oder <em>y</em> beeinflusst, kann auf eine weitere Methode zur Aufdeckung von Scheinkorrelationen, redundanten oder maskierten Zusammenhängen zurückgegriffen werden; die Semipartialkorrelation. Bei dieser Methode wird der Einfluss der Drittvariablen nur aus einer der beiden Variablen herausgerechnet. Die Semipartialkorrelation r<sub>x(y.z)</sub> entspricht der Korrelation zwischen x und dem Residuum von y bei Vorhersage durch z.</p>
<p><img src="/post/Partial3.png" style="width:50.0%" /></p>
<pre class="r"><code># Semipartialkorrelation durch Nutzung des Residuums
cor(Schulleistungen$reading, res_math_IQ)</code></pre>
<pre><code>## [1] -0.03087634</code></pre>
<p>Auch hier verwenden wir absichtlich die Funktion <code>cor()</code> statt <code>cor.test()</code>, da die inferenzstatistische Absicherung nicht korrekt durchgeführt werden würde aufgrund der Freiheitsgrade.</p>
<p>Mit der Funktion <code>spcor.test()</code> aus dem zuvor installierten Paket <code>ppcor</code> lässt sich die Semipartialkorrelation direkt ermitteln und die inferenzstatistische Absicherung gelingt.</p>
<pre class="r"><code># Semipartialkorrelation mit Funktion
spcor.test(x=Schulleistungen$reading, y=Schulleistungen$math, z=Schulleistungen$IQ)</code></pre>
<pre><code>##      estimate   p.value  statistic   n gp  Method
## 1 -0.03087634 0.7615954 -0.3042418 100  1 pearson</code></pre>
<p>Der Koeffizient der Semipartialkorrelation (r<sub>x(y.z)</sub>) beträgt -0.03 und ist nicht signifikant (p=0). Es zeigt sich also, dass der ursprüngliche Zusammenhang zwischen Lese- und Mathematikleistung (<em>r<sub>xy</sub></em>= 0) verschwindet, wenn der Einfluss der allgemeinen Intelligenz auf die Mathematikleistung kontrolliert wird.</p>
</div>
<div id="wann-wähle-ich-die-partial--und-wann-die-semipartialkorrelation" class="section level2">
<h2>Wann wähle ich die Partial- und wann die Semipartialkorrelation?</h2>
<p>Ob Sie in Ihren Untersuchungen die Partial- oder Semipartialkorrelation zur Kontrolle von Drittvariablen verwenden, begründet sich primär in theoretischen Annahmen. Bei der Partialkorrelation nehmen Sie an, dass die Drittvariable <em>z</em> beide Variablen <em>x</em> und <em>y</em> ursächlich beeinflusst. In unserem Beispiel stellen wir uns den IQ als Ursache für die Leistungen in Mathematik und Lesen vor, daher wäre eine Partialkorrelation angebracht.
Die Semipartialkorrelation ist dann das Mittel der Wahl, wenn die Drittvariable nur eine der beiden Variablen <em>x</em> oder <em>y</em> theoretisch kausal bedingt und zwischen den anderen Variablen lediglich ein ungerichteter Zusammenhang angenommen wird. In unserem Beispiel würde dies bedeuten, dass wir beispielsweise lediglich annehmen, dass der IQ die Matehmatikleistung bedingt, jedoch nicht die Leseleitung. Eine mögliche Begründung könnte sein, dass Mathematik stark von der abstrakten Vorstellungkraft profitiert, die im IQ abgebildet ist, die Leseleistung hingegen eine Fertigkeit ist, die vorallem erlernt wird. Da diese Annahme schwer empirisch zu stützen ist, eignet sich die Semipartialkorrelation in unserem Beispiel weniger als die Partialkorrelaton. Dies ist vermutlich in den meisten Anwendungsbereichen so. Im Rahmen der Regressionsanalyse stellt die Semi-Partialkorrelation eine wichtige Rolle in der Berechnung des Determinationskoeffizienten dar. Darüber erfähren Sie in der nächsten Sitzung mehr.</p>
</div>
<div id="zusammenfassung" class="section level1">
<h1>Zusammenfassung</h1>
<p>Wir haben in dem Tutorial die Partial- und Semipartialkorrelation als Erweiterungen der Korrelation kennengelernt. Die Zusammenhänge zwischen den drei Größen soll der folgende Plot nochmal zusammenfassen.</p>
<p><img src="/post/Partial4.png" style="width:90.0%" /></p>
<hr />
<div id="AppendixA" class="section level2 anchorhead">
<h2>Appendix A</h2>
<details>
<summary>
<strong>Inferenzstatistik der Partialkorrelation und weiter konzeptionelle Überlegungen</strong>
</summary>
<p>Bevor wir uns die Inferenzstatistik der Partialkorrelation genauer ansehen, wiederholen wir die Inferenzstatistik der Korrelation und schauen uns nochmal genau an, welche Eigenschaften eigentlich so ein Residuum hat.</p>
<div id="inferenzstatistik-der-korrelation" class="section level3">
<h3>Inferenzstatistik der Korrelation</h3>
<p>Um eine Korrelation auf Signifikanz zu prüfen, untersuchen wir die Nullhypothese, dass die Korrelation <span class="math inline">\(\rho\)</span> in der Population tatsächlich 0 ist: <span class="math inline">\(H_0:\rho=0\)</span>. Unter dieser Nullhypothese lässt sich eine Teststatistik <span class="math inline">\(T\)</span> für den empirischen Korrelationskoeffizient <span class="math inline">\(r\)</span> herleiten, welche <span class="math inline">\(t(n-2)\)</span> verteilt ist, also die der <span class="math inline">\(t\)</span>-Verteilung, die wir aus Mittelwertsunterschieden bereits kennen, folgt (<em>diese Formel gilt nur für die Pearson Produkt-Momentkorrelation</em>):</p>
<p><span class="math display">\[\begin{align*}
T=\frac{r\sqrt{n-2}}{\sqrt{1-r^2}}\sim t(n-2)
\end{align*}\]</span></p>
<p>Liegt <span class="math inline">\(T\)</span> weit entfernt von der 0, so spricht dies gegen die <span class="math inline">\(H_0\)</span>-Hypothese. Der zugehörige <span class="math inline">\(t\)</span> und <span class="math inline">\(p\)</span> Wert aus der <code>cor.test</code> Funktion werden genau mit dieser Formel, bzw. mit <code>pt</code> (für den zugehörigen <span class="math inline">\(p\)</span>-Wert zu einem bestimmten <span class="math inline">\(t\)</span>-Wert), bestimmt.</p>
<pre class="r"><code># Infos aus cor.test
cortest &lt;- cor.test(Schulleistungen$reading, Schulleistungen$math)
# correlation r
r_cortest &lt;- cortest$estimate
# t-Wert
t_cortest &lt;- cortest$statistic
# p-Wert
p_cortest &lt;- cortest$p.value

# zu Fuß:
r &lt;- cor(Schulleistungen$reading, Schulleistungen$math)
n &lt;- nrow(Schulleistungen) # Anzahl Personen
t &lt;- r*sqrt(n-2)/sqrt(1-r^2)
p &lt;- 2*pt(q = abs(t), df = n-2, lower.tail = F) # ungerichtet (deswegen *2)

# Vergleiche
# t
t_cortest</code></pre>
<pre><code>##        t 
## 3.995921</code></pre>
<pre class="r"><code>t</code></pre>
<pre><code>## [1] 3.995921</code></pre>
<pre class="r"><code># p
p_cortest</code></pre>
<pre><code>## [1] 0.0001248311</code></pre>
<pre class="r"><code>p</code></pre>
<pre><code>## [1] 0.0001248311</code></pre>
<p>Super, die Formeln stimmen also. Widmen wir uns nun den Regressionsresiduen.</p>
</div>
<div id="eigenschaften-der-regressionsresiduen" class="section level3">
<h3>Eigenschaften der Regressionsresiduen</h3>
<p>Wir wollen <em>z</em> aus <em>x</em> und <em>y</em> herauspartialisieren. Wenn wir eine Regression rechnen und bspw. <em>y</em> durch <em>z</em> “vorhersagen”, dann schätzen wir ein Modell: <span class="math inline">\(y=\beta_{y,0}+\beta_{y,1}z+\varepsilon_y\)</span>. Die Annahme an das Residuum lautet, dass es einen Mittelwert von 0 hat (auch Erwartungswert: <span class="math inline">\(\mathbb{E}[\varepsilon_y]=0\)</span>) und dass es mit dem Prädiktor unkorreliert ist: <span class="math inline">\(\mathbb{C}ov[z,\varepsilon_y]=0\)</span>. Wenn die Kovarianz mit <em>z</em> Null ist, so ist auch die Korrelation mit <em>z</em> Null. Gleichzeitig ist <span class="math inline">\(\varepsilon_y\)</span> aber hoch mit <em>y</em> korreliert. Es gilt <span class="math inline">\(\mathbb{C}ov[y,\varepsilon_y]&gt;0\)</span>. Damit haben wir quasi eine neue Variable <span class="math inline">\(\varepsilon_y\)</span> gefunden, die sehr viel mit <em>y</em> gemein hat, aber nichts mehr mit <em>z</em>. Somit ist <span class="math inline">\(\varepsilon_y\)</span> also der Anteil von <em>y</em> der nichts mehr mit <em>z</em> zu tun hat. Hängt dieser noch mit dem ebenso bereinigten Anteil von <em>x</em>, <span class="math inline">\(\varepsilon_x\)</span> (resultierend aus <span class="math inline">\(x=\beta_{x,0}+\beta_{x,1}z+\varepsilon_x\)</span>) zusammen, dann bedeutet dies, dass es Anteile von <em>x</em> und <em>y</em> gibt, die mit einander linear Zusammenhang. Diese Beziehung ist dann nicht durch <em>z</em> erklärbar, da dieses unkorreliert zu den Residuen ist. Hier ein empirisches Beispiel:</p>
<pre class="r"><code># Regression 
reg_reading_IQ &lt;- lm(reading ~ IQ, data = Schulleistungen) # x ~ z
reg_math_IQ &lt;- lm(math ~ IQ, data = Schulleistungen) # y ~ z 

# Residuen in Objekt ablegen (Residuen x)
res_reading_IQ &lt;- residuals(reg_reading_IQ) # eps_x

# Residuen in Objekt ablegen (Residuen y)
res_math_IQ &lt;- residuals(reg_math_IQ) # eps_y</code></pre>
<p>Prüfen wir doch mal ein paar Eigenschaften des Residuums:</p>
<pre class="r"><code># Mittelwert 0
mean(res_reading_IQ) # mean(eps_x)</code></pre>
<pre><code>## [1] -1.20997e-15</code></pre>
<pre class="r"><code>round(mean(res_reading_IQ), 14) # mean(eps_x) gerundet auf 14 Nachkommastellen</code></pre>
<pre><code>## [1] 0</code></pre>
<p>Das Residuum hat wie im Erwartungswert festgehalten den Mittelwert von 0. Weiterhin zeigt es auch keinen Zusammenhang zum Prädiktor in der Regression, in der es aufgetreten ist:</p>
<pre class="r"><code># Korrelation/Kovarianze mit dem Prädiktor z
cov(Schulleistungen$IQ, res_reading_IQ) # 0</code></pre>
<pre><code>## [1] 5.203189e-14</code></pre>
<pre class="r"><code>cor(Schulleistungen$IQ, res_reading_IQ) # 0</code></pre>
<pre><code>## [1] 3.771093e-17</code></pre>
<pre class="r"><code>round(cor(Schulleistungen$IQ, res_reading_IQ), 16) # 0 gerundet auf 16 Nachkommastellen</code></pre>
<pre><code>## [1] 0</code></pre>
<p>Nun interessiert uns aber, wie stark dieses Residuum noch mit der abhängigen Variable (<em>x</em> oder <em>y</em>) zusammenhängt:</p>
<pre class="r"><code>cor(Schulleistungen$reading, res_reading_IQ)</code></pre>
<pre><code>## [1] 0.8228272</code></pre>
<p>Die Korrelation fällt sehr hoch aus: <span class="math inline">\(r_{\varepsilon_y,y}=\)</span> 0.823. Das ist doch super! Das heißt nämlich, dass <span class="math inline">\(\varepsilon_y\)</span> sehr viel Information von <em>y</em> enthält, gleichzeitig aber nichts mehr mit <em>z</em> gemein hat!</p>
</div>
<div id="woher-kommt-die-formel-der-partialkorrelation" class="section level3">
<h3>Woher kommt die Formel der Partialkorrelation</h3>
<p>Die Partialkorrelation hatten wir in diesem Beitrag über die Residuen einer Regression pro Variable definiert. Im vorherigen Abschnitt hatten wir die Eigenschaften dieser Residuen näher beleuchtet. Die Partialkorrelation war jetzt nichts weiter als die Korrelation der Residuen <span class="math inline">\(\varepsilon_x\)</span> mit <span class="math inline">\(\varepsilon_y\)</span> also dem Anteil von <em>x</em> und <em>y</em>, der nicht mit <em>z</em> korreliert war. Die Partialkorrelation lautet:</p>
<pre class="r"><code># Partialkorrelation
partial_cor &lt;- cor(res_math_IQ, res_reading_IQ)
partial_cor</code></pre>
<pre><code>## [1] -0.0375247</code></pre>
<p>Wir kennen aber auch die Formel für die Partialkorrelation:</p>
<p><span class="math display">\[r_{xy.z}=\frac{r_{xy}-r_{xz}r_{yz}}{\sqrt{1-r_{xz}^2}\sqrt{1-r_{yz}^2}}\]</span></p>
<p>Wenn wir die Korrelationen in Objekten abspeichern, so lässt sich diese Formel sehr leicht umsetzen:</p>
<pre class="r"><code># Partialkorrelation via Formel
r_xy &lt;- cor(Schulleistungen$math, Schulleistungen$reading)
r_xz &lt;- cor(Schulleistungen$math, Schulleistungen$IQ)
r_yz &lt;- cor(Schulleistungen$reading, Schulleistungen$IQ)

r_xy.z &lt;- (r_xy - r_xz*r_yz)/(sqrt(1-r_xz^2)*sqrt(1-r_yz^2))
r_xy.z</code></pre>
<pre><code>## [1] -0.0375247</code></pre>
<p>Offensichtlich kommen beide Befehle zum gleichen Ergebnis.
Interessierten sei gesagt, dass der verwendete Code für die Berechnung <code>r_xy.z</code> das Gleiche ist wie:</p>
<pre class="r"><code>r_xy.z &lt;- (r_xy - r_xz*r_yz)/sqrt(1-r_xz^2)/sqrt(1-r_yz^2)</code></pre>
<p>Hier besteht allerdings der Vorteil, dass man die Klammer im Nenner nicht vergessen kann.</p>
<p>Wir wollen uns nun der Frage widmen, wie diese Formel entsteht. Dazu müssen wir eine kleine Vorbereitung treffen: wir müssen den <code>Schulleistungen</code> Datensatz standardisieren. Danach ist der Mittelwert und die Varianz jeder Variable 0 (Mittelwert) und 1 (Varianz). Dies geht sehr leicht mit dem <code>scale</code> Befehl. Anschließend müssen wir alle Ananlysen nochmals wiederholen, aber keine Sorge, die Partialkorrelation bleibt identisch:</p>
<pre class="r"><code># Schulleistungen standardisieren:
Schulleistungen_std &lt;- data.frame(scale(Schulleistungen))
# Regression 
reg_reading_IQ &lt;- lm(reading ~ IQ, data = Schulleistungen_std) # x ~ z
reg_math_IQ &lt;- lm(math ~ IQ, data = Schulleistungen_std) # y ~ z 

# Residuen in Objekt ablegen (Residuen x)
res_reading_IQ &lt;- residuals(reg_reading_IQ) # eps_x

# Residuen in Objekt ablegen (Residuen y)
res_math_IQ &lt;- residuals(reg_math_IQ) # eps_y

# Partialkorrelation
partial_cor &lt;- cor(res_math_IQ, res_reading_IQ)
partial_cor</code></pre>
<pre><code>## [1] -0.0375247</code></pre>
<pre class="r"><code># Partialkorrelation via Formel
r_xy &lt;- cor(Schulleistungen_std$math, Schulleistungen_std$reading)
r_xz &lt;- cor(Schulleistungen_std$math, Schulleistungen_std$IQ)
r_yz &lt;- cor(Schulleistungen_std$reading, Schulleistungen_std$IQ)

r_xy.z &lt;- (r_xy - r_xz*r_yz)/(sqrt(1-r_xz^2)*sqrt(1-r_yz^2))
r_xy.z</code></pre>
<pre><code>## [1] -0.0375247</code></pre>
<p>Sie sehen, alles bleibt identisch! Der Hauptunterschied ist nur, einige Korrelationen jetzt einfach Kovarianzen sind, da die Varianzen der Variablen = 1 sind:</p>
<pre class="r"><code>cor(Schulleistungen_std$math, Schulleistungen_std$reading)</code></pre>
<pre><code>## [1] 0.3743059</code></pre>
<pre class="r"><code>cov(Schulleistungen_std$math, Schulleistungen_std$reading)</code></pre>
<pre><code>## [1] 0.3743059</code></pre>
<p>Wir wissen, dass die Korrelation gerade die Kovarianz geteilt durch das Produkt der Standardabweichungen ist. Das bedeutet, dass in der Formel gelten muss:</p>
<p><span class="math display">\[\begin{align*}
\mathbb{C}ov[\varepsilon_x, \varepsilon_y] &amp;= r_{xy} - r_{xz}r_{yz}\\
\mathbb{V}ar[\varepsilon_x] &amp;= 1-r_{xz}^2\\
\mathbb{V}ar[\varepsilon_y] &amp;= 1-r_{yz}^2
\end{align*}\]</span></p>
<p>Die Wurzel aus den letzten beiden Ausdrücken ergibt dann die Standardabweichung (von <span class="math inline">\(\varepsilon_x\)</span> und <span class="math inline">\(\varepsilon_y\)</span>). Prüfen wir das doch mal:</p>
<pre class="r"><code># Cov[eps_x, eps_y]
cov(res_math_IQ, res_reading_IQ)</code></pre>
<pre><code>## [1] -0.02212311</code></pre>
<pre class="r"><code>r_xy - r_xz*r_yz</code></pre>
<pre><code>## [1] -0.02212311</code></pre>
<pre class="r"><code># -&gt; identisch!

# Var[eps_x]
var(res_math_IQ)</code></pre>
<pre><code>## [1] 0.513382</code></pre>
<pre class="r"><code>1-r_xz^2</code></pre>
<pre><code>## [1] 0.513382</code></pre>
<pre class="r"><code># -&gt; identisch!

# Var[eps_y]
var(res_reading_IQ)</code></pre>
<pre><code>## [1] 0.6770446</code></pre>
<pre class="r"><code>1-r_yz^2</code></pre>
<pre><code>## [1] 0.6770446</code></pre>
<pre class="r"><code># -&gt; identisch!</code></pre>
<p>Daraus können wir nun ableiten, dass die Partialkorrelationsformel sich einfach aus der Korrelation der Residuen <span class="math inline">\(\varepsilon_x\)</span> und <span class="math inline">\(\varepsilon_y\)</span> ableitet. Wir fanden es spannend dies einmal ausführlich nieder zu schreiben und wir hoffen, dass es eventuell beim Verständnis dieser doch etwas trockenen Materie geholfen hat. Weiterer Fun-Fact: <span class="math inline">\(1-r_{xz}^2\)</span> ist gerade der Anteil an Varianz von <em>x</em>, der nicht durch <em>z</em> erklärt werden kann - genau diesen haben wir auch für die Partialkorrelation benötigt. Das Gleiche gilt für <span class="math inline">\(1-r_{yz}^2\)</span> - nur eben für <em>y</em>.</p>
</div>
<div id="inferenzstatistik-der-partialkorrelation" class="section level3">
<h3>Inferenzstatistik der Partialkorrelation</h3>
<p>Nun kommen wir endlich zur Inferenzstatistik der Partialkorrelation. Dabei geht es hauptsächlich um die Frage, warum wir nicht einfach <code>cor.test()</code> mit den Residuen nutzen können. Um eine Partialkorrelation auf Signifikanz zu prüfen, untersuchen wir die Nullhypothese, dass die Partialkorrelation <span class="math inline">\(\rho_{xy.z}\)</span> in der Population tatsächlich 0 ist: <span class="math inline">\(H_0:\rho_{xy.z}=0\)</span>. Unter dieser Nullhypothese lässt sich eine Teststatistik <span class="math inline">\(T\)</span> für die empirischn Partialkorrelationskoeffizient <span class="math inline">\(r_{xy.z}\)</span> herleiten, welche <span class="math inline">\(t(n-k-2)\)</span> verteilt ist, wobei <span class="math inline">\(k\)</span> die Anzahl an Prädiktoren sind, die herauspartialisiert wurden (es wäre also auch vektorwertiges <em>z</em> zulässig!). <span class="math inline">\(T\)</span> folgt also wieder der <span class="math inline">\(t\)</span>-Verteilung (<em>diese Formel gilt nur Partialkorrelationen, die für die Pearson Produkt-Momentkorrelation bestimmt sind</em>):</p>
<p><span class="math display">\[\begin{align*}
T=\frac{r_{xy.z}\sqrt{n-k-2}}{\sqrt{1-r_{xy.z}^2}}\sim t(n-k-2)
\end{align*}\]</span></p>
<p>Liegt <span class="math inline">\(T\)</span> weit entfernt von der 0, so spricht dies gegen die <span class="math inline">\(H_0\)</span>-Hypothese. Der zugehörige <span class="math inline">\(t\)</span> und <span class="math inline">\(p\)</span> Wert aus dem <code>ppcor</code> Paket werden genau mit dieser Formel, bzw. mit <code>pt</code> (für den zugehörigen <span class="math inline">\(p\)</span>-Wert zu einem bestimmten <span class="math inline">\(t\)</span>-Wert), bestimmt.</p>
<p>Wir sehen also, dass die <code>cor.test</code> Funktion, wenn wir ihr die Residuen übergeben, gar nicht zum richtigen Ergebnis kommen <em>kann</em>,. Das liegt daran, dass die <code>cor.test</code> Funktion davon ausgeht, dass ganz normale Variablen korreliert werden sollen. Residuen sind aber dahingehend besonders, dass sie bereits verrechnet wurden. Wir haben ja bereits Regressionen mit <em>z</em> vorgenommen, um die entsprechenden Anteile von <em>z</em> herauszurechnen. Damit haben wir Informationen über die Daten benutzt. Nämlich diese, die die Korrelationen mit <em>x</em> und <em>y</em> mit <em>z</em> betreffen. Diese genutzen Informationen müssen eingepreist werden, was durch das Anpassen der Freiheitsgrade in der <span class="math inline">\(t\)</span>-Verteilung geschieht (wir nutzen hier <code>df = n-1-2</code> und nicht <code>df=n-2</code>).</p>
<p>Weil es so viel Freude bereitet diese Inhalte tiefer zu verstehen, prüfen wir auch dies noch einmal:</p>
<pre class="r"><code># Infos aus pcor.test
pcortest &lt;- pcor.test(Schulleistungen$reading, Schulleistungen$math, Schulleistungen$IQ)
# correlation r
r_pcortest &lt;- pcortest$estimate
# t-Wert
t_pcortest &lt;- pcortest$statistic
# p-Wert
p_pcortest &lt;- pcortest$p.value

# zu Fuß:
n &lt;- nrow(Schulleistungen) # Anzahl Personen
t &lt;- r_xy.z*sqrt(n-1-2)/sqrt(1-r_xy.z^2)
p &lt;- 2*pt(q = abs(t), df = n-1-2, lower.tail = F) # ungerichtet (deswegen *2)

# Vergleiche
# t
t_pcortest</code></pre>
<pre><code>## [1] -0.3698359</code></pre>
<pre class="r"><code>t</code></pre>
<pre><code>## [1] -0.3698359</code></pre>
<pre class="r"><code># p
p_pcortest</code></pre>
<pre><code>## [1] 0.712311</code></pre>
<pre class="r"><code>p</code></pre>
<pre><code>## [1] 0.712311</code></pre>
<p>Super, auch diesmal stimmen die Formeln. Im Laufe Ihres Studiums wird dieser Sachverhalt immer wieder aufgegriffen und auch noch näher erklärt. An dieser Stelle reicht es uns zu wissen, dass wir bei der Partialkorrelation andere <span class="math inline">\(df\)</span> verwenden müssen.</p>
</details>
</div>
</div>
<hr />
<div id="r-skript" class="section level2">
<h2>R-Skript</h2>
<p>Den gesamten <code>R</code>-Code, der in dieser Sitzung genutzt wird, können Sie <a href="/post/PsyBSc7_R_Files/03_partial.R"><svg aria-hidden="true" role="img" viewBox="0 0 512 512" style="height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:currentColor;overflow:visible;position:relative;"><path d="M288 32c0-17.7-14.3-32-32-32s-32 14.3-32 32V274.7l-73.4-73.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3l128 128c12.5 12.5 32.8 12.5 45.3 0l128-128c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L288 274.7V32zM64 352c-35.3 0-64 28.7-64 64v32c0 35.3 28.7 64 64 64H448c35.3 0 64-28.7 64-64V416c0-35.3-28.7-64-64-64H346.5l-45.3 45.3c-25 25-65.5 25-90.5 0L165.5 352H64zm368 56a24 24 0 1 1 0 48 24 24 0 1 1 0-48z"/></svg>hier herunterladen</a>.</p>
</div>
</div>
