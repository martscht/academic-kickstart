---
title: Tests und Konfidenzintervalle
date: '2020-12-11'
slug: tests-und-konfidenzintervalle
categories:
  - BSc2
tags:
  - t-Test
subtitle: ''
summary: ''
authors: [scheppa-lahyani, nehler]
lastmod: '2020-12-11T19:04:08+01:00'
featured: no
header:
  image: "/header/BSc2_Tests.jpg"
  caption: "[Courtesy of pxhere](https://pxhere.com/de/photo/1240882)"
projects: []
---



```{r setup, cache = FALSE, echo = FALSE, warning = FALSE, message = FALSE}
knitr::opts_chunk$set(error = TRUE)
library(knitr)
```


  
<details><summary>Kernfragen dieser Lehreinheit</summary>

* Wie berechne ich, ob es einen **Unterschied zwischen einer Stichprobe und der dazugehörigen Population** gibt? 
* Wann und wie rechne ich einen **z-Test (Einstichproben-Gauss-Test)**? Wie interpretiere ich die Ergebnisse?
* Wie bestimme ich das **Konfidenzintervall** des wahren Werts $\mu$?
* Wann und wie rechne ich einen **t-Test**? Welche Voraussetzungen hat dieser? Wie interpretiere ich die Ergebnisse?
* Wie gehe ich mit **gerichteten vs. ungerichteten Hypothesen** um?
* Was ist Cohen's *d* und wie berechne ich es? Wie interpretiere ich die Ergebnisse?

</details>

***
 
## Was erwartet Sie heute?
  
Nachdem wir uns die letzten Wochen mit der Deskriptivstatistik und Zusammenhangsmaßen beschäftigt haben, wird unser Thema nun Gruppenunterschiede sein. Wir interessieren uns heute vor allem für den Unterschied zwischen dem Mittelwert einer Stichprobe und dem Mittelwert der dazugehörigen Population, aus der die Stichprobe stammt.

## Aufbau der Sitzung

* z-Test
* Konfidenzintervalle
* t-Test
* Beispiel am Datensatz
* Effektgröße

***

# Let's start

Der durchschnittliche IQ der Population ist $\mu_0$ = 100 und die Standardabweichung ist 15. Eine Forschungsgruppe glaubt aber, dass sich das verändert hat und entscheidet, diese Vermutung an der zufälligen Stichprobe von 75 Erwachsenen zu testen. Sie finden heraus, dass der durchschnittliche IQ der Stichprobe $\mu_1$ = 105 (*SD* = 17) ist.

**Was wären hier $H_0$ und $H_1$?**
  
$\alpha$ = .05 

$H_0$: Der durchschnittliche IQ der Stichprobe ist gleich oder geringer als zuvor.

$H_0$: $\mu_0$ $\geq$ $\mu_1$
  
$H_1$: Der durchschnittliche IQ der Stichprobe ist höher als zuvor.

$H_1$: $\mu_0$ $<$ $\mu_1$
  
Die Frage: Reicht dieses deskriptive Ergebnis (100 vs. 105) um daraus schlusszufolgern, dass der durchschnittliche IQ sich verändert hat?  
  
**Nein**. Erst mit Hilfe des z- oder t-Tests kann herausgefunden werden, wie (un)wahrscheinlich die beobachtete Diskrepanz (100 vs. 105) ist. 

ABER: ob z- oder t-Test zum Einsatz kommt, hängt davon ab, ob neben dem Mittelwert auch die Standardabweichung (*SD*, $\sigma$) der Grundgesamtheit bekannt ist.  
In diesem Fall ist die *SD* bekannt, demnach wäre ein z-Test an dieser Stelle anzuwenden.

## z-Test

Der **z-Test** oder **Einstichproben-Gauss-Test** setzt voraus, dass das Merkmal in der Population, auf die sich die Nullhypothese ($H_0$) bezieht, normalverteilt ist und der Mittelwert sowie die Standardabweichung bekannt sind.  
Des Weiteren verwendet der Gauss-Test grundsätzlich die Standardnormalverteilung als Stichprobenkennwerteverteilung (SKV), deswegen ist er nicht für kleine Stichproben geeignet.  
Der Einstichproben-Gauss-Test prüft anhand des arithmetischen Mittels einer Stichprobe, ob der Erwartungswert der zugehörigen Grundgesamtheit ungleich (bzw. kleiner oder größer) als ein vorgegebener Wert ist.

Die Formel für den **empirischen *z-*Wert** $z_{emp}$ ist:
  
$$z_{emp} = |\frac{\bar{x} - {\mu}}{\sigma_{\bar{x}}}|$$
  wobei sich der Standardfehler (*SE*) des Mittelwerts wie folgt berechnet:
  
$$\sigma_{\bar{x}} = {\frac{{\sigma}}{\sqrt{n}}}$$
  
Zunächst legen wir alle für den *z-*Wert relevanten Informationen in unser Environment ab, wobei wir auch schon den Standardfehler des Mittelwerts ($\sigma_{\bar{x}}$) berechnen.

```{r}
mean_IQ <- 100 #Mean Grundgesamtheit
sd_IQ <- 15 #SD der Grundgesamtheit
sample_size <- 75 #Stichprobengröße
se_IQ <- sd_IQ/sqrt(sample_size) #standard error (SE), also Standardfehler
new_mean_IQ <- 105 #Stichprobenmittelwert
new_sd_IQ <- 17 #SD der Stichprobe (Populationsschätzer)
```

Demnach wird der empirische *z-*Wert $z_{emp}$ wie folgt berechnet (Bedenkt, dass es immer um den Betrag des Ergebnisses geht, weshalb wir die Funktion `abs()` verwenden.):

```{r}
z_IQ <- abs((new_mean_IQ-mean_IQ)/(sd_IQ/sqrt(sample_size))) #abs() berechnet den Betrag des Ergebnisses
z_IQ
```

bzw.

```{r}
z_IQ <- abs((new_mean_IQ-mean_IQ)/se_IQ)
z_IQ
```

Der empirische *z-*Wert $z_{emp}$ ist eine Angabe, um wie viele Standardabweichungen der Mittelwerte der SKV (*SE*) der Mittelwert der Stichprobe vom Mittelwert der Grundgesamtheit abweicht.  
Der beobachtete Stichprobenmittelwert weicht demnach um **$z_{IQ}$ = 2.89** *SE* (nach oben) vom Mittelwert der Grundgesamtheit ab.  
Um entscheiden zu können, ob es sich um eine signifikante Abweichung handelt, muss der **kritische *z-*Wert** $z_{krit}$ bestimmt werden.  
Für eine Irrtumswahrscheinlichkeit von 5% und eine einseitige Hypothesentestung wäre dies:
  
```{r}
z_krit <- qnorm(1-.05) #bei einer zweiseitigen Testung würden wir qnorm(1-(.05/2)) verwenden
z_krit
```

Der **kritische *z-*Wert** beträgt demnach **$z_{krit}$ = 1.64**. Damit das Ergebnis als signifikant gewertet wird, muss der empirische *z-*Wert $z_{emp}$ größer sein als der kritsiche *z-*Wert (**$z_{IQ}$ > $z_{krit}$**). Hierfür können wir auch eine logische Abfrage nutzen:
  
```{r}
z_IQ > z_krit
```

Das Ergebnis `TRUE` zeigt uns, dass es sich um einen signifikanten Unterschied handelt.
Mit einer Irrtumswahrscheinlichkeit von 5% kann die $H_0$ verworfen werden. Der durchschnittliche IQ der Stichprobe ist höher als der durchschnittliche IQ der Grundgesamtheit.

### Weitere Möglichkeit: `pnorm`

Wie wahrscheinlich ist es angesichts der bekannten Normalverteilung der *z-*Werte den beobachteten empirischen *z-*Wert $z_{emp}$ zu erhalten?
  
Wie hoch ist die Wahrscheinlichkeit diesen oder einen GRÖßEREN (einseitig) *z-*Wert zu erreichen?
  
```{r}
p_z_IQ_oneside <- pnorm(z_IQ, lower.tail = FALSE)
p_z_IQ_oneside
```

Wie hoch ist die Wahrscheinlichkeit diesen oder einen EXTREMEREN (zweiseitig) *z-*Wert zu erreichen?
  
```{r}
p_z_IQ_twoside <- 2*pnorm(z_IQ, lower.tail = FALSE) #verdoppeln, da zweiseitig
p_z_IQ_twoside
```

Wir erkennen, dass in beiden Fällen der Wert kleiner als .05 (5%) ist. Demnach ist die Wahrscheinlichkeit, diesen Wert per Zufall erhalten zu haben, sehr gering.

***

## Konfidenzintervalle 

Wir können auch ein Konfidenzintervall um den wahren Populationsmittelwert $\mu$ bestimmen. Es geht dabei darum, in welchem Wertebereich der wahre Wert $\mu$ mit einer gewissen vorgegebenen Wahrscheinlichkeit liegt. Wenn wir also ein 95%-Konfidenzintervall wählen und wir aus der selben Grundgesamtheit wiederholt die selbe Anzahl an Fällen ziehen, so ist in 95% der Fälle der Mittelwert in dem vorgegeben Konfidenzintervall enthalten.

Dabei gilt:
  
$$\mu = \bar{x} \pm \frac{z_{krit}*\sigma}{\sqrt{n}}$$
  
Ein 95%-Konfidenzintervall ist ein Intervall, welches in 95% der Fälle beim Ziehen aus der selben Grundgesamtheit den wahren Wert $\mu$ enthält.

Wenn wir ein 95%-Konfidenzintervall bestimmen wollen, brauchen wir den **kritischen *z-*Wert** $z_{krit}$ aus der SNV (Standardnormalverteilung).  
Für Konfidenzintervalle nutzen wir einen *z-*Wert für 2-seitiges Testen:
  
```{r}
z_krit2 <- qnorm(1-(.05/2))
z_krit2
```

```{r}
positive_mean_IQ <- new_mean_IQ+((z_krit2*new_sd_IQ)/sqrt(sample_size))
positive_mean_IQ

negative_mean_IQ <- new_mean_IQ-((z_krit2*new_sd_IQ)/sqrt(sample_size))
negative_mean_IQ

conf_interval_IQ <- c(negative_mean_IQ, positive_mean_IQ )
conf_interval_IQ
```

In diesem Fall liegt der wahre IQ Wert der Grundgesamtheit $\mu$, aus der die Stichprobe gezogen wurde, zwischen `r conf_interval_IQ[1]` und `r conf_interval_IQ[2]`. Das bedeutet, dass in 95% der Fälle in einer wiederholten Ziehung aus der Grundgesamtheit die mittleren IQ-Werte zwischen `r conf_interval_IQ[1]` und `r conf_interval_IQ[2]` liegen.

***

## t-Test

Der t-Test ersetzt den z-Test, wenn die Varianz in der Grundgesamtheit nicht bekannt ist. Mit dem t-Test für eine Stichprobe kann man den **Stichprobenmittelwert** einer Variablen mit einem **bekannten Populationsmittelwert** vergleichen. 

Der Test hat die folgenden Voraussetzungen:
  
 1. Metrisch skalierte abhängige Variable
 2. Bei *n* < 30 : Normalverteilung der abhängigen Variable in der Population.

Unterschiedliche Quellen geben an, dass die durchschnittliche Größe der Männer in Deutschland 180 cm beträgt (z.B. https://www.länderdaten.info/durchschnittliche-körpergrössen.php). Eine Forschungsgruppe vermutet jedoch, dass die Männer in Deutschland eigentlich größer sind und ermittelt von zehn zufällig gezogenen Männer die Körpergröße.

Die Größe der Männer beträgt: 183, 178, 175, 186, 185, 179, 181, 179, 182, 177 (gemessen in cm). 

Kann die Vermutung der Forschungsgruppe bestätigt werden?
  
```{r}
men.height <- c(183, 178, 175, 186, 185, 179, 181, 179, 182, 177)
mean.men.height <- mean(men.height)
sd.men.height <- sd(men.height)
average.men.height <- 180
n.men.height <- length(men.height)
```

```{r}
mean.men.height
sd.men.height
average.men.height
n.men.height
```


### Voraussetzungsprüfung

*Normalverteilt?*
  
**Variante 1**: Inferenzstatistik: Der **Shapiro-Wilk Test** prüft auf die Normalverteilung von Variablen. 
Der Wert *W* ist die Teststatistik. 
Eine Normalverteilung in der Population liegt vor, wenn das Ergebnis NICHT signifikant ist.

```{r}
shapiro.test(men.height)
```

**Variante 2**: Grafisch: In einem sog. QQ-Plot werden die unter der Normalverteilung erwarteten Quantile und die tatsächlich beobachteten Quantile in einem Streudiagramm dargestellt. Je deutlicher die Punkte auf der Geraden liegen, desto näher ist die beobachtete Verteilung an der Normalverteilung.

```{r}
qqnorm(men.height) 
qqline(men.height)
```

Beide Varianten lassen darauf schließen, dass die Annahme auf Normalverteilung nicht verworfen werden muss.

### Signifikanz bestimmen

Der t-Test basiert auf folgender Formel und unterscheidet sich damit nur minimal vom z-Test:
  
$$t_{emp} = |\frac{\bar{x} - {\mu}}{\hat\sigma_{\bar{x}}}|$$
wobei sich der Standardfehler (*SE*)  des Mittelwerts wie folgt berechnet:
  
$$\hat\sigma_{\bar{x}} = {\frac{{\hat\sigma}}{\sqrt{n}}}$$
  
  
Der Standardfehler des Mittelwerts wird demnach auf der Basis der Stichprobenvarianz geschätzt.

```{r}
se.men.height <- sd.men.height/sqrt(n.men.height)
```

Bestimmen des empirischen *t-*Wertes $t_{emp}$:
  
```{r}
t.men.height <- abs((mean.men.height-average.men.height)/se.men.height)
t.men.height
```

Bestimmen des kritischen *t-*Wertes $t_{krit}$:

```{r}
krit.t.men.height <- qt(0.95, df=n.men.height-1) 
krit.t.men.height
```

Ist der empirische größer als der kritische *t-*Wert ($t_{emp} > t_{krit}$)?
  
```{r}
t.men.height > krit.t.men.height
```

Der empirische *t-*Wert wird hier nicht überboten.

Alternativ: Bestimmen des kritischen *p*-Wertes $p_{krit}$:

```{r}
p.t.men.height <- pt(t.men.height, n.men.height-1, lower.tail = F) #einseitige Testung
p.t.men.height
```

Der empirische *p*-Wert liegt über .05 ($p_{emp} > p_{krit}$). 

Die Differenz zwischen dem Mittelwert der Population $\mu$ und dem beobachteten Mittelwert $\bar{x}$ in der Stichprobe ist nicht signifikant. Demnach wird die $H_0$ mit einer Irrtumswahrscheinlichkeit von 5% beibehalten.

### t-test mit `t.test` Funktion

Natürlich geht alles auch noch einfacher:
  
```{r, echo = FALSE}
height_test <- t.test(men.height, mu=180, alternative="greater") #alternative bestimmt, ob die Hypothese gerichtet ist oder nicht. Siehe hierzu ?t.test.
```

```{r}
t.test(men.height, mu=180, alternative="greater") #alternative bestimmt, ob die Hypothese gerichtet ist oder nicht. Siehe hierzu ?t.test.
```

Hier haben wir nun alle wichtigen Informationen gebündelt. 

`t` = $t_{emp}$ = `r height_test$statistic`

`df` = Freiheitsgrade = `r height_test$parameter`

`p-value` = $p_{emp}$ = `r height_test$p.value`

`mean of x` = $\bar{x}$ = `r height_test$estimate`

Wir erkennen auch hier, dass der empirische *p*-Wert über .05 liegt ($p_{emp} > p_{krit}$). Demnach wird die $H_0$ mit einer Irrtumswahrscheinlichkeit von 5% beibehalten.

Das 95%ige Konfidenzintervall wird uns ebenfalls ausgegeben. Beachten Sie, dass es sich aufgrund unserer Hypothese um ein einseitiges Intervall handelt (nach oben offen). Basierend auf der Stichprobe liegt der wahre Wert $\mu$ zwischen `r height_test$conf.int[1]` und $\infty$. Man erkennt also, dass der Wert von 180 in diesem Konfidenzintervall liegt, was ebenso bestätigt, dass es keinen Unterschied gibt.


## Beispiel mit unserem Datensatz

Den Datensatz `fb21.rda`, mit dem im Folgenden gearbeitet wird, können Sie [<i class="fas fa-download"></i> hier herunterladen](/post/fb21.rda). Anschließend müssen Sie ihn lokal einladen.

```{r, eval = F}
load('fb21.rda')
```

Alternativ können Sie ihn auch direkt über den folgenden Befehl in `R` laden:

```{r}
load(url('https://pandar.netlify.app/post/fb21.rda'))
```


**Unterscheidet sich unsere studentische Stichprobe in ihrem Neurotizismuswert von Studierenden im Allgemeinen?** 
  Wir nehmen an, dass der mittlere Neurotizismuswert in der Population der Studierenden bei $\mu$ = 3.3 liegt. 

 1. Ist die erste Voraussetzung erfüllt?
 2. Normalverteilungsananahme darf verletzt sein (verzerrt das Ergebnis des t-Tests nicht), wenn die Stichprobe mindestens 30 Personen umfasst. Dann gilt der *zentrale Grenzwertsatz*: "Die Stichprobenkennwertverteilung nähert sich einer Normalverteilung an, selbst wenn diese nicht normalverteilt ist."

Bevor wir in die inferenzstatistische Analyse einsteigen, ist es immer gut, sich einen Überblick über die deskriptiven Werte zu verschaffen. Wir können nun natürlich einfach die bereits gelernten Funktionen zu Mittelwert, Varianz, Minimum, etc. nutzen. Doch gibt es einen schnelleren Weg? Die Basisinstallation von `R` bietet uns keine Alternative. Jedoch gibt es zusätzliche *Pakete*, die den Pool an möglichen Funktionen erweitern. Die Logik wird im Folgenden erläutert.

### Wie können andere Funktionen in R genutzt werden? - Library und Pakete

R ist in einer Pakete-Logik aufgebaut. Das liegt daran, dass es immer mehr Funktionen in R gibt, die aber nie jemand alle gleichzeitig brauchen wird. Zur Schonung der Kapazität sind diese Funktionalitäten also in Pakete aufgeteilt. In *Basispaketen*, die standardmäßig geladen werden (also vorinstalliert sind beim Öffnen von R) sind grundlegende Befehle und Analysen implementiert (Beispiele für solche Basispakete sind `base`, `stats`, `graphics`). Für spezifischere Analysen (also Funktionen) müssen *Zusatzpakete* teilweise erst installiert, zumindest aber immer per Hand geladen werden (Beispiele sind `psych`, `car`, `ggplot2`). Nur die Funktionen von erst installierten und dann geladenen Paketen können in einem Skript benutzt werden.

Unter dem Reiter *Packages* wird die *Library* angezeigt. Hier sind alle Pakete enthalten, die einmal installiert wurden. Pakete müssen ab und zu (per Hand) aktualisiert werden. 


![](/post/library.JPG)

Sobald Sie eigene Pakete installiert haben, gibt es in dem Reiter *Packages* die Einteilung in die *Sytem Library* (also standardmäßig installierte Pakete) und die *User Library* (von Ihnen installierte Pakete).

Die folgenden Bilder verdeutlichen nochmal das Prinzip von Installieren und Laden. Bei der Installation von R werden die Basispakete automatisch in die Library installiert. Zusatzpakete müssen mit der Funktion `install.packages()` mit dem Paketnamen als Argument installiert werden. Hierzu ist meist eine Internetverbindung nötig.


![](/post/pakete_installieren.JPG)


Beim Start von R werden die Basispakete automatisch geladen. Zusatzpakete müssen hingegen mit der Funktion `library()` mit dem Paketnamen als Argument geladen werden.


![](/post/pakete_laden.JPG)


Gehen wir das Prinzip an dem Beispielpaket `psych` durch, das verschiedene Operationen enthält, die in der psychologischen Forschung häufig benötigt werden. Die Installation muss dem Laden des Paketes logischerweise vorausgestellt sein. Wenn R einmal geschlossen wird, müssen alle Zusatzpakete neu geladen, jedoch nicht neu installiert werden.

```{r, eval = FALSE}
install.packages('psych')          # installieren
```

```{r}
library(psych)                     # laden
```

Wir erhalten hier als Warning Message den Hinweis, unter welcher Version das Paket erstellt wurde.
Eine kleine Suche nach Hilfe zu Pakete kann man mit `??`erhalten.

```{r, eval = F}
??psych                          # Hilfe
```

Da das Paket `psych` nun geladen ist, können wir Funktionen aus diesem nutzen. Für unsere Übersicht über deskriptive Maße der Variable `neuro` gibt es die Funktion `describe`.

```{r}
describe(fb21$neuro)
```

Wir bekommen auf einen Schlag sehr viele relevante Informationen über unsere Variable. Der Mittelwert unserer Stichprobe liegt beispielsweise bei `r mean(fb21$neuro)`. Beachten Sie, dass auch bei `describe` unter `sd` die geschätzte Populationsstandardabweichung angegeben wird (wie bei der Basis-Funktion `sd`). Man müsste Sie also auch umrechnen, um nur eine Angabe über die Stichprobe zu machen. 

### Hypothesengenerierung

**Variante 1**:

**Ungerichtete $H_1$**: "Der mittlere Neurotizismuswert unserer Stichprobe unterscheidet sich vom mittleren Neurotizismuswert der Studierenden-Population" 
--> zweiseitiger t-Test

**Ungerichtete $H_0$**: "Der mittlere Neurotizismuswert unserer Stichprobe unterscheidet sich nicht vom mittleren Neurotizismuswert der Studierenden-Population"

**Variante 2**:

**Gerichtete $H_1$**: "Der mittlere Neurotizismuswert unserer Stichprobe ist höher (niedriger) als der mittlere Neurotizismuswert der Studierenden-Population."
--> einseitiger t-Test

**Gerichtete $H_0$**: "Der mittlere Neurotizismuswert unserer Stichprobe ist gleich oder niedriger (höher) als der mittlere Neurotizismuswert der Studierenden-Population."

In der Praxis würde man sich für eine der beiden Hypothesen-Varianten (*ungerichtet* vs. *gerichtet*) entscheiden. Zu Übungszwecken werden aber alle beide Varianten durchgespielt.

```{r}
t.test(fb21$neuro, mu=3.3) #ungerichtet
t.test(fb21$neuro, mu=3.3, alternative="less") #gerichtet, Stichprobenmittelwert geringer
t.test(fb21$neuro, mu=3.3, alternative="greater") #gerichtet, Stichprobenmittelwert höher
```

**Konfidenzintervall:** Wir erkennen, dass das 95%-ige Konfidenzintervall per Standardeinstellung berechnet wird. Falls wir dieses vergrößern oder verkleinern wollen, müssen wir dies explizit formulieren im Argument `conf.level`:
  
```{r}
t.test(fb21$neuro, mu=3.3, conf.level=0.99) #99%-iges Konfidenzintervall für die ungerichtete Hypothese
t.test(fb21$neuro, mu=3.3, alternative="less", conf.level=0.99) #99%-iges Konfidenzintervall für die gerichtete Hypothese (Stichprobenmittelwert geringer)
t.test(fb21$neuro, mu=3.3, alternative="greater", conf.level=0.99) #99%-iges Konfidenzintervall für die gerichtete Hypothese (Stichprobenmittelwert höher)
```

Es zeigt sich, dass der Neurotizismuswert der Studierenden sich von der Studierenden-Population nicht unterscheidet.

**Ungerichtet**

Die $H_0$ wird mit einer Irrtumswahrscheinlichkeit von 5% beibehalten. Der Neurotizismuswert der Studierenden unterscheidet sich nicht von der Studierenden-Population.

**Gerichtet (niedriger)**

Die $H_0$ wird mit einer Irrtumswahrscheinlichkeit von 5% beibehalten. Der Neurotizismuswert der Studierenden ist nicht kleiner als der der Studierenden-Population.
*Bemerke*: Einen gerichteten t-Test, der $\bar{x} < \mu$ untersucht, würde man an dieser Stelle nicht durchführen, da die deskriptiven Werte schon gegen die Hypothese sprechen (da $\bar{x} > \mu$ und nicht $\bar{x} < \mu$).

**Gerichtet (höher)***

Die $H_0$ wird mit einer Irrtumswahrscheinlichkeit von 5% beibehalten. Der Neurotizismuswert der Studierenden ist nicht höher als der der Studierenden-Population.

***

## Effektgröße

Als Effektgröße für Mittelwertsunterschiede kann **Cohen's d** (Cohen, 1988) verwendet werden. 

Cohen, J. (1988). *Statistical power analysis for the Behavioral Sciences*. Routledge.

Dieses statistische Effektmaß beschreibt die Relevanz von signifikanten Ergebnissen. Zudem kann es verwendet werden, um den Effekt über verschiedene Studien hinweg zu vergleichen.

$$d = |\frac{\bar{x} - {\mu}}{\sigma}|$$

```{r}
mean_Neuro <- mean(fb21$neuro) #Neurotizismuswert der Stichprobe
sd_Neuro <- sd(fb21$neuro, na.rm = T) #Stichproben SD (Populationsschätzer)
mean_Popu_Neuro <- 3.3 #Neurotizismuswert der Grundgesamtheit
d <- abs((mean_Neuro-mean_Popu_Neuro)/sd_Neuro) #abs(), da Betrag
d
```

Die Effektgröße ist in diesem Fall mit einem Wert von .1469 klein.

Es gilt nach Cohen (1988):

*d* $\geq$ 0.14 -> kleiner Effekt

*d* $\geq$ 0.35 -> mittlerer Effekt

*d* $\geq$ 0.57 -> großer Effekt

***
