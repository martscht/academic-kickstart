---
title: Selektionseffekte
date: '2020-11-06'
slug: selektionseffekte
categories:
  - MSc5
tags:
  - Selektion
  - Heckman
  - Bias
  - lineares Modell
  - Normalverteilung
subtitle: ''
summary: ''
authors: [irmer]
lastmod: '2021-01-14T19:22:32+02:00'
featured: no
header:
  image: "/header/MSc5_Selection_post.jpg"
  caption: "[Courtesy of pxhere](https://pxhere.com/en/photo/545043)"
projects: []
---



<div id="einleitung" class="section level2">
<h2>Einleitung</h2>
<p>Selektionseffekte können drastische Auswirkungen auf Datenanalysen haben. Sie treten auf, wenn die Stichprobe nicht repräsentativ für die Grundgesamtheit ist, da nur ein ganz bestimmter Teil beobachtbar war. Hängt die Selektion von einer Variable ab, die wir erhoben haben, so können wir diesen Effekt herausrechnen. Einer der Ersten denen dies gelang war der Ökonom James J. Heckman (<a href="https://hds.hebis.de/ubffm/EBSCO/Record?id=edsjsr.10.2307.1912352%7Cedsjsr">Heckman, 1979</a>, siehe auch <a href="https://hds.hebis.de/ubffm/EBSCO/Record?id=ED478203%7Ceric">Briggs, 2004</a>).</p>
</div>
<div id="selektionsbias-am-heckman-modell" class="section level2">
<h2>Selektionsbias am Heckman Modell</h2>
<p>Entsprechend wollen wir den Selektionsbias an dem einfachsten “Heckman”-Modell untersuchen. Dazu schauen wir uns ein Modell mit einer Prädiktorvariable an.</p>
<p><span class="math display">\[Y_i = b_0 + b_1X_i + \underbrace{e_i}_{rZ_i+\varepsilon_i}\]</span>
<span class="math inline">\(b_0\)</span> und <span class="math inline">\(b_1\)</span> sind hier die Regressionskoeffizienten in der Population, welche es zu schätzen gilt. <span class="math inline">\(X_i\)</span> ist der Prädiktor und <span class="math inline">\(e_i\)</span> ist das Regressionsresiduum der Person/Erhebung <span class="math inline">\(i\)</span> (<span class="math inline">\(i=1,\dots,n\)</span>, wobei <span class="math inline">\(n\)</span> die Gesamtstichprobengröße ist). Das allgemeine Modell wird natürlich für beliebig viele Prädiktoren formuliert. Die Idee von Heckman war nun, dass, wenn sich bspw. nur bestimmte Personen (das Ganze kann natürlich auch für den nicht-psychologischen Kontext formuliert werden) bereiterklären an der Studie teilzunehmen, dann werden die Regressionsgewichte verschätzt. Wenn nun die Selektion auf eine ganz bestimmte Weise abläuft, dann lässt sich dieser Bias wieder herausrechnen. Heckman nahm dazu an, dass die Selektion dem sogenannten Probit-Modell folgt. Das Probit-Modell ist, ähnlich wie das logistische Modell (siehe bspw. <a href="/post/logistische-regression">logistische Regressionssitzung in <code>R</code></a>), ein Regressionsmodell zur Modellierung dichotomer abhängiger Variablen.</p>
<div id="probit-modell-der-selektionsmechanismus" class="section level3">
<h3>Probit-Modell: der Selektionsmechanismus</h3>
<p>Und zwar ist im Probit-Modell die Wahrscheinlichkeit selektiert zu werden (Erfolg zu haben) nicht die logistische (Ogiven)-Funktion, wie in der logistischen Regression, sondern die Verteilungsfunktion der Standardnormalverteilung <span class="math inline">\(\Phi\)</span>. Dies hört sich zunächst sehr kompliziert an, ist es aber eigentlich nicht. Wir schauen uns den <em>Selektionsmechanismus</em> an, um dieser Sache auf den Grund zu gehen. Eine Person nimmt an der Studie teil, wenn sie sich selbst in diese hinein selektiert, da sie bspw. ein bestimmtes Mindset hat und somit eine bestimmte Variablenkonstellation aufweist, die sie mit erhöhter Wahrscheinlichkeit an der Studie teilnehmen lässt.</p>
<p><span class="math display">\[s_i = 1 \Longleftrightarrow Z_i \le \beta_0 + \beta_1X_i\]</span>
und <span class="math display">\[s_i=0 \text{ sonst}\]</span></p>
<p><span class="math inline">\(\Longleftrightarrow\)</span> bedeutet “<em>genau dann wenn</em>”. Ist <span class="math inline">\(s_i=1\)</span>, so wird selegiert (die Person nimmt teil). <span class="math inline">\(Z_i\)</span> ist standardnormal-verteilt und <span class="math inline">\(\beta_0\)</span> und <span class="math inline">\(\beta_1\)</span> sind die Regressionsgewichte der Selektion. Die Variable (hier <span class="math inline">\(X\)</span>), die für die Selektion zuständig ist, ist beliebig. Sie muss nur bekannt und beobachtet sein! Wir schauen uns den (speziellen) Fall an, in welchem auch der Prädiktor aus der Regression für die Selektion zuständig ist. Dies könnte zum Beispiel der Fall sein, wenn wir den Therapieeffekt einer langwierigen Therapieform untersuchen wollen (Symptomstärke nach vollendeter Therapie = <span class="math inline">\(Y\)</span>) und diese durch das Commitment (<span class="math inline">\(X\)</span>) vorhersagen. Menschen, die mehr Commitment zur Therapie zeigen, ziehen die Therapie wahrscheinlicher bis zum Ende durch und werden somit mit höherer Wahrscheinlichkeit in den Pool der beobachteten Stichprobe selegiert. Der Ausdruck <span class="math inline">\(Z_i \le \beta_0 + \beta_1X_i\)</span> beschreibt diese Phänomen nochmals. Wir wissen aus früheren Semestern, dass die Standardnormalverteilung einen Mittelwert von 0 hat und eine Standardabweichung von 1. Somit ist <span class="math inline">\(Z_i\)</span> genau dann mit hoher Wahrscheinlichkeit kleiner oder gleich groß wie <span class="math inline">\(\beta_0 + \beta_1X_i\)</span> (und damit würde <span class="math inline">\(s_i=1\)</span> ausfallen), wenn dieser Ausdruck groß ist. Schauen wir uns dies einmal an: Angenommen <span class="math inline">\(\beta_0 + \beta_1X_i = -1\)</span> (kleiner Wert), dann ist <span class="math inline">\(Z_i\)</span> sehr häufig größer als <span class="math inline">\(-1\)</span>.</p>
<p><img src="/post/2020-11-06-MSc5_Selektionseffekte_files/figure-html/unnamed-chunk-1-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>In schwarz sehen wir den Bereich, in welchem <span class="math inline">\(Z_i\)</span> kleiner als <span class="math inline">\(-1\)</span> ausfällt und somit selegiert werden würde. Sie erinnern sich vielleicht, dass bei einer Standardnormalverteilung gerade der Wert -1 bedeutet, dass die Variable eine Standardabweichung kleiner als der Mittelwert (0) ausfällt. In blau sehen wir außerdem die Dichte der Standardnormalverteilung. Die vertikale gestrichelte Linie repräsentiert hier den Wert -1. Nehmen wir nun an, dass <span class="math inline">\(\beta_0 + \beta_1X_i = 2\)</span> (großer Wert), dann ist <span class="math inline">\(Z_i\)</span> sehr häufig kleiner als <span class="math inline">\(2\)</span>.</p>
<p><img src="/post/2020-11-06-MSc5_Selektionseffekte_files/figure-html/unnamed-chunk-2-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Für den Code der Grafiken, siehe in <a href="#AppendixA">Appendix A</a> nach. Die schwarze Fläche ist genau die Wahrscheinlichkeit, dass die Standardnormalverteilung einen Wert kleiner der oberen Grenze annimmt: also <span class="math inline">\(\mathbb{P}(Z_i\le\beta_0+\beta_1X_i)=\Phi(\beta_0+\beta_1X_i)\)</span>. Diese fällt für einen Wert von 2 deutlich größer als für einen Wert von -1 aus! In <code>R</code> lässt sich diese Wahrscheinlichkeit sehr leicht bestimmen:</p>
<pre class="r"><code>set.seed(123456)                 # Vergleichbarkeit
Z &lt;- rnorm(10000)                # 10^4 std. normalverteilte Zufallsvariablen simulieren
pnorm(q = -1, mean = 0, sd = 1)  # Theoretische Wahrscheinlichkeit, dass Z &lt;= -1</code></pre>
<pre><code>## [1] 0.1586553</code></pre>
<pre class="r"><code>mean(Z &lt;= -1)                    # Empirische (beoabachtete) Wahrscheinlichkeit, dass Z &lt;= -1</code></pre>
<pre><code>## [1] 0.1544</code></pre>
<pre class="r"><code>pnorm(q = 2, mean = 0, sd = 1)   # Theoretische Wahrscheinlichkeit, dass Z &lt;= 2</code></pre>
<pre><code>## [1] 0.9772499</code></pre>
<pre class="r"><code>mean(Z &lt;= 2)                     # Empirische (beoabachtete) Wahrscheinlichkeit, dass Z &lt;= 2</code></pre>
<pre><code>## [1] 0.9745</code></pre>
<p>Die beobachtete Wahrscheinlichkeit, dass die generierte Variable <code>Z</code> kleiner oder gleich groß wie -1 bzw. wie 2 ist, liegt sehr nah an der theoretischen dran (<code>Z &lt;= -1</code> bzw. <code>Z &lt;= 2</code> wandeln den Vektor <code>Z</code> in <code>TRUE</code> und <code>FALSE</code> um, was dann mit der <code>mean</code> Funktion in die relative Häufigkeit von <code>TRUE</code>, was <code>R</code>-intern als 1 verstanden wird, bestimmt; <code>FALSE</code> wird <code>R</code>-intern als 0 verstanden; siehe auch <a href="/post/logistische-regression">Sitzung zur logistischen Regression</a> um relative Häufigkeiten von 01-Folgen zu wiederholen oder in der <a href="/post/simulationsstudien-in-r">Sitzung zu Simulationsstudien in <code>R</code></a>, in welcher dies zur Berechnung der Power etc. verwendet wurde). Analog zur logistischen Regression wählen wir <span class="math inline">\(p=\mathbb{P}(s_i=1|X_i)=\mathbb{P}(Z_i\le\beta_0+\beta_1X_i)=\Phi(\beta_0+\beta_1X_i)\)</span> und bestimmen die Gewichte mit der Maximum Likelihood Methode. Dies geschieht in der <code>glm</code>-Funktion, die wir <a href="/post/logistische-regression">Sitzung zur logistischen Regression</a> behandelt haben. Hier muss lediglich das <code>family</code>-Argument angepasst werden: <code>family = binomial(link = "probit")</code>:</p>
<p><a id="glmProbit"></a></p>
<pre class="r"><code>set.seed(1234567)
n &lt;- 1000
Z &lt;- rnorm(n = n, mean = 0, sd = 1)
X &lt;- rnorm(n = n, mean = 2, sd = 5) # die Verteilung von X ist beliebig

# Selektion
beta0 &lt;- -2
beta1 &lt;- .5
s &lt;- beta0 + beta1*X + Z &gt; 0

probit_model &lt;- glm(s ~ 1 + X, family = binomial(link = &quot;probit&quot;))
summary(probit_model)</code></pre>
<pre><code>## 
## Call:
## glm(formula = s ~ 1 + X, family = binomial(link = &quot;probit&quot;))
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -2.48089  -0.30945  -0.01573   0.11969   2.94812  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -2.10216    0.13720  -15.32   &lt;2e-16 ***
## X            0.50715    0.03248   15.61   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1279.40  on 999  degrees of freedom
## Residual deviance:  488.79  on 998  degrees of freedom
## AIC: 492.79
## 
## Number of Fisher Scoring iterations: 7</code></pre>
<p>Der Output gleicht fast vollkommen dem der logistischen Regression. Lediglich die <code>family</code>-Spezifikation ist eine andere. Der Output ist analog zum logistischen Modell (Logit-Modell) zu interpretieren. Wir sehen, dass die Schätzungen von <span class="math inline">\(\beta_0\)</span> und <span class="math inline">\(\beta_1\)</span> recht nah an den vorgegebenen wahren Werten -2 und 0.5 dran liegen! Dies bedeutet also, dass wir mit der Probit-Regression die Koeffizienten <span class="math inline">\(\beta_0\)</span> und <span class="math inline">\(\beta_1\)</span> schätzen können.</p>
</div>
<div id="die-populationsregressionsgleichung" class="section level3">
<h3>Die Populationsregressionsgleichung</h3>
<p>Selegieren wir unsere Stichprobe anhand des eben vorgestellten Selektionsmechanismus, so folgt, dass die Residuum der Regressionsanalyse nicht länger unabhnängig von den Prädiktoren ist. Dies können wir in der Populationsgleichung auch explizit vermerken, indem wir <span class="math inline">\(Z\)</span> in die Gleichung mit aufnehmen. Im Endeffekt ist dies nur eine Umformulierung des Residuums, welche auf Grund der Normalverteilung der Residuuen geschieht (genaue Begründungen gehen über das nötige Wissen für diese Sitzung hinaus, aber im Grunde ist die Kovarianz zwischen <span class="math inline">\(e_i\)</span> und <span class="math inline">\(X_i\)</span> in Gruppe <span class="math inline">\(s_i=1\)</span> gegeben durch den Parameter <span class="math inline">\(r\)</span> sowie den Selektionsparameter <span class="math inline">\(\beta_1\)</span>):</p>
<p><span class="math display">\[Y_i = b_0+b_1X_i +rZ_i+\varepsilon_i,\]</span>
wobei das Residuum zuvor <span class="math inline">\(e_i=rZ_i+\varepsilon_i\)</span> war. <span class="math inline">\(Z\)</span> ist hier das <span class="math inline">\(Z\)</span> von der Selektion - diese ist auf Populationsebene allerdings eine latente (nicht beobachtbare) Variable. Wäre die Stichprobe repräsentativ, so wäre <span class="math inline">\(X\)</span> und <span class="math inline">\(Z\)</span> unkorreliert und wir hätten kein Problem. Würden jedoch nur bspw. im Vergleich zu <span class="math inline">\(Z\)</span> große <span class="math inline">\(X\)</span> mit erhöhter Wahrscheinlichkeit in die Stichprobe gewählt, so resultiert eine Korrelation zwischen <span class="math inline">\(X\)</span> und <span class="math inline">\(Z\)</span>, die die Parameterschätzung verzerrt. Damit kommt es zu einem Parameterbias, falls die Selektion tatsächlich von anderen Variablen (hier <span class="math inline">\(X\)</span>) abhängt (hier also <span class="math inline">\(\beta_1\neq0\)</span>). Um dennoch sinnvolle Schätzungen für <span class="math inline">\(b_0\)</span> und <span class="math inline">\(b_1\)</span> abgegeben zu können, müssen wir das <span class="math inline">\(Z\)</span> in der Populationsgleichung durch dessen (bedingte) Erwartung annähern. Glücklicherweise müssen wir das nicht mehr nachrechnen. Das haben andere bereits für uns erledigt. Die (bedingte) Erwartung für <span class="math inline">\(Z_i\)</span> ist gerade das inverse von Mills-Ratio (<span class="math inline">\(\lambda\)</span>), welches von <span class="math inline">\(\beta_0, \beta_1\)</span> und <span class="math inline">\(X\)</span> abhängt:
<span class="math display">\[\lambda(X)=\frac{\phi(\beta_0+\beta_1X)}{\Phi(\beta_0+\beta_1X)},\]</span>
wobei <span class="math inline">\(\phi\)</span> die Dichte- (in <code>R</code>: <code>dnorm(x = ?, mean = 0, sd = 1)</code>) und <span class="math inline">\(\Phi\)</span> die Verteilungsfunktion (in <code>R</code>: <code>pnorm(q = ?, mean = 0, sd = 1)</code>) der Standardnormalverteilung ist.</p>
</div>
<div id="einfluss-der-selektion" class="section level3">
<h3>Einfluss der Selektion</h3>
<p>Um den Einfluss auf die Regression genauer zu erkennen, schauen wir uns ein simuliertes Beispiel in <code>R</code> an: Wir beginnen damit die Daten für die Populationsregression zu simulieren: <code>b0</code> und <code>b1</code> sind die Regressionskoeffizienten <span class="math inline">\(b_0 = 0.5\)</span> und <span class="math inline">\(b_1 = 1.2\)</span>, <code>X</code> und <code>Z</code> sind <span class="math inline">\(X\)</span> und <span class="math inline">\(Z\)</span>, <code>r</code> ist <span class="math inline">\(r = 2\)</span> in der Populationsgleichung und <code>sigma</code> ist die Standardabweichung (= 2) des unabhängigen Teils des Residuums <span class="math inline">\(\varepsilon\)</span>.</p>
<pre class="r"><code>set.seed(1234567)
n &lt;- 1000
X &lt;- rnorm(n = n, mean = 2, sd = 2)
Z &lt;- rnorm(n = n, mean = 0, sd = 1)

# Populationsregression
b0 &lt;- 0.5
b1 &lt;- 1.2
r &lt;- 2
sigma &lt;- 2

eps &lt;- rnorm(n = n, mean = 0, sd = sigma)
Y &lt;- b0 + b1*X + r*Z + eps # Populationsregression


# Selektion
beta0 &lt;- -2
beta1 &lt;- .5
s &lt;- beta0 + beta1*X + Z &gt; 0   # Selektionsmechanismus

Y_obs &lt;-rep(NA, length(Y))
Y_obs[s == 1] &lt;- Y[ s== 1]     # beobachtbares Y</code></pre>
<p>Nachdem wir <span class="math inline">\(Y\)</span> simuliert haben (als gäbe es keinen Selektionseffekt), fügen wir die Selektion nachträglich hinzu. Dafür erstellen wir zunächst einen leeren Vektor <code>Y_obs</code> mit dem <code>reps</code> Befehl, den wir aus der letzten <a href="/post/simulationsstudien-in-r">Sitzung zu Simulationsstudien in <code>R</code></a> kennen. Anschließend füllen wir alle Stellen des Vektors an denen <span class="math inline">\(s\)</span> gleich 1 ist, also an dem selegiert wurde (in <code>R</code>: <code>s == 1</code>), mit den entsprechenden Werten aus <span class="math inline">\(Y\)</span> auf. Wenn wir nun eine Regression rechnen und nur die beobachteten Werte verwenden (<code>R</code> führt per Default list-wise deletion: listenweisen Fallausschluss) durch, so erkennen wir, dass die Regressionsparameter deutlich verschätzt sind. Das Modell dazu nennen wir <code>reg_obs</code> für Regressionsobjekt observed. Führen wir hingegen eine Regression auf Populationsebene durch, indem wir <code>Y</code> anstatt <code>Y_obs</code> verwenden, so liegen die Regressionsschätzer hier sehr nah an den wahren Werten. Hierbei müssen wir unbedingt beachten, dass <code>Z</code> in beiden Gleichungen nicht auftaucht (die wahren Werte sind: <span class="math inline">\(b_0 = 0.5\)</span> und <span class="math inline">\(b_1 = 1.2\)</span>):</p>
<pre class="r"><code>reg_obs &lt;- lm(Y_obs ~ X)
reg_obs</code></pre>
<pre><code>## 
## Call:
## lm(formula = Y_obs ~ X)
## 
## Coefficients:
## (Intercept)            X  
##      5.4403       0.3922</code></pre>
<pre class="r"><code>reg_pop &lt;- lm(Y ~ X)
reg_pop</code></pre>
<pre><code>## 
## Call:
## lm(formula = Y ~ X)
## 
## Coefficients:
## (Intercept)            X  
##      0.5175       1.1619</code></pre>
<p>Die Unterschiede liegen daran, dass <span class="math inline">\(X\)</span> und <span class="math inline">\(Z\)</span> korreliert sind in der selegierten Stichprobe:</p>
<pre class="r"><code>cor(X[s==1], Z[s==1])</code></pre>
<pre><code>## [1] -0.6101458</code></pre>
<p>Während sie es nicht sind in der repräsentativen Gesamtstichprobe:</p>
<pre class="r"><code>cor(X, Z)</code></pre>
<pre><code>## [1] -0.03086995</code></pre>
<p>Wir können das Ganze auch mit einen Signifikanztest untermauern:</p>
<pre class="r"><code>cor.test(X[s==1], Z[s==1])</code></pre>
<pre><code>## 
##  Pearson&#39;s product-moment correlation
## 
## data:  X[s == 1] and Z[s == 1]
## t = -11.628, df = 228, p-value &lt; 2.2e-16
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  -0.6854068 -0.5219864
## sample estimates:
##        cor 
## -0.6101458</code></pre>
<pre class="r"><code>cor.test(X, Z)</code></pre>
<pre><code>## 
##  Pearson&#39;s product-moment correlation
## 
## data:  X and Z
## t = -0.97568, df = 998, p-value = 0.3295
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  -0.09268566  0.03118280
## sample estimates:
##         cor 
## -0.03086995</code></pre>
<p>Nur im repräsentativen Fall ist die Korrelation nicht signifikant von 0 verschieden!</p>
<p>Diese Phänomen lässt sich auch sehr gut in einer Grafik veranschaulichen:</p>
<p><img src="/post/2020-11-06-MSc5_Selektionseffekte_files/figure-html/unnamed-chunk-10-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Hier repräsentieren die hellblauen Punkte die repräsentative Stichprobe. Die schwarzen Punkte hingegen stellen die selegierte/verzerrte Stichprobe (biased sample) dar. Genauso zeigt die blaue Linie die korrekte Regressionsgerade, während die goldene Linie die verzerrte darstellt (für den Code der Grafik, siehe <a href="#AppendixA">Appendix A</a>). Wir erkennen deutlich die Verzerrung der Ergebnisse. Sie können sich die Verzerrung wie folgt erklären: die Regression wird unter der Annahme der Unabhängigkeit der Residuuen von den Prädiktoren in der Regressionsgleich geschätzt. Da aber <span class="math inline">\(e_i=rZ_i+\varepsilon_i\)</span> gilt und wir weiter oben gesehen haben, dass <span class="math inline">\(X\)</span> und <span class="math inline">\(Z\)</span> in der Selektionsgruppe korreliert sind, bedeutet dies, dass das Residuum nicht länger unabhängig vom Prädiktor ist. Da dies aber eine Annahme ist und das Verfahren nur so geschätzt werden kann, werden die Parameter so verzerrt, damit die Residuen möglichst unsystematisch sind. Insbesondere der Mittelwert der Residuen muss 0 sein für jede Ausprägung von X. Es muss also gelten <span class="math inline">\(\mathbb{E}[e_i|X_i]=0\)</span>. Allerdings ist dies nicht der Fall in der Selektionsgruppe:</p>
<p><span class="math display">\[\begin{align}
\mathbb{E}[e_i|X_i, s_i = 1]&amp;=\mathbb{E}[rZ+\varepsilon_i|X_i]&amp;&amp; \\
&amp;= r\mathbb{E}[Z|X_i, s_i = 1] &amp;&amp;| \text{da }\varepsilon_i\text{ den unsystematischen Anteil enthält}\\
&amp;=r\lambda(X_i) &amp;&amp;| \text{mit } \lambda(X_i)=\frac{\phi(\beta_0+\beta_1X_i)}{\Phi(\beta_0+\beta_1X_i)}, \text{ wie oben motiviert}\\
&amp;\neq0.&amp;&amp; | \text{ falls } r\neq0 \text{ und }\beta_1\neq0
\end{align}\]</span></p>
<p>Die Umformung und Herleitung gehen über den Stoff dieses Kurses hinaus und dienen nur zu Illustrationszwecken. Dennoch hebt dies hervor, welchen fatalen Fehler wir begehen können, wenn unsere Stichproben verzerrt sind und wie enorm wichtig es ist, dass die Daten unabhängig gezogen werden und repräsentativ sind!</p>
</div>
<div id="heckman-ansatz-zum-schätzen-der-regressionskoeffizienten-in-r" class="section level3">
<h3>Heckman Ansatz zum Schätzen der Regressionskoeffizienten in <code>R</code></h3>
<p>Nun wollen wir allerdings noch die Methode von Heckman verwenden, um doch noch zu den richtigen Regressionsparameter zu kommen. Die Idee ist hierbei, dass wir zunächst die Probit-Regression verwenden, um den Selektionseffekt zu schätzt, damit dann das inverse von Mills-Ratio bestimmen und dieses dann in die Regression von den tatsächlich beobachteten <span class="math inline">\(Y\)</span>-Werten mit aufnehmen und somit den Effekt von <span class="math inline">\(Z\)</span> herausrechnen können. Glücklicherweise müssen wir dies nicht selbst programmieren, sondern können einfach auf die Funktion <code>heckit</code> (abgeleitet vom Namensgeber Heckman) des <code>sampleSelection</code> Pakets von <a href="https://hds.hebis.de/ubffm/EBSCO/Record?id=edsbas.C30BA3A7%7Cedsbas">Toomet und Henningsen (2008)</a> zurückgreifen (dieses muss zuvor installiert werden: <code>install.packages("sampleSelection")</code>): Der Funktion <code>heckit</code> müssen wir zwei Argumente übergeben: die Selektionsgleichung (<code>selection = s ~ 1 + X</code>) sowie die Regressionsgleichung (<code>outcome = Y_obs ~ 1 + X</code>). Wir speichern das Ganze als Objekt <code>heckman</code> ab und schauen uns die Summary an:</p>
<pre class="r"><code>library(sampleSelection) # Paket laden
heckman &lt;- heckit(selection = s ~ 1 + X, outcome = Y_obs ~ 1 + X)
summary(heckman)</code></pre>
<pre><code>## --------------------------------------------
## Tobit 2 model (sample selection model)
## 2-step Heckman / heckit estimation
## 1000 observations (770 censored and 230 observed)
## 7 free parameters (df = 994)
## Probit selection equation:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -1.97383    0.10924  -18.07   &lt;2e-16 ***
## X            0.47668    0.03332   14.30   &lt;2e-16 ***
## Outcome equation:
##             Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept)  -1.4740     5.3634  -0.275   0.7835  
## X             1.3896     0.7893   1.761   0.0786 .
## Multiple R-Squared:0.0687,   Adjusted R-Squared:0.0605
##    Error terms:
##               Estimate Std. Error t value Pr(&gt;|t|)
## invMillsRatio   3.2594     2.4788   1.315    0.189
## sigma           3.5439         NA      NA       NA
## rho             0.9197         NA      NA       NA
## --------------------------------------------</code></pre>
<p>Die Summary sagt uns:</p>
<pre><code>##  --------------------------------------------
##  Tobit 2 model (sample selection model)
##  2-step Heckman / heckit estimation
##  1000 observations (770 censored and 230 observed)
##  7 free parameters (df = 994)</code></pre>
<p>dass hier das 2-Stufige Heckman-Modell angesetzt wurde. Insgesamt von den 1000 Beoachtungen waren 770 zensiert (nicht beobachtet) und nur 230 konnten beobachtet werden. Insgesamt wurden 7 Parameter geschätzt (<span class="math inline">\(\beta_0, \beta_1, b_0, b_1, r, \sigma\)</span> und <span class="math inline">\(\rho\)</span>, wobei <span class="math inline">\(\rho\)</span> eine Schätzung für die Korrelation zwischen den Residuen der Selektionsgleichung und der Regressionsgleichung ist - diesen Parameter haben wir bisher nicht benutzt).</p>
<pre><code>##  Probit selection equation:
##              Estimate Std. Error t value Pr(&gt;|t|)    
##  (Intercept) -1.97383    0.10924  -18.07   &lt;2e-16 ***
##  X            0.47668    0.03332   14.30   &lt;2e-16 ***</code></pre>
<p>ist quasi das Ergebnis der Probit-Regression. Dieses Ergebnis unterscheidet sich nur geringfügig von dem Ergebnis der Schätzung mit der <code>glm</code>-Funktion (siehe <a href="#glmProbit">oben</a>).</p>
<pre><code>##  Outcome equation:
##              Estimate Std. Error t value Pr(&gt;|t|)  
##  (Intercept)  -1.4740     5.3634  -0.275   0.7835  
##  X             1.3896     0.7893   1.761   0.0786 .
##  Multiple R-Squared:0.0687,   Adjusted R-Squared:0.0605</code></pre>
<p>zeigt das Regressionsergebnis. Hier ist deutlich zu sehen, dass die Standardfehler sehr groß sind und der Anteil erklärter Varianz klein. Dies kann allerdings auch sehr stark an der sehr geringen Stichprobengröße liegen. Das Heckman-Modell braucht im Vergleich zur normalen Regression deutlich größere Stichprobengrößen!</p>
<pre><code>##     Error terms:
##                Estimate Std. Error t value Pr(&gt;|t|)
##  invMillsRatio   3.2594     2.4788   1.315    0.189
##  sigma           3.5439         NA      NA       NA
##  rho             0.9197         NA      NA       NA
##  --------------------------------------------</code></pre>
<p>zeigt die übrigen Parameter im Modell. <code>invMillsRatio</code> ist hierbei der angenäherte Einfluss von <span class="math inline">\(Z\)</span> in der Regressionsgleichung - also eine Schätzung für <code>r</code>. <code>sigma</code> ist die unabhängige Residualvarianz in der Regressionsgleichung. Für diese und für <code>rho</code> ist keine Signifikanzentscheidung möglich.</p>
<p>Wir können, wie bei fast allen geschätzten Modellen, die Parameterschätzungen auch mit dem <code>coef</code> Befehl erhalten. Wenden wir diesen auf die Summary an, erhalten wir hier auch die Standardfehler:</p>
<pre class="r"><code>coef(summary(heckman))</code></pre>
<pre><code>##                 Estimate Std. Error     t value     Pr(&gt;|t|)
## (Intercept)   -1.9738300 0.10924269 -18.0683023 2.542551e-63
## X              0.4766792 0.03332402  14.3043738 2.410538e-42
## (Intercept)   -1.4739783 5.36337752  -0.2748228 7.835095e-01
## X              1.3896056 0.78931666   1.7605172 7.862765e-02
## invMillsRatio  3.2593587 2.47880769   1.3148897 1.888502e-01
## sigma          3.5439299         NA          NA           NA
## rho            0.9197018         NA          NA           NA</code></pre>
<p>So können die Koeffizienten auf leichte Weise weiterverwendet werden (bspw. für eine Simulationsstudie).</p>
<p><strong>Ergebnisinterpretation:</strong> Insgesamt ist keiner der Parameter im Regressionsmodell des Heckman-Modells signifikant. Da wir aber das Modell vorgegeben haben, wird dies daran liegen, dass die Stichprobengröße zu klein ausgefallen ist. Wir wissen nämlich, dass es Effekte gab!</p>
<p>Wiederholen Sie doch einmal gleiches Modell für eine Stichprobengröße von <span class="math inline">\(n=10^5\)</span> (Achtung: unter Umständen müssen Sie hier eine Weile auf das Ergebnis warten!). Sie können auch gerne die in <a href="#AppendixB">Appendix B</a> zu findende Funktion nutzen, mit welcher Sie das Heckman-Modell direkt als <code>data.frame</code> simulieren können und anschließend mit den oben besprochenen Mitteln auswerten können.</p>
<p>Wenn Sie erstmal einen vorgefertigten Datensatz untersuchen wollen, so laden Sie sich doch diesen herunter: <a href="/post/HeckData.rda"><svg style="height:0.8em;top:.04em;position:relative;" viewBox="0 0 512 512"><path d="M216 0h80c13.3 0 24 10.7 24 24v168h87.7c17.8 0 26.7 21.5 14.1 34.1L269.7 378.3c-7.5 7.5-19.8 7.5-27.3 0L90.1 226.1c-12.6-12.6-3.7-34.1 14.1-34.1H192V24c0-13.3 10.7-24 24-24zm296 376v112c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V376c0-13.3 10.7-24 24-24h146.7l49 49c20.1 20.1 52.5 20.1 72.6 0l49-49H488c13.3 0 24 10.7 24 24zm-124 88c0-11-9-20-20-20s-20 9-20 20 9 20 20 20 20-9 20-20zm64 0c0-11-9-20-20-20s-20 9-20 20 9 20 20 20 20-9 20-20z"/></svg> Datensatz “<code>HeckData.rda</code>”</a>. Sie können <code>HeckData.rda</code> auch analog herunterladen und direkt einladen:</p>
<pre class="r"><code>load(url(&quot;https://pandar.netlify.app/post/HeckData.rda&quot;))</code></pre>
<p>In diesem Datensatz sind zu in dieser Sitzung präsentierten Parametern (<span class="math inline">\(\beta_0=-2,\beta_1=0.5, b_0 = 0.5, b_1=1.2, r= 2,\sigma=2\)</span>) die Variablen <span class="math inline">\(X,Y_{obs}\)</span> und <span class="math inline">\(s\)</span> enthalten, sowie - zu Demonstrationszwecken - <span class="math inline">\(Z\)</span> und <span class="math inline">\(Y\)</span>.</p>
<p>Den gesamten <code>R</code>-Code, der in dieser Sitzung genutzt wird, können Sie <a href="https://raw.githubusercontent.com/jpirmer/MSc1_FEI/master/R-Scripts/2_Sample_Selection_Bias-RCode.R"><svg style="height:0.8em;top:.04em;position:relative;" viewBox="0 0 512 512"><path d="M216 0h80c13.3 0 24 10.7 24 24v168h87.7c17.8 0 26.7 21.5 14.1 34.1L269.7 378.3c-7.5 7.5-19.8 7.5-27.3 0L90.1 226.1c-12.6-12.6-3.7-34.1 14.1-34.1H192V24c0-13.3 10.7-24 24-24zm296 376v112c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V376c0-13.3 10.7-24 24-24h146.7l49 49c20.1 20.1 52.5 20.1 72.6 0l49-49H488c13.3 0 24 10.7 24 24zm-124 88c0-11-9-20-20-20s-20 9-20 20 9 20 20 20 20-9 20-20zm64 0c0-11-9-20-20-20s-20 9-20 20 9 20 20 20 20-9 20-20z"/></svg> hier herunterladen</a>.</p>
</div>
</div>
<div id="appendix" class="section level2">
<h2>Appendix</h2>
<div id="AppendixA" class="section level3">
<h3>Appendix A: Code zu <code>R</code> Grafiken</h3>
<pre class="r"><code>set.seed(123456) # Vergleichbarkeit
Z &lt;- rnorm(10000) # 10^4 std. normalverteilte Zufallsvariablen simulieren
h &lt;- hist(Z, breaks=50, plot=FALSE)
cuts &lt;- cut(h$breaks, c(-4,-1,4)) # Grenzen festlegen für Färbung
plot(h, col=cuts,  freq = F)
lines(x = seq(-4,4,0.01), dnorm(seq(-4,4,0.01)), col = &quot;blue&quot;, lwd = 3)
abline(v = -1, lwd = 3, lty = 3, col = &quot;gold3&quot;)</code></pre>
<p><img src="/post/2020-11-06-MSc5_Selektionseffekte_files/figure-html/unnamed-chunk-18-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>h &lt;- hist(Z, breaks=50, plot=FALSE)
cuts &lt;- cut(h$breaks, c(-4,2,4)) # Grenzen festlegen für Färbung
plot(h, col=cuts,  freq = F)
lines(x = seq(-4,4,0.01), dnorm(seq(-4,4,0.01)), col = &quot;blue&quot;, lwd = 3)
abline(v = 2, lwd = 3, lty = 3, col = &quot;gold3&quot;)</code></pre>
<p><img src="/post/2020-11-06-MSc5_Selektionseffekte_files/figure-html/unnamed-chunk-19-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>plot(X, Y, pch = 16, col = &quot;skyblue&quot;, cex = 1.5)
points(X, Y_obs, pch = 16, col = &quot;black&quot;)
abline(reg_pop, col = &quot;blue&quot;, lwd = 5)
abline(reg_obs, col = &quot;gold3&quot;, lwd = 5)
legend(x = &quot;bottomright&quot;, legend = c(&quot;all&quot;, &quot;observed&quot;, &quot;regression: all&quot;, &quot;regression: observed&quot;), col = c(&quot;skyblue&quot;, &quot;black&quot;, &quot;blue&quot;, &quot;gold3&quot;), lwd = c(NA, NA, 5, 5), pch = c(16, 16, NA, NA))</code></pre>
<p><img src="/post/2020-11-06-MSc5_Selektionseffekte_files/figure-html/unnamed-chunk-20-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="AppendixB" class="section level3">
<h3>Appendix B: Heckman Modell simulieren</h3>
<p>Wenn Sie die folgende Funktion von <code>simulate_heckman &lt;- function(</code> bis <code>}</code> markieren und ausführen, dann sollte die Funktion oben rechts in Ihrem <code>R</code>-Studiofenser unter der Rubrik <strong>Functions</strong> zu finden sein. Wenn Sie die Funktion anschließend mit folgenden Argumenten ausführen <code>n = 10^5</code>,<code>beta0 = -2</code>, <code>beta1 = 0.5</code>, <code>b0 = 0.5</code>, <code>b1 = 1.2</code>, <code>r = 2</code> und <code>sigma = 2</code> wählen, das entstehende Objekt als <code>data_heckman</code> abspeichern und die <code>head</code> Funktion darauf anwenden, so können sie 100000 Replikationen unter dem Heckman Modell mit den zuvor gewählten Parametern simulieren und sich davon die ersten 6 Zeile (Werte) ausgeben lassen. Anschließen schätzen Sie das Modell wie folgt:</p>
<pre class="r"><code>simulate_heckman &lt;- function(n, 
                             beta0, beta1,
                             b0, b1, r,
                             sigma)
{
     X &lt;- rnorm(n = n, mean = 2, sd = 2)
     Z &lt;- rnorm(n = n, mean = 0, sd = 1)
     
     # Populationsregression
     eps &lt;- rnorm(n = n, mean = 0, sd = sigma)
     Y &lt;- b0 + b1*X + r*Z + eps # Populationsregression
     
     
     # Selektion
     s &lt;- beta0 + beta1*X + Z &gt; 0   # Selektionsmechanismus
     
     Y_obs &lt;-rep(NA, length(Y))
     Y_obs[s == 1] &lt;- Y[s == 1]     # beobachtbares Y
     
     df &lt;- data.frame(&quot;X&quot; = X, &quot;s&quot; = s, &quot;Y_obs&quot; = Y_obs)
     return(df)
}

set.seed(404) # Vergleichbarkeit
# Daten simulieren
data_heckman &lt;- simulate_heckman(n = 10^5, 
                                 beta0 = -2, beta1 = 0.5, 
                                 b0 = 0.5, b1 = 1.2, r = 2, sigma = 2)
head(data_heckman) # Daten ansehen</code></pre>
<pre><code>##            X     s    Y_obs
## 1  3.5472786  TRUE  8.01523
## 2  2.1193767 FALSE       NA
## 3  1.3658543 FALSE       NA
## 4  0.9815069 FALSE       NA
## 5 -0.3060732 FALSE       NA
## 6  4.1044826  TRUE 11.09013</code></pre>
<pre class="r"><code># Heckman Modell schätzen für große n
heckman_model  &lt;- heckit(selection = s ~ 1 + X, outcome = Y_obs ~ 1 + X, data = data_heckman)
summary(heckman_model)</code></pre>
<pre><code>##  --------------------------------------------
##  Tobit 2 model (sample selection model)
##  2-step Heckman / heckit estimation
##  100000 observations (76025 censored and 23975 observed)
##  7 free parameters (df = 99994)
##  Probit selection equation:
##               Estimate Std. Error t value Pr(&gt;|t|)    
##  (Intercept) -2.004099   0.011093  -180.7   &lt;2e-16 ***
##  X            0.502194   0.003425   146.6   &lt;2e-16 ***
##  Outcome equation:
##              Estimate Std. Error t value Pr(&gt;|t|)    
##  (Intercept)  0.41902    0.37339   1.122    0.262    
##  X            1.21045    0.05605  21.595   &lt;2e-16 ***
##  Multiple R-Squared:0.1329,   Adjusted R-Squared:0.1328
##     Error terms:
##                Estimate Std. Error t value Pr(&gt;|t|)    
##  invMillsRatio   2.0698     0.1747   11.85   &lt;2e-16 ***
##  sigma           2.8613         NA      NA       NA    
##  rho             0.7234         NA      NA       NA    
##  ---
##  Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
##  --------------------------------------------</code></pre>
</div>
</div>
<div id="literatur" class="section level2">
<h2>Literatur</h2>
<p><a href="https://hds.hebis.de/ubffm/EBSCO/Record?id=ED478203%7Ceric">Briggs, D. C. (2004).</a> Causal inference and the Heckman model. <em>Journal of Educational and Behavioral Statistics, 29</em>(4), 397–420.</p>
<p><a href="https://hds.hebis.de/ubffm/EBSCO/Record?id=edsjsr.10.2307.1912352%7Cedsjsr">Heckman, J. (1979).</a> Sample selection bias as a specification error.<em>Econometrica</em>, 47, 153–161.</p>
<p><a href="https://hds.hebis.de/ubffm/EBSCO/Record?id=edsbas.C30BA3A7%7Cedsbas">Toomet, O., &amp; Henningsen, A. (2008).</a> Sample selection models in R: Package sampleSelection.<em>Journal of Statistical Software, 27</em>(7), 1-23.</p>
<ul>
<li><small> <em>Blau hinterlegte Autorenangaben führen Sie direkt zur universitätsinternen Ressource.</em></li>
</ul>
</div>
