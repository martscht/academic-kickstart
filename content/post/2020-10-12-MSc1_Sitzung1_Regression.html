---
title: Regression und Ausreißerdiagnostik
date: '2020-10-06'
slug: regression-und-ausreisserdiagnostik
categories:
  - MSc1
tags:
  - Regression
  - Diagnostik
  - Voraussetzungen
  - Ausreißer
  - ggplot
  - linear
  - nichtlinear
subtitle: ''
summary: ''
authors: [irmer, hartig]
lastmod: '2020-10-14T16:40:21+02:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---



<div id="einleitung" class="section level2">
<h2>Einleitung</h2>
<p>In der <a href="/post/einleitung-und-wiederholung">Einführungssitzung</a> hatten wir etwas über das Einlesen von Datensätzen, einfache Deskriptivstatistiken und den <span class="math inline">\(t\)</span>-Test gelernt und in diesem Rahmen einige Grundlagen der Statistik wiederholt. Nun wollen wir mit etwas komplexeren, aber bereits bekannten, Methoden weitermachen und eine multiple Regression in <code>R</code> durchführen. Hierbei werden wir auch die zu diesem Verfahren notwendigen Voraussetzungen prüfen sowie das Vorliegen von Ausreißern untersuchen.</p>
<p>Bevor wir dazu die Daten einlesen, sollten wir als erstes die nötigen <code>R</code>-Pakete laden. Die <code>R</code>-Pakete, die wir im Folgenden brauchen, sind: das <code>car</code>-Paket, das <code>MASS</code>-Paket sowie das Paket mit dem Namen <code>lm.beta</code>. Diese Pakete müssen zunächst installiert werden. Dies können Sie via <code>install.packages</code> machen:</p>
<pre class="r"><code>install.packages(&quot;car&quot;)            # Die Installation ist nur einmalig von Nöten!
install.packages(&quot;lm.beta&quot;)        # Sie müssen nur zu Update-Zwecken erneut installiert werden.
install.packages(&quot;MASS&quot;)</code></pre>
<p>Anschließend werden Pakete mit der <code>library</code>-Funktion geladen:</p>
<pre class="r"><code>library(lm.beta)  # Standardisierte beta-Koeffizienten für die Regression
library(car)      # Zusätzliche Funktion für Diagnostik von Datensätzen
library(MASS)     # Zusätzliche Funktion für Diagnostik von Datensätzen</code></pre>
<div id="daten-einladen" class="section level3">
<h3>Daten einladen</h3>
<p>Der Datensatz ist der selbe, wie in der <a href="/post/einleitung-und-wiederholung">Einführungssitzung</a>: Eine Stichprobe von 100 Schülerinnen und Schülern hat einen Lese- und einen Mathematiktest, sowie zusätzlich einen allgemeinen Intelligenztest, bearbeitet. Im Datensatz enthalten ist zudem das Geschlecht (<code>female</code>, 0=m, 1=w). Sie können den <a href="/post/Schulleistungen.rda"><svg style="height:0.8em;top:.04em;position:relative;" viewBox="0 0 512 512"><path d="M216 0h80c13.3 0 24 10.7 24 24v168h87.7c17.8 0 26.7 21.5 14.1 34.1L269.7 378.3c-7.5 7.5-19.8 7.5-27.3 0L90.1 226.1c-12.6-12.6-3.7-34.1 14.1-34.1H192V24c0-13.3 10.7-24 24-24zm296 376v112c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V376c0-13.3 10.7-24 24-24h146.7l49 49c20.1 20.1 52.5 20.1 72.6 0l49-49H488c13.3 0 24 10.7 24 24zm-124 88c0-11-9-20-20-20s-20 9-20 20 9 20 20 20 20-9 20-20zm64 0c0-11-9-20-20-20s-20 9-20 20 9 20 20 20 20-9 20-20z"/></svg> Datensatz “Schulleistungen.rda” hier herunterladen</a>.</p>
<p>Nun müssen wir mit <code>load</code> die Daten laden. Liegt der Datensatz bspw. auf dem Desktop, so müssen wir den Dateipfad dorthin legen und können dann den Datensatz laden (wir gehen hier davon aus, dass Ihr PC “Musterfrau” heißt) <em>Tipp: Verwenden Sie unbedingt die automatische Vervollständigung von <code>R</code>-Studio, wie in der letzten Sitzung beschrieben</em>.</p>
<pre class="r"><code>load(&quot;C:/Users/Musterfrau/Desktop/Schulleistungen.rda&quot;)</code></pre>
<p>Genauso sind Sie in der Lage, den Datensatz direkt aus dem Internet zu laden. Hierzu brauchen Sie nur die URL und müssen <code>R</code> sagen, dass es sich bei dieser um eine URL handelt, indem Sie die Funktion <code>url</code> auf den Link anwenden. Der funktioniernde Befehl sieht so aus (wobei die URL in Anführungszeichen geschrieben werden muss):</p>
<pre class="r"><code>load(url(&quot;https://pandar.netlify.app/post/Schulleistungen.rda&quot;))</code></pre>
<p>Nun sollte in <code>R</code>-Studio oben rechts in dem Fenster unter der Rubrik “Data” unser Datensatz mit dem Namen “<em>Schulleistungen</em>” erscheinen.</p>
</div>
<div id="ein-überblick-über-die-daten" class="section level3">
<h3>Ein Überblick über die Daten</h3>
<p>Wir können uns die ersten (6) Zeilen des Datensatzes mit der Funktion <code>head</code> ansehen. Dazu müssen wir diese Funktion auf den Datensatz (das Objekt) <code>Schulleistungen</code> anwenden:</p>
<pre class="r"><code>head(Schulleistungen)</code></pre>
<pre><code>##   female        IQ  reading     math
## 1      1  81.77950 449.5884 451.9832
## 2      1 106.75898 544.8495 589.6540
## 3      0  99.14033 331.3466 509.3267
## 4      1 111.91499 531.5384 560.4300
## 5      1 116.12682 604.3759 659.4524
## 6      0 106.14127 308.7457 602.8577</code></pre>
<p>Wir erkennen die 4 Spalten mit dem Geschlecht, dem IQ, der Lese- und der Mathematikleistung.</p>
<p>Das Folgende fundiert zum Teil auf Sitzungen zur Korrelation und Regression aus <a href="https://qis.server.uni-frankfurt.de/qisserver/rds?state=verpublish&amp;status=init&amp;vmfile=no&amp;publishid=291102&amp;moduleCall=webInfo&amp;publishConfFile=webInfo&amp;publishSubDir=veranstaltung">Veranstaltungen aus dem Bachelorstudium des Sommersemesters 2020</a> verfasst von Luisa Grützmacher, Johanna Schüller und <a href="/authors/irmer">Julien P. Irmer</a>. Sie können die Originalsitzungen in dessen <a href="https://github.com/martscht/PsyBSc7"><svg style="height:0.8em;top:.04em;position:relative;" viewBox="0 0 496 512"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> GitHub Repository “PsyBSc7”</a> einsehen. Die Daten und einige Analysen zu diesen Sitzungen stammen von <a href="/authors/hartig">Prof. Dr. Johannes Hartig</a> aus dem Sommersemester 2019.</p>
</div>
</div>
<div id="multiple--lineare-regression" class="section level2">
<h2>(Multiple-) Lineare Regression</h2>
<p>Eine Wiederholung der Regressionsanalyse (und der Korrelation) finden Sie bspw. in <a href="https://hds.hebis.de/ubffm/Record/HEB366849158">Eid, Gollwitzer und Schmitt (2017)</a> Kapitel 16 bis 19, <a href="https://hds.hebis.de/ubffm/Record/HEB369761391">Agresti und Finlay (2013)</a>, Kapitel 9 bis 11 und <a href="https://hds.hebis.de/ubffm/Record/HEB371183324">Pituch und Stevens (2016)</a> in Kapitel 3.</p>
<p>Das Ziel einer Regression besteht darin, die Variation einer Variable durch eine oder mehrere andere Variablen vorherzusagen (Prognose und Erklärung). Die vorhergesagte Variable wird als Kriterium, Regressand oder auch abhängige Variable (AV) bezeichnet und üblicherweise mit <span class="math inline">\(y\)</span> symbolisiert. Die Variablen zur Vorhersage der abhängigen Variablen werden als Prädiktoren, Regressoren oder unabhängige Variablen (UV) bezeichnet und üblicherweise mit <span class="math inline">\(x\)</span> symbolisiert.
Die häufigste Form der Regressionsanalyse ist die lineare Regression, bei der der Zusammenhang über eine Gerade bzw. eine (Hyper-)Ebene beschrieben wird. Demzufolge kann die lineare Beziehung zwischen den vorhergesagten Werten und den Werten der unabhängigen Variablen mathematisch folgendermaßen beschrieben werden:</p>
<p><span class="math display">\[y_i = b_0 +b_{1}x_{i1} + ... +b_{m}x_{im} + e_i\]</span></p>
<ul>
<li>Ordinatenabschnitt/ <span class="math inline">\(y\)</span>-Achsenabschnitt/ Konstante/ Interzept <span class="math inline">\(b_0\)</span>:
<ul>
<li>Schnittpunkt der Regressionsgeraden mit der <span class="math inline">\(y\)</span>-Achse</li>
<li>Erwartung von y, wenn alle UVs den Wert 0 annehmen</li>
</ul></li>
<li>Regressionsgewichte <span class="math inline">\(b_{1},\dots, b_m\)</span>:
<ul>
<li>beziffern die Steigung der Regressionsgeraden</li>
<li>Interpretation: die Steigung der Geraden lässt erkennen, um wie viele Einheiten <span class="math inline">\(y\)</span> zunimmt, wenn (das jeweilige) x um eine Einheit zunimmt (unter Kontrolle aller weiteren Variablen im Modell)</li>
</ul></li>
<li>Regressionsresiduum (kurz: Residuum), Residualwert oder Fehlerwert <span class="math inline">\(e_i\)</span>:
<ul>
<li>die Differenz zwischen einem beobachteten und einem vorhergesagten y-Wert (<span class="math inline">\(e_i=y_i-\hat{y}_i\)</span>)</li>
<li>je größer die Fehlerwerte (betraglich), umso größer ist die Abweichung (betraglich) eines beobachteten vom vorhergesagten Wert</li>
</ul></li>
</ul>
<p>In Matrixschreibweise sieht die Gleichung folgendermaßen aus
<span class="math display">\[Y = X\mathbf{b} + \mathbf{e},\]</span>
was wiederum eine Kurzschreibweise für
<span class="math display">\[\underbrace{\begin{pmatrix}y_1 \\ \vdots \\ y_n \end{pmatrix}}_Y = \underbrace{\begin{pmatrix}1 &amp; x_{i1} &amp; \dots &amp; x_{m1}\\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 1 &amp;  x_{n1} &amp; \dots &amp; x_{nm} \end{pmatrix}}_X\underbrace{\begin{pmatrix}b_1\\ \vdots\\ b_m\end{pmatrix}}_\mathbf{b} + \underbrace{\begin{pmatrix}e_1 \\ \vdots \\ e_n \end{pmatrix}}_\mathbf{e}\]</span>
ist. Wenn Sie also Ihre Daten in folgender Form vorliegen haben, so können Sie <code>R</code> nutzen und mit Hilfe von Matrixoperationen die Regressionskoeffizienten mit Hilfe des Kleinste-Quadrate-Schätzer berechnen:
<span class="math display">\[\hat{\mathbf{b}}=(X&#39;X)^{-1}X&#39;Y\]</span>
Wie dies genau gemacht wird und wie weitere Kennwerte in der Regression (bspw. <span class="math inline">\(R^2\)</span>) “zu Fuß” bestimmt werden, können Sie im <a href="#AppendixA">Appendix A</a> nachlesen - <em>hierbei ist zu beachten, dass Appendix A als “weiterführende Literatur” anzusehen und somit nicht verpflichtend ist</em>.</p>
<div id="unser-modell-und-das-lesen-von-r-outputs" class="section level3">
<h3>Unser Modell und das Lesen von <code>R</code>-Outputs</h3>
<p>Wir wollen mit Hilfe eines Regressionsmodells die Leseleistung durch das Geschlecht und durch die Intelligenz vorhersagen. Dies funktioniert in <code>R</code> ganz leicht mit der <code>lm</code> ("<strong>l</strong>inear <strong>m</strong>odeling) Funktion. Dieser müssen wir zwei Argumente übergeben: 1) unsere angenommene Beziehung zwischen den Variablen; 2) den Datensatz, in welchem die Variablen zu finden sind:</p>
<pre class="r"><code>lm(reading ~ 1 + female + IQ, data = Schulleistungen)</code></pre>
<pre><code>## 
## Call:
## lm(formula = reading ~ 1 + female + IQ, data = Schulleistungen)
## 
## Coefficients:
## (Intercept)       female           IQ  
##      88.209       38.470        3.944</code></pre>
<p>Hierbei zeigt die Tilde (<code>~</code>) auf, welche Variable die AV ist (sie steht links der Tilde), welche die UVs sind (sie stehen rechts der Tilde und werden durch <code>+</code> getrennt) und ob das Interzept mitgeschätzt werden soll (per Default wird dieses mitgeschätzt, was bedeutet, dass “<code>1 +</code>” redundant ist und daher hier weggelassen werden könnte - nicht mit einbezogen wird das Interzept via “<code>0 +</code>”). Diese Notation wird in sehr vielen Modell verwendet, in welchen es um die Beziehung zwischen unabhängigen und abhängigen Varibalen geht! Im <a href="#AppendixB">Appendix B</a> können Sie nachlesen, welche weiteren Befehle zum geleichen Ergebnis führen und wie bspw. explizit das Interzept in die Gleichung mit aufgenommen werden kann (oder darauf verzichtet wird!).</p>
<p>Im Output sehen wir die Parameterschätzungen unseres Regressionsmodells:
<span class="math display">\[\text{Reading}_i=b_0+b_1\text{Female}_i+b_2\text{IQ}_i+\varepsilon_i,\]</span>
für <span class="math inline">\(i=1,\dots,100=:n\)</span>. Wir wollen uns die Ergebnisse unserer Regressionsanalyse noch detaillierter anschauen. Dazu können wir wieder die <code>summary</code> Funktion anwenden. Um auch die standardisierten Ergebnisse zu erhalten, verwenden wir die Funktion <code>lm.beta</code> (<em>lm</em> steht hier für lineares Modell und <em>beta</em> für die standardisierten Koeffizienten. Achtung: Häufig werden allerdings auch unstandardisierten Regressionkoeffizienten als <span class="math inline">\(\beta\)</span>s bezeichnet, sodass darauf stets zu achten ist, bspw. in <a href="#AppendixD">Appendix D</a> dieser Sitzung).</p>
<p>Der <code>lm</code> Befehl erzeugt ein Objekt, welches wir weiterverwenden können, um darauf zum Beispiel die Funktion <code>summary</code> auszuführen. Wir speichern dieses Objekt ab, indem wir eine Zuordnung durchführen via <code>&lt;-</code> und einen Namen vergeben. Sind wir doch mal besonders phantasievoll und nennen unser Modell: “<em>our_model</em>”.</p>
<pre class="r"><code>our_model &lt;- lm(reading ~ 1 + female + IQ, data = Schulleistungen)
summary(our_model)</code></pre>
<pre><code>## 
## Call:
## lm(formula = reading ~ 1 + female + IQ, data = Schulleistungen)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -208.779  -64.215   -0.211   58.652  174.254 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  88.2093    56.5061   1.561   0.1218    
## female       38.4705    17.3863   2.213   0.0293 *  
## IQ            3.9444     0.5529   7.134 1.77e-10 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 86.34 on 97 degrees of freedom
## Multiple R-squared:  0.3555, Adjusted R-squared:  0.3422 
## F-statistic: 26.75 on 2 and 97 DF,  p-value: 5.594e-10</code></pre>
<p>Funktionen werden immer “von innen nach außen” angewandt, was ganz einfach bedeutet, dass die innerste Funktion zuerst angwandt wird und ein Objekt erzeugt, auf welches anschließend die nächst äußere Funktion angewandt werden kann. Bspw. können wir, bevor wir die Summary anfordern, <code>lm.beta</code> aus dem gleichnamigen Paket auf das Objekt anwenden, um zusätzlich standardisierte Koeffizienten zu erhalten:</p>
<pre class="r"><code>summary(lm.beta(our_model))</code></pre>
<pre><code>## 
## Call:
## lm(formula = reading ~ 1 + female + IQ, data = Schulleistungen)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -208.779  -64.215   -0.211   58.652  174.254 
## 
## Coefficients:
##             Estimate Standardized Std. Error t value Pr(&gt;|t|)    
## (Intercept)  88.2093       0.0000    56.5061   1.561   0.1218    
## female       38.4705       0.1810    17.3863   2.213   0.0293 *  
## IQ            3.9444       0.5836     0.5529   7.134 1.77e-10 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 86.34 on 97 degrees of freedom
## Multiple R-squared:  0.3555, Adjusted R-squared:  0.3422 
## F-statistic: 26.75 on 2 and 97 DF,  p-value: 5.594e-10</code></pre>
<p>Die Summay ist eine weitverbreitete Funktion, die Objekte zusammenfasst und interessante Informationen für uns auf einmal bereitstellt. <code>R</code> Outputs sehen fast immer so aus, weswegen es von unabdingbarer Wichtigkeit ist, dass wir uns mit diesem Output vertraut machen. Was können wir diesem nun Schritt für Schritt entnehmen?</p>
<pre><code>## 
##  Call:
##  lm(formula = reading ~ 1 + female + IQ, data = Schulleistungen)</code></pre>
<p>Fasst noch einmal zusammen, welches Objekt “zusammengefasst” wird. Hier steht sozusagen das zuvor untersuchte <code>lm</code>-Objekt (<code>our_model</code>, bzw. <code>lm.beta(our_model)</code>).</p>
<pre><code>## 
## Residuals:
##     Min       1Q   Median       3Q      Max 
## -208.779  -64.215   -0.211   58.652  174.254</code></pre>
<p>Diese Deskriptivstatistiken (gerundet auf 3 Nachkommastellen) geben uns ein Gefühl für die Datengrundlage: die Überschrift sagt uns, dass es hierbei um die Residuen im Regressionsmodell geht. <code>Min</code> steht für das Minimum (-208.779), <code>1Q</code> beschreibt das erste Quartil (-64.215); also den Prozentrang von 25% - es liegen 25% darunter und 75% der Werte darüber; <code>Median</code> beschreibt den 50. Prozentrang (-0.211), <code>3Q</code> beschreibt das 3. Quartil, also Prozentrang 75% (58.652) und <code>Max</code> ist der maximale Wert der Residuen (174.254). Der Mittelwert trägt hier keine Information, da die Residuen immer so bestimmt werden, dass sie im Mittel verschwinden, also ihr Mittelwert bei 0 liegt. Da der Median auch sehr nah an der 0 liegt, zeigt dies, dass die Resiuden wahrscheinlich recht symmetrisch verteilt sind. Auch das 1. und 3. Quartil verteilen sich ähnlich (also entgegengesetzte Vorzeichen aber betraglich ähnliche Werte), was ebenfalls für die Symmetrie spricht. Wir können die Residuen unserem <code>our_model</code> Objekt ganz leicht entlocken, indem wir den Befehl <code>resid</code> auf dieses Objekt anwenden oder <code>our_model$residuals</code> tippen. Bspw. ergibt sich dann das 1. Quartil oder der Mittelwert als:</p>
<pre class="r"><code>quantile(x = resid(our_model), probs = .25) # .25 = 25% = 25. PR mit der Funktion resid()</code></pre>
<pre><code>##       25% 
## -64.21544</code></pre>
<pre class="r"><code>mean(x = our_model$residuals) # Mittelwert mit Referenzierung aus dem lm Objekt &quot;our_model&quot;</code></pre>
<pre><code>## [1] 1.048103e-15</code></pre>
<p>Mit Hilfe der <code>quantile</code> Funktion können wir beliebige Prozentränge eines Vektors anfordern. Hierbei gibt <code>resid(our_model)</code> (genauso wie <code>our_model$residuals</code>) einen Vektor mit Residuen aus. Indem wir <code>probs</code> z.B. auf <code>.01</code> setzen, würden wir den 1. Prozentrang (1%) erhalten.</p>
<p>Die Zahl, die beim Mittelwert ausgegeben wird, ist folgendermaßen zu lesen: <code>e-15</code> steht für <span class="math inline">\(*10^{-15}=0.000000000000001\)</span> (die Dezimalstelle wird um 15 Stellen nach links verschoben), somit ist <code>1.048103e-15</code><span class="math inline">\(=1.048103*10^{-15}=0.000000000000001048103\approx 0\)</span>. Hier kommt in diesem Beispiel nicht exakt 0 heraus, da innerhalb der Berechnungen immer auf eine gewisse Genauigkeit gerundet wird.</p>
<p>Nun kommen wir zum eigentlich Spannenden, nämlich der Übersicht über die Parameterschätzung (<code>Coefficients</code>). Diese sieht in sehr vielen Analysen sehr ähnlich aus (z.B. logistische Regression oder Multi-Level-Analysen/hierarchische Regression aus diesem Kurs oder konfirmatorische Faktorenanalyse und Strukturgleichungsmodelle aus dem Folgekurs im Sommersemester).</p>
<pre><code>## 
## Coefficients:
##            Estimate Standardized Std. Error t value Pr(&gt;|t|)    
## (Intercept)  88.2093       0.0000    56.5061   1.561   0.1218    
## female       38.4705       0.1810    17.3863   2.213   0.0293 *  
## IQ            3.9444       0.5836     0.5529   7.134 1.77e-10 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Insgesamt gibt es 6 Spalten, wobei die <code>Standardized</code>-Spalte extra durch das Paket <code>lm.beta</code> angefordert wurde (sie ist also nicht immer in der Summary enthalten). In der ersten Spalte stehen die Variablennamen, die selbsterklärend sein sollten. Die Spalte <code>Estimate</code> zeigt den unstandardisierten Parameter (hier Regressionsgewicht). Z.B. liegt das Interzept <span class="math inline">\(b_0\)</span> bei 88.2093. Das Partialregressionsgewicht vom Geschlecht <span class="math inline">\(b_\text{female}\)</span> liegt bei 38.4705. Da Mädchen mit <code>1</code> kodiert sind, bedeutet dies: Wenn Mädchen im Vergleich zu Jungen betrachtet werden, so steigt die Leseleistung durchschnittlich um 38.4705 Punkte (“durchschnittlich” in der Interpretation ist enorm wichtig, da es ja noch den Vorhersagefehler gibt). Folglich können wir das Interzept ebenfalls interpretieren: Ein Junge mit einem Intelligenzquotienten von 0 (dieser Wert ist leider unrealistisch, da der IQ hier nicht zentriert wurde - die Effekte von Zentrierung schauen wir uns in der nächsten Sitzung zu Multi-Level-Modeling genauer an!) hat eine durchschnittliche Leseleistung von 88.2093.
In der Spalte <code>Standardized</code> stehen die standardisierten Regressionsgewichte. Hier werden die Daten so transformiert, dass sowohl die AV als auch die UVs jeweils Mittelwerte von 0 und Varianzen (bzw. Standardabweichungen) von 1 aufweisen. Das Interzept ist nun nicht länger interessant, da, wenn <span class="math inline">\(y\)</span> einen Mittelwert von 0 hat und auch die unabhängigen Variablen zentriert sind (also Mittelwerte von 0 haben), dann ist das Interzept gerade jener vorhergesagte Wert für <span class="math inline">\(y\)</span>, der anfällt, wenn alle Prädiktoren den Wert 0 -also ihren Mittelwert - annehmen. Bei der Regression ist es aber so, dass an der Stelle, an der die Prädiktoren ihren Mittelwert annehmen, auch der Mittelwert von <span class="math inline">\(y\)</span> liegt; hier also der Wert 0. Folglich ist das Interzept im standardisierten Fall <strong><em>immer</em></strong> 0.
Das standardisierte Regressionsgewicht der Intelligenz <span class="math inline">\(\beta_\text{IQ}\)</span> liegt bei 0.5836, was bedeutet, dass bei einer Erhöhung der Intelligenz um eine Standardabweichung auch die Leseleistung im Mittel (im Durchschnitt) um 0.5836 Standardabweichungen ansteigt. Da das Geschlecht <em>hier</em> nur zwei Ausprägungen hat, lässt sich der standardisierte Koeffizient <span class="math inline">\(\beta_\text{female}\)</span> von 0.181 wie folgt interpretieren: Steigt die Variable Geschlecht um eine Standardabweichung - werden also Mädchen im Vergleich zu Jungen betrachtet - so steigt die Leseleistung um durchschnittlich 0.181 Standardabweichungen.
Die Spalte <code>Std.Error</code> enthält die Standardfehler. Diese werden in <span class="math inline">\(t\)</span>-Werte umgerechnet via <span class="math inline">\(\frac{Est}{SE}\)</span>: es wird also die Parameterschätzung durch seinen Standardfehler geteilt und in der nächsten Spalte <code>t value</code> dargestellt. In einigen Summaries wird auch anstelle des <span class="math inline">\(t\)</span>-Werts der <span class="math inline">\(z\)</span>-Wert verwendet. Hierbei ändert sich nichts, nur wird zur Herleitung der <span class="math inline">\(p\)</span>-Werte eben die <span class="math inline">\(z\)</span>- anstatt der <span class="math inline">\(t\)</span>-Verteilung verwendet. Wenn wir uns allerdings an Statistik aus dem Bachelor erinnern, so bemerken wir, dass für große Stichproben die <span class="math inline">\(t\)</span> und die <span class="math inline">\(z\)</span>-Verteilung identisch (bzw. sehr nahe beieinander liegend) sind. Als groß gilt hierbei bereits eine Stichprobengröße von 50! In der Spalte <code>Pr(&gt;|t|)</code> stehen die zugehörigen <span class="math inline">\(p\)</span>-Werte. “Pr” steht für die Wahrscheinlichkeit (<strong>Pr</strong>obability), dass die Teststatistik (hier der <span class="math inline">\(t\)</span>-Wert) im Betrag ein extremeres Ergebnis aufzeigt, als das Bebachtete. Außerdem bekommen wir noch mit Sternchen angezeigt, auf welchem Signifikanzniveau die einzelnen Parameter statistisch bedeutsam sind. Dem Output entnehmen wir, dass das Interzept nicht bedeutsam ist - die Effekte der Prädiktoren sind allerdings auf dem 5%-Niveau statistisch signifikant. Dies bedeutet bspw. für die Intelligenz, dass <em>mit einer Irrtumswahrscheinlichkeit von 5% der Regressionsparameter der Intelligenz in der Population nicht 0 ist und somit auch in der Population mit dieser Irrtumswahrscheinlichkeit von einem Effekt zu sprechen ist</em>. Hierbei ist es essentiell, dass sich die statistiche Interpretation immer auf die Population bezieht. Dass ein Koeffizient nicht 0 ist (in der Stichprobe), erkennen wir einfach daran, dass er von 0 abweicht, allerdings kann dieses Ergebnis eben durch Zufall aufgetreten sein - so wie es der Fall beim Interzept ist. Dieses weicht offensichtlich in unserer Stichprobe von 0 ab, allerdings nicht stark genug, als dass man dies auch für die Population schlussfolgern würde (mit einer Irrtumswahrscheinlichkeit von 5%).</p>
<p>Regressionskoeffizienten können einzeln signifikant sein, ohne, dass signifikante Anteile der Variation der abhängigen Variable erklärt werden.</p>
<pre><code>## 
## Residual standard error: 86.34 on 97 degrees of freedom
## Multiple R-squared:  0.3555, Adjusted R-squared:  0.3422 
## F-statistic: 26.75 on 2 and 97 DF,  p-value: 5.594e-10</code></pre>
<p>Dazu entnehmen wir dem letzten Block den Standardfehler der Residuen (<code>Residual standard error</code>), der im Grunde die Fehlervariation von <span class="math inline">\(y\)</span> beschreibt, sowie das multiple <span class="math inline">\(R^2\)</span> (<code>Multiple R-squared</code>), welches anzeigt, dass ca. 35.5% der Variation der Leseleistung auf die Prädiktoren Geschlecht und Intelligenz zurückgeführt werden kann. Dieses Varianzinkrement ist statistisch signifikant, was wir am <span class="math inline">\(F\)</span>-Test in der letzten Zeile ablesen können. Der <span class="math inline">\(F\)</span>-Wert (<code>F-statistic</code>) liegt bei 26.75, wobei die Hypothesenfreiheitsgrade hier gerade 2 sind (<span class="math inline">\(df_h\)</span>) und die Residualfreiheitsgrade bei 97 (<span class="math inline">\(df_e\)</span>) liegen. Der zugehörige <span class="math inline">\(p\)</span>-Wert ist deutlich kleiner als 5% und liegt bei <span class="math inline">\(5.594*10^{-10}\)</span>. Dies bedeutet, dass mit einer Irrtumswahrscheinlichkeit von 5% auch in der Population eine Vorhersage der Leseleistung durch Geschlecht und Intelligenz gemeinsam angenommen werden kann (<span class="math inline">\(R^2\neq0\)</span>). In einem Artikel (oder einer Abschlussarbeit) würden wir zur Untermauerung <em>F</em>(2,97)=26.75, p&lt;.001 in den Fließtext schreiben.</p>
<p>Wenn wir diese Siginfikanzentscheidungen nutzen wollen, um die Effekte in der Population auf diese Weise zu interpretieren, so müssen einige Voraussetzungen erfüllt sein, die zunächst noch geprüft werden müssten. Bspw. nehmen wir für ein Regressionsmodell an, dass die Regressoren lineare Beziehungen mit dem Kriterium aufweisen. Die Personen/Erhebungen sollten unabhängig und identisch verteilt sein (sie sollten aus einer i.i.d. Population stammen, also keinerlei Beziehung untereinander aufweisen und dem gleichen Populationsmodell folgen). Die Residuen innerhalb der Regression werden als normalverteilt und homoskedastisch (also mit gleicher Varianz über alle Ausprägungen der Prädiktoren) angenommen. Nur unter diesen Voraussetzungen lassen sich die Signifikanzentscheidungen überhaupt interpretieren. Außerdem beeinflussen Ausreißer die Schätzungen der Regressionskoeffizienten drastisch.</p>
</div>
</div>
<div id="prüfen-der-voraussetzungen" class="section level2">
<h2>Prüfen der Voraussetzungen</h2>
<p><em>Dieser Abschnitt fundiert auf der Sitzung “Regression III” von <a href="/authors/irmer">Julien P. Irmer</a> aus dem Bachelorstudium Psychologie.</em> Die Voraussetzung der Unabhängigkeit und der Gleichverteiltheit ist und bleibt eine Annahme, die wir nicht prüfen können. Wir können jedoch durch das Studiendesign (Randomisierung, Repräsentativität) diese Annahme plausibilisieren. Wir schauen uns als nächstes die Linearitätsannahme an und machen danach mit der Verteilung der Residuen weiter. Zum Schluss schauen wir uns neben der Multikollinearität noch mögliche Ausreißer an:</p>
<ul>
<li>Linearität</li>
<li>Homoskedastizität</li>
<li>Normalverteilung der Residuen</li>
<li>Multikollinearität</li>
<li>Identifikation von Ausreißern und einflussreichen Datenpunkten</li>
</ul>
<p>Im Folgenden werden wir mit dem unstandardisierten Modell weiterarbeiten, welches wir im Objekt <code>our_model</code> gespeichert hatten.</p>
<p>Vertiefende Literatur zum folgenden Stoff finden wir bspw. in <a href="(https://hds.hebis.de/ubffm/Record/HEB366849158)">Eid, et al. (2017)</a> in Kapitel 19.13 und <a href="https://hds.hebis.de/ubffm/Record/HEB371183324">Pituch und Stevens (2016)</a> in Kapitel 3.10 - 3.14.</p>
<div id="linearität" class="section level3">
<h3>Linearität</h3>
<p>Eine grafische Prüfung der partiellen Linearität zwischen den einzelnen Prädiktorvariablen und dem Kriterium kann durch <em>partielle Regressionsplots</em> (engl. <em>Partialplots</em>) erfolgen. Dafür sagen wir in einem Zwischenschritt einen einzelnen Prädiktor durch alle übrigen Prädiktoren im Modell vorher und speichern die Residuen, die sich aus dieser Regression ergeben. Diese kennzeichnen den eigenständigen Anteil, den ein Prädiktor nicht mit den übrigen Prädiktoren gemein hat (er wird also von den übrigen Prädiktoren bereinigt). Dann werden die Residuen aus dieser Vorhersage gegen die Residuen des Kriteriums bei Vorhersage durch den jeweiligen Prädiktor dargestellt. Diese Grafiken können Hinweise auf systematische nicht-lineare Zusammenhänge geben, die in der Modellspezifikation nicht berücksichtigt wurden. Die zugehörige <code>R</code>-Funktion des <code>car</code> Pakets heißt <code>avPlots</code> und braucht als Argument lediglich das spezifizierte Regressionsmodell <code>our_model</code>.</p>
<pre class="r"><code>avPlots(model = our_model, pch = 16, lwd = 4)</code></pre>
<p><img src="/post/2020-10-12-MSc1_Sitzung1_Regression_files/figure-html/unnamed-chunk-14-1.png" width="960" style="display: block; margin: auto;" /></p>
<p>Mit Hilfe der Argumente <code>pch=16</code> und <code>lwd=4</code> werden die Darstellung der Punkte (ausgefüllt anstatt leer) sowie die Dicke der Linie (vierfache Dicke) manipuliert (<em>für mehr zu Grafikparametern in <code>R</code> siehe <a href="https://www.statmethods.net/advgraphs/parameters.html"><svg style="height:0.8em;top:.04em;position:relative;" viewBox="0 0 640 512"><path d="M622.34 153.2L343.4 67.5c-15.2-4.67-31.6-4.67-46.79 0L17.66 153.2c-23.54 7.23-23.54 38.36 0 45.59l48.63 14.94c-10.67 13.19-17.23 29.28-17.88 46.9C38.78 266.15 32 276.11 32 288c0 10.78 5.68 19.85 13.86 25.65L20.33 428.53C18.11 438.52 25.71 448 35.94 448h56.11c10.24 0 17.84-9.48 15.62-19.47L82.14 313.65C90.32 307.85 96 298.78 96 288c0-11.57-6.47-21.25-15.66-26.87.76-15.02 8.44-28.3 20.69-36.72L296.6 284.5c9.06 2.78 26.44 6.25 46.79 0l278.95-85.7c23.55-7.24 23.55-38.36 0-45.6zM352.79 315.09c-28.53 8.76-52.84 3.92-65.59 0l-145.02-44.55L128 384c0 35.35 85.96 64 192 64s192-28.65 192-64l-14.18-113.47-145.03 44.56z"/></svg> hier</a></em>). Den Achsenbeschriftungen ist zu entnehmen, dass auf der Y-Achse jeweils <em>reading | others</em> dargestellt ist. Die vertikale Linie <em>|</em> steht hierbei für den mathematischen Ausdruck <em>gegeben</em>. <em>Others</em> steht hierbei für alle weiteren (anderen) Prädiktoren im Modell. Dies bedeutet, dass es sich hierbei um die Residuen aus der Regression von <em>reading</em> auf alle anderen Prädiktoren handelt. Bei den unabhängigen Variablen (UV, <em>female</em>, <em>IQ</em>) steht <em>UV | others</em> also jeweils für die jeweilige UV gegeben der anderen UVs im Modell. Somit beschreiben die beiden Plots jeweils die Beziehungen, die die UVs über die anderen UVs im Modell hinaus mit dem Kriterium (AV, abhängige Variable) haben. Es ist zu beachten, dass die Variable Geschlecht hier nur zwei Ausprägungen hat.</p>
<p>Die hier dargestellten Plots sehen sehr linear aus, weswegen wir nicht an der Liniearitätsannahme zweifeln.</p>
</div>
</div>
<div id="verteilung-der-residuen" class="section level2">
<h2>Verteilung der Residuen</h2>
<p>Regressionsresiduen sollten homoskedastisch und normalverteilt sein.</p>
<div id="homoskedastizität" class="section level3">
<h3>Homoskedastizität</h3>
<p>Die Varianz der Residuen sollte unabhängig von den Ausprägungen der Prädiktoren sein. Dies wird i.d.R. grafisch geprüft, indem die Residuen <span class="math inline">\(e_i\)</span> gegen die vorhergesagten Werte <span class="math inline">\(\hat{y}_i\)</span> geplottet werden: der sogenannte <em>Residuenplot</em> (engl.: <em>residual plot</em>). In diesem Streudiagramm sollten die Residuen gleichmäßig über <span class="math inline">\(\hat{y}_i\)</span> streuen und keine systematischen Trends (linear, quadratisch, auffächernd, o.ä.) erkennbar sein. <strong>Prüfung nicht-linearer Zusammenhänge:</strong> Die Funktion <code>residualPlots</code> des Pakets <code>car</code> erzeugt separate Streudiagramme für die Residuen in Abhängigkeit von jedem einzelnen Prädiktor <span class="math inline">\(x_j\)</span> und von den vorhergesagten Werten <span class="math inline">\(\hat{y}_i\)</span> (<em>“Fitted Values”</em>); als Input braucht die Funktion das Modell <code>our_model</code>. Zusätzlich wird für jeden Plot ein quadratischer Trend eingezeichnet und auf Signifikanz getestet, wodurch eine zusätzliche Prüfung auf nicht-lineare Effekte erfolgt. Sind diese Tests nicht signifikant, ist davon auszugehen, dass diese Effekte nicht vorliegen und die Voraussetzung der Homoskedastizität damit nicht verletzt ist.</p>
<pre class="r"><code>residualPlots(model = our_model, pch = 16)</code></pre>
<p><img src="/post/2020-10-12-MSc1_Sitzung1_Regression_files/figure-html/unnamed-chunk-15-1.png" width="960" style="display: block; margin: auto;" /></p>
<pre><code>##            Test stat Pr(&gt;|Test stat|)
## female        0.0207           0.9835
## IQ            1.4015           0.1643
## Tukey test    0.5234           0.6007</code></pre>
<p>Da die Plots alle recht unsystematisch aussehen und die nichtlinearen Effekte nicht signifikant sind, spricht all dies für die Homoskedastizitätsannahme.</p>
</div>
<div id="normalverteilung" class="section level3">
<h3>Normalverteilung</h3>
<p>Voraussetzung für die Signifikanztests im Kontext der linearen Regression ist die Normalverteilung der Residuen. Auch diese Annahme wird i.d.R. grafisch geprüft. Hierfür bietet sich zum einen ein Histogramm der Residuen an, zum anderen ein <em>Q-Q-Diagramm</em> (oder “Quantile-Quantile-Plot”). Zusätzlich zur grafischen Darstellung erlaubt z.B. der Kolmogorov-Smirnov (<code>ks.test</code>) Test eine inferenzstatistische Prüfung, welcher eine deskriptive Verteilung mit einer theoretischen Verteilung vergleicht.</p>
<p>Wir beginnen mit der Vorbereitung der Daten. Wir wollen die studentisierten Residuen grafisch darstellen. Der Befehl <code>studres</code> aus dem <code>MASS</code> Paket speichert die Residuen aus einem <code>lm</code> Objekt, also aus dem definierten Modell <code>our_model</code>, und studentisiert diese. Das Studentisieren der Residuen bezeichnet eine Art der Standardisierung, sodass anschließend der Mittelwert <em>0</em> und die Varianz <em>1</em> ist (somit lassen sich solche Plots immer gleich interpretieren und besser vergleichen). Mit der Funktion <code>hist</code> erstellen wir ein Histogramm (Zusatzeinstellung <code>freq = F</code> bewirkt, dass wir relative anstatt absoluter Häufigkeiten betrachten). Anschließend müssen wir die Normalverteilung einzeichnen. Hierzu erzeugen wir zunächst eine Sequenz von Zahlen (mit <code>seq</code>), die vom Minimum von <code>res</code> (<code>from = min(res)</code>) bis zum Maximum von <code>res</code> (<code>to = max(res)</code>) in 0.01er-Schritten (<code>by = 0.01</code>) laufen soll. <code>dnorm</code> berechnet die Dichte einer Normalverteilung zu gegebenem <code>x</code> und vorgegebenem Mittelwert und Standardabweichung. Wir wollen die empirsche Varianz und den empirischen Mittelwert verwenden und wählen daher <code>dnorm(x = x, mean = mean(res), sd = sd(res))</code>, um die Dichte der Normalverteilung für alle Punkte in x (also vom Minimum bis zum Maximum der studentisierten Residuen <code>res</code>) zu bestimmen (für weitere Informationen siehe bspw. <a href="https://en.wikibooks.org/wiki/R_Programming/Probability_Distributions"><svg style="height:0.8em;top:.04em;position:relative;" viewBox="0 0 640 512"><path d="M622.34 153.2L343.4 67.5c-15.2-4.67-31.6-4.67-46.79 0L17.66 153.2c-23.54 7.23-23.54 38.36 0 45.59l48.63 14.94c-10.67 13.19-17.23 29.28-17.88 46.9C38.78 266.15 32 276.11 32 288c0 10.78 5.68 19.85 13.86 25.65L20.33 428.53C18.11 438.52 25.71 448 35.94 448h56.11c10.24 0 17.84-9.48 15.62-19.47L82.14 313.65C90.32 307.85 96 298.78 96 288c0-11.57-6.47-21.25-15.66-26.87.76-15.02 8.44-28.3 20.69-36.72L296.6 284.5c9.06 2.78 26.44 6.25 46.79 0l278.95-85.7c23.55-7.24 23.55-38.36 0-45.6zM352.79 315.09c-28.53 8.76-52.84 3.92-65.59 0l-145.02-44.55L128 384c0 35.35 85.96 64 192 64s192-28.65 192-64l-14.18-113.47-145.03 44.56z"/></svg> <code>R</code>-Verteilungen auf Wiki</a>
oder
<a href="https://www.statmethods.net/advgraphs/probability.html"><svg style="height:0.8em;top:.04em;position:relative;" viewBox="0 0 640 512"><path d="M622.34 153.2L343.4 67.5c-15.2-4.67-31.6-4.67-46.79 0L17.66 153.2c-23.54 7.23-23.54 38.36 0 45.59l48.63 14.94c-10.67 13.19-17.23 29.28-17.88 46.9C38.78 266.15 32 276.11 32 288c0 10.78 5.68 19.85 13.86 25.65L20.33 428.53C18.11 438.52 25.71 448 35.94 448h56.11c10.24 0 17.84-9.48 15.62-19.47L82.14 313.65C90.32 307.85 96 298.78 96 288c0-11.57-6.47-21.25-15.66-26.87.76-15.02 8.44-28.3 20.69-36.72L296.6 284.5c9.06 2.78 26.44 6.25 46.79 0l278.95-85.7c23.55-7.24 23.55-38.36 0-45.6zM352.79 315.09c-28.53 8.76-52.84 3.92-65.59 0l-145.02-44.55L128 384c0 35.35 85.96 64 192 64s192-28.65 192-64l-14.18-113.47-145.03 44.56z"/></svg> <code>R</code>: Verteilungen auf Data Camp</a>). Mit dem <code>lines</code> Befehl plotten wir anschließend diese Dichte als y-Werte gegen unsere vorgegebenen x-Werte. <code>lwd = 3</code> bewirkt erneut, dass diese Linie 3 mal so dick wie der Default ausfällt. Außerdem gibt es noch weitere Grafikparameter, wie etwa <code>main</code>, mit dem sich die Grafiküberschrift ändern lässt (probieren Sie es doch selbst aus!). Wer sich mit <code>R</code> schon länger auskennt, und mit dem Paket <code>ggplot2</code> vertraut ist, kann im <a href="#AppendixC">Appendix C</a> den Code für die <code>ggplot</code> Funktion dort nachlesen.</p>
<pre class="r"><code>res &lt;- studres(our_model) # Studentisierte Residuen als Objekt speichern
hist(res, freq = F)
xWerte &lt;- seq(from = min(res), to = max(res), by = 0.01)
lines(x = xWerte, y = dnorm(x = xWerte, mean = mean(res), sd = sd(res)), lwd = 3)</code></pre>
<p><img src="/post/2020-10-12-MSc1_Sitzung1_Regression_files/figure-html/unnamed-chunk-16-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Das Histogramm zeigt keine großen Verstöße gegen die Normalverteilungsannahme.</p>
<p>Ähnliche Informationen sind aus dem <em>Q-Q-Plot</em> zu entnehmen. Dafür kann der Befehl <code>qqPlot</code> aus dem <code>car</code> Paket genutzt werden. Hier kann die Verteilung, gegen die geprüft werden soll, direkt mit <code>distribution</code> festgelegt werden.</p>
<pre class="r"><code>qqPlot(our_model, pch = 16, distribution = &quot;norm&quot;)</code></pre>
<p><img src="/post/2020-10-12-MSc1_Sitzung1_Regression_files/figure-html/unnamed-chunk-17-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre><code>## [1]  6 33</code></pre>
<p>Durch die Angabe von <code>distribution = "norm"</code> als Argument für die Funktion <code>qqPlot</code> des <code>car</code>-Pakets wird ein Vergleich der vorliegenden Variable gegen eine Normalverteilung erzeugt (auch der Vergleich zu anderen Verteilungen wäre möglich - die Anführungszeichen sind dabei essentiell. Welche weiteren Verteilungen es gibt, kann bspw. in der oben angegebenen Quelle gefunden werden). Die gestrichelten Linien zeigen ein “gebootstraptes” (vgl. <a href="https://hds.hebis.de/ubffm/Record/HEB366849158">Eid, et al., 2017, S. 283</a>) 95% Konfidenzintervall an. In diesem Plot sind studentisierte Residuen dargestellt, welche gegen die zugehörigen Qunatile der angenommenen Verteilung abgetragen werden (vgl. <a href="https://hds.hebis.de/ubffm/Record/HEB366849158">Eid, 2017, S. 717 und folgend</a>). Die Punkte sollten möglichst nah an der durchgezogenen Linie (Winkelhalbierende oder Gerade “<span class="math inline">\(y=x\)</span>”) liegen. Zahlen im Plot zeigen extreme Werte auf (zur Erinnerung, bei einer Standardnormalverteilung mit Mittelwert 0 und Varianz 1 sind Werte ab 1.96 auf dem 5% Niveau extrem - deshalb werden hier die Residuen der ProbandInnen 6 und 33 als extrem dargestellt). Insgesamt scheint auch hier kein Grund zum Zweifeln an der Normalverteilungsannahme gegeben.</p>
<p>Nun wollen wir die Voraussetzung zusätzlich mittels des Kolmogorov Smirnov Tests überprüfen (siehe bspw. <a href="https://hds.hebis.de/ubffm/Record/HEB366849158">Eid, et al., 2017, Kapitel 10.16.1</a>). Dieser testet, wenn wir gegen die Normalverteilung prüfen, im Grunde genommen die Null-Hypothese: <span class="math inline">\(H_0:\)</span> <em>“Normalverteilung liegt vor”</em>. (Wir wissen allerdings, dass wir Hypothesen nur verwerfen können und nicht wirklich annehmen, weshalb diese Tests immer nur unter Vorbehalt interpretiert werden sollten - Aussagen wie etwa: <em>“<span class="math inline">\(H_0\)</span> nicht verworfen; daraus folgt, dass in der Population Normalverteilung herrscht”</em>, sind nicht zulässig. Wird die <span class="math inline">\(H_0\)</span> nicht verworfen, nehmen wir nur weiterhin an, dass eine Normalverteilung vorliegt, ein Beweis ist dies jedoch nicht). Die Funktion <code>ks.test</code> benötigt als Argument die Variable, die auf Normalverteilung geprüft werden soll (hier also <code>x = res</code>); und als weiteres Argument die Vergleichsverteilung - entweder ebenfalls als Vektor (Vergleich von empirischen Verteilungen) oder die theoretisch angenommene Verteilungsfunktion in Anführungszeichen (es sind alle in <code>R</code> implementierten Verteilungen möglich); hier <code>y = "pnorm"</code>. In diesem spezifischen Beispiel brauchen wir nun keine weiteren Einstellungen, da die Residuen studentisiert wurden. Normalerweise müssten wir noch die verteilungsspezifischen Parameter vorgeben: hier der Mittelwert und die Standardabweichung. Der vollständige Befehl für alle Normalverteilungen mit dem beobachten Mittelwert und beobachteter Standardbweichung erhalten wir so: <code>ks.test(x = res, y = "pnorm", mean(res), sd(res))</code>, wir könnten dort auch Zahlen vorgeben. Hier geht es aber noch einfacher:</p>
<pre class="r"><code>ks.test(x = res, y = &quot;pnorm&quot;)</code></pre>
<pre><code>## 
##  One-sample Kolmogorov-Smirnov test
## 
## data:  res
## D = 0.032672, p-value = 0.9999
## alternative hypothesis: two-sided</code></pre>
<p>Der <span class="math inline">\(p\)</span>-Wert von 0.9999 zeigt an, dass keine signifikante Diskrepanz zwischen der Normalverteilung und unseren Residuen vorliegt. Für unser Modell zeigen somit alle Ergebnisse übereinstimmend, dass die Residuen normalverteilt sind (bzw. lehnen diese Hypothese nicht ab: <em>Kolmogorov Smirnov Test</em>). Auch die Mahalanobisdistanz kann verwendet werden, um Daten auf (multivariate) Normalverteilung zu prüfen. Dafür muss die Mahalanobisdistanz <span class="math inline">\(\chi^2\)</span>-verteilt sein, mit der Anzahl an Variablen als <span class="math inline">\(df\)</span>. Dies könnte dann wieder via <em>Q-Q-Plot</em> oder <em>Histogramm</em> erfolgen, bzw. via dem Kolmogorov-Smirnov-Test gegen die <span class="math inline">\(\chi^2(df)\)</span>-Verteilung. Die Mahalanobisdistanz wird später noch im Zusammenhang mit Ausreißern vorgestellt (dort wird dann auch nochmals die multivariate Normalverteilung aufgegriffen).</p>
<p>Wäre der <em>Kolmogorov Smirnov Test gegen Normalverteilung</em> auf dem 5%-Niveau signifikant, würde dies dafür sprechen, dass “mit einer Irrtumswahrscheinlichkeit von 5% die Null-Hypothese auf Gleichheit der Normalverteilung mit unserer empirischen Verteilung verworfen werden muss, da in der Population die Abweichung der empirischen zur theoretischen Normalverteilung mit der Irrtumswahrscheinlichkeit von 5% ungleich 0 ist”.</p>
<p>Schiefe Verteilungen, welche sich im Histogramm oder im Q-Q-Plot widerspiegeln, sowie zu einem signifikanten <em>Kolmogorov Smirnov Test gegen Normalverteilung</em> führen können, könnten Indizien für nichtlineare (z.B. quadratische) Beziehungen im Datensatz sein. Weitere Analysen wären hierfür nötig.</p>
</div>
</div>
<div id="multikollinearität" class="section level2">
<h2>Multikollinearität</h2>
<p>Multikollinearität ist ein potenzielles Problem der multiplen Regressionsanalyse und liegt vor, wenn zwei oder mehrere Prädiktoren hoch miteinander korrelieren. Hohe Multikollinearität</p>
<ul>
<li>schränkt die mögliche multiple Korrelation ein, da die Prädiktoren redundant sind und überlappende Varianzanteile in <span class="math inline">\(y\)</span> erklären.</li>
<li>erschwert die Identifikation von bedeutsamen Prädiktoren, da deren Effekte untereinander konfundiert sind (die Effekte können schwer voneinander getrennt werden).</li>
<li>bewirkt eine Erhöung der Standardfehler der Regressionkoeffizienten <em>(der Standardfehler ist die Standardabweichung zu der Varianz der Regressionskoeffizienten bei wiederholter Stichprobenziehung und Schätzung)</em>. Dies bedeutet, dass die Schätzungen der Regressionsparameter instabil, und damit weniger verlässlich, werden.
Weitere Informationen zur Instabilität und zu Standardfehlern kann der/die interessierte Leser*in in <a href="#AppendixD">Appendix D</a> nachlesen.</li>
</ul>
<p>Multikollinearität kann durch Inspektion der <em>bivariaten Zusammenhänge</em> (Korrelationsmatrix) der Prädiktoren <span class="math inline">\(x_j\)</span> untersucht werden. Leider kann dies aber nicht alle Formen von Multikollinearität aufdecken. Darüber hinaus ist die Berechung der sogennanten <em>Toleranz</em> (T) und des <em>Varianzinflationsfaktors</em> (VIF) für jeden Prädiktor möglich. Hierfür wird nacheinander für jeden Prädiktor <span class="math inline">\(x_j\)</span> der Varianzanteil <span class="math inline">\(R_j^2\)</span> berechnet, der durch Vorhersage von <span class="math inline">\(x_j\)</span> durch <em>alle anderen Prädiktoren</em> in der Regression erklärt wird. Toleranz und VIF sind wie folgt definiert:</p>
<ul>
<li><span class="math inline">\(T_j = 1-R^2_j = \frac{1}{VIF_j}\)</span></li>
<li><span class="math inline">\(VIF = \frac{1}{1-R^2_j} = \frac{1}{T_j}\)</span></li>
</ul>
<p>Offensichtlich genügt eine der beiden Statistiken, da sie vollständig ineinander überführbar und damit redundant sind. Empfehlungen als Grenzwert für Kollinearitätsprobleme sind z. B. <span class="math inline">\(VIF_j&gt;10\)</span> (<span class="math inline">\(T_j&lt;0.1\)</span>; siehe <a href="https://hds.hebis.de/ubffm/Record/HEB366849158">Eid, et al., 2017, S. 712 und folgend</a>). Die Varianzinflationsfaktoren der Prädiktoren im Modell können mit der Funktion <code>vif</code> des <code>car</code>-Paktes bestimmt werden, der Toleranzwert als Kehrwert des VIFs.</p>
<pre class="r"><code># Korrelation der Prädiktoren
cor(Schulleistungen$female, Schulleistungen$IQ)</code></pre>
<pre><code>## [1] -0.08467395</code></pre>
<p>Die Prädiktoren sind nur schwach negativ korreliert. Wir schauen uns trotzdem den VIF und die Toleranz an. Dazu übergeben wir wieder das definierte Regressionsmodell an <code>vif</code>.</p>
<pre class="r"><code>vif(our_model)        # VIF</code></pre>
<pre><code>##   female       IQ 
## 1.007221 1.007221</code></pre>
<pre class="r"><code>1/vif(our_model)      # Toleranz</code></pre>
<pre><code>##    female        IQ 
## 0.9928303 0.9928303</code></pre>
<p>In diesem Beispiel mit nur 2 Prädiktoren ist <span class="math inline">\(R_j^2=cor(\text{female},\text{IQ})^2\)</span> und die Formeln sind daher sehr einfach auch mit Hand zu bestimmen:</p>
<pre class="r"><code>1/(1-cor(Schulleistungen$female, Schulleistungen$IQ)^2) # 1/(1-R^2) = VIF</code></pre>
<pre><code>## [1] 1.007221</code></pre>
<pre class="r"><code>1-cor(Schulleistungen$female, Schulleistungen$IQ)^2 # 1-R^2 = Toleranz</code></pre>
<pre><code>## [1] 0.9928303</code></pre>
<p>Für unser Modell wird ersichtlich, dass die Prädiktoren praktisch unkorreliert sind und dementsprechend kein Multikollinearitätsproblem vorliegt. Unabhängigkeit folgt hieraus allerdings nicht, da nicht-lineare Beziehungen zwischen den Variablen bestehen könnten, die durch diese Indizes nicht abgebildet werden.</p>
</div>
<div id="identifikation-von-ausreißern-und-einflussreichen-datenpunkten" class="section level2">
<h2>Identifikation von Ausreißern und einflussreichen Datenpunkten</h2>
<p>Die Plausibilität unserer Daten ist enorm wichtig. Aus diesem Grund sollten Ausreißer oder einflussreiche Datenpunkte analysiert werden. Diese können bspw. durch Eingabefehler entstehen (Alter von 211 Jahren anstatt 21) oder es sind seltene Fälle (hochintelligentes Kind in einer Normstichprobe), welche so in natürlicher Weise (aber mit sehr geringer Häufigkeit) auftreten können. Es muss dann entschieden werden, ob Ausreißer die Repräsentativität der Stichprobe gefährden und ob diese daher besser ausgeschlossen werden sollten.</p>
<div id="hebelwerte" class="section level3">
<h3>Hebelwerte</h3>
<p><em>Hebelwerte</em> <span class="math inline">\(h_j\)</span> (engl.: leverage values) erlauben die Identifikation von Ausreißern aus der gemeinsamen Verteilung der unabhängigen Variablen, d.h. sie geben an, wie weit ein Wert vom Mittelwert einer Prädiktorvariable entfernt ist. Je höher der Hebelwert, desto weiter liegt der einzelne Fall vom Mittelwert der gemeinsamen Verteilung der unabhängigen Variablen entfernt und desto stärker kann somit der Einfluss auf die Regressionsgewichte sein. Diese werden mit der Funktion <code>hatvalues</code> ermittelt. Kriterien zur Beurteilung der Hebelwerte variieren, so werden von <a href="https://hds.hebis.de/ubffm/Record/HEB366849158">Eid et al. (2017, S. 707 und folgend)</a> Grenzen von <span class="math inline">\(2\cdot k / n\)</span> für große und <span class="math inline">\(3\cdot k / n\)</span> für kleine Stichproben vorgeschlagen, in den Vorlesungsfolien von Prof. Dr. Klein aus dem Bachelor werden Werte von <span class="math inline">\(4/n\)</span> als auffällig eingestuft (hierbei ist <span class="math inline">\(k\)</span> die Anzahl an Prädiktoren und <span class="math inline">\(n\)</span> die Anzahl der Beobachtungen). Alternativ zu einem festen Cut-Off-Kriterium kann die Verteilung der Hebelwerte inspiziert werden, wobei diejenigen Werte als kritisch bewertet werden, die aus der Verteilung ausreißen. Die Funktion <code>hatvalues</code> erzeugt die Hebelwerte aus einem Regression-Objekt. Wir wollen diese als Histogramm darstellen.</p>
<pre class="r"><code>n &lt;- length(residuals(our_model))   # Anzahl an Personen bestimmen
h &lt;- hatvalues(our_model)           # Hebelwerte
hist(h, breaks  = 20)               
abline(v = 4/n, col = &quot;red&quot;)  # Cut-off bei 4/n</code></pre>
<p><img src="/post/2020-10-12-MSc1_Sitzung1_Regression_files/figure-html/unnamed-chunk-22-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Das Zusatzargument <code>breaks = 20</code> in <code>hist</code> gibt an, dass 20 Balken gezeichnet werden sollen. <code>abline</code> ist eine Funktion, die eine Gerade einem Plot hinzufügt. Dem Argument <code>v</code> wird hierbei der Punkt übergeben, an welchem eine <strong>v</strong>ertikale Linie eingezeichnet werden soll. <code>col = "red"</code> gibt an, dass diese Linie rot sein soll.</p>
</div>
<div id="cooks-distanz" class="section level3">
<h3>Cook’s Distanz</h3>
<p><em>Cook’s Distanz</em> <span class="math inline">\(CD_i\)</span> bezieht sich auf Ausreißer auf der abhängigen Variablen und gibt eine Schätzung an, wie stark sich die Regressionsgewichte verändern, wenn eine Person <span class="math inline">\(i\)</span> aus dem Datensatz entfernt wird. Fälle, deren Elimination zu einer deutlichen Veränderung der Ergebnisse führen würden, sollten kritisch geprüft werden. Als einfache Daumenregel gilt, dass <span class="math inline">\(CD_i&gt;1\)</span> auf einen einflussreichen Datenpunkt hinweist. Cook’s Distanz kann mit der Funktion <code>cooks.distance</code> ermittelt werden.</p>
<pre class="r"><code># Cooks Distanz
CD &lt;- cooks.distance(our_model) # Cooks Distanz
hist(CD, breaks  = 20)
abline(v = 1, col = &quot;red&quot;)  # Cut-off bei 1</code></pre>
<p><img src="/post/2020-10-12-MSc1_Sitzung1_Regression_files/figure-html/unnamed-chunk-23-1.png" width="672" style="display: block; margin: auto;" />
In diesem Plot ist die vertikale Linie nicht enthalten, da der Plot schon zu früh entlang der x-Achse aufhört. Wir können die Grenzen mit <code>xlim = c(0,1)</code> explizit von 0 bis 1 vorgeben:</p>
<pre class="r"><code># Cooks Distanz nochmal
hist(CD, breaks  = 20, xlim = c(0, 1))
abline(v = 1, col = &quot;red&quot;)  # Cut-off bei 1</code></pre>
<p><img src="/post/2020-10-12-MSc1_Sitzung1_Regression_files/figure-html/unnamed-chunk-24-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="blasendiagramm" class="section level3">
<h3>Blasendiagramm</h3>
<p>Die Funktion <code>influencePlot</code> des <code>car</code>-Paketes erzeugt ein “Blasendiagramm” zur simultanen grafischen Darstellung von Hebelwerten (auf der x-Achse), studentisierten Residuen (auf der y-Achse) und Cooks Distanz (als Größe der Blasen). Vertikale Bezugslinien markieren das Doppelte und Dreifache des durchschnittlichen Hebelwertes, horizontale Bezugslinien die Werte -2, 0 und 2 auf der Skala der studentisierten Residuen. Fälle, die nach einem der drei Kriterien als Ausreißer identifiziert werden, werden im Streudiagramm durch ihre Zeilennummer gekennzeichnet. Diese Zeilennummern können verwendet werden, um sich die Daten der auffälligen Fälle anzeigen zu lassen. Sie werden durch <code>InfPlot</code> ausgegeben. Auf diese kann durch <code>as.numeric(row.names(InfPlot))</code> zugegriffen werden.</p>
<pre class="r"><code># Blasendiagramm mit Hebelwerten, studentisierten Residuen und Cooks Distanz
# In &quot;IDs&quot; werden die Zeilennummern der auffälligen Fälle gespeichert,
# welche gleichzeitig als Zahlen im Blasendiagramm ausgegeben werden
InfPlot &lt;- influencePlot(our_model)</code></pre>
<p><img src="/post/2020-10-12-MSc1_Sitzung1_Regression_files/figure-html/unnamed-chunk-25-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>IDs &lt;- as.numeric(row.names(InfPlot))
# Werte der identifizierten Fälle
InfPlot</code></pre>
<pre><code>##      StudRes        Hat      CookD
## 6  -2.377185 0.02350872 0.04327384
## 9   1.984692 0.07863226 0.10876003
## 33 -2.511223 0.02211252 0.04506800
## 80  2.133104 0.07208157 0.11365968
## 99 -1.270057 0.10669659 0.06381762</code></pre>
<p>Schauen wir uns die möglichen Ausreißer an und standardisieren die Ergebnisse für eine bessere Interpretierbarkeit.</p>
<pre class="r"><code># Rohdaten der auffälligen Fälle (gerundet für bessere Übersichtlichkeit)
round(Schulleistungen[IDs,],2)</code></pre>
<pre><code>##    female     IQ reading   math
## 6       0 106.14  308.75 602.86
## 9       1 135.20  822.01 749.56
## 33      1  87.55  263.23 494.10
## 80      1  60.77  540.63 366.73
## 99      0  54.05  198.11 367.98</code></pre>
<pre class="r"><code># z-standardisierte Werte der auffälligen Fälle
round(scale(Schulleistungen)[IDs,],2) </code></pre>
<pre><code>##      female    IQ reading  math
## [1,]  -1.08  0.51   -1.76  0.35
## [2,]   0.92  2.35    3.06  1.61
## [3,]   0.92 -0.67   -2.19 -0.58
## [4,]   0.92 -2.37    0.42 -1.67
## [5,]  -1.08 -2.80   -2.80 -1.66</code></pre>
<p>Die Funktion <code>scale</code> z-standardisiert den Datensatz, mit Hilfe von <code>[IDs,]</code>, werden die entsprechenden Zeilen der Ausreißer aus dem Datensatz ausgegeben und anschließend auf 2 Nachkommastellen gerundet. Hierbei ist es extrem wichtig, dass wir <code>scale(Schulleistungen)[IDs,]</code> und nicht <code>scale(Schulleistungen[IDs,])</code> schreiben (das ist mir nämlich schon mal passiert…<svg style="height:0.8em;top:.04em;position:relative;" viewBox="0 0 496 512"><path d="M248 8C111 8 0 119 0 256s111 248 248 248 248-111 248-248S385 8 248 8zm80 168c17.7 0 32 14.3 32 32s-14.3 32-32 32-32-14.3-32-32 14.3-32 32-32zm-160 0c17.7 0 32 14.3 32 32s-14.3 32-32 32-32-14.3-32-32 14.3-32 32-32zm170.2 218.2C315.8 367.4 282.9 352 248 352s-67.8 15.4-90.2 42.2c-13.5 16.3-38.1-4.2-24.6-20.5C161.7 339.6 203.6 320 248 320s86.3 19.6 114.7 53.8c13.6 16.2-11 36.7-24.5 20.4z"/></svg>), da bei der zweiten Schreibweise die Daten reskaliert (z-standardisiert) werden, allerdings auf Basis der ausgewählten Fälle (n=5) und nicht auf Basis der gesamten Stichprobe (n=100). Mit Hilfe der z-standardisierten Ergebnisse lassen sich Ausreißer hinsichtlich ihrer Ausprägungen einordnen:</p>
<div id="interpretation" class="section level4">
<h4>Interpretation</h4>
<p>Was ist an den fünf identifizierten Fällen konkret auffällig?</p>
<ul>
<li><em>Fall 6</em>: unterdurchschnittliche Lesekompetenz bei gleichzeitig durchschnittlichem IQ und durchschnittlicher Matheleistung</li>
<li><em>Fall 9</em>: Sehr hohe Werte in IQ, Lese- und Matheleistung</li>
<li><em>Fall 33</em>: Unterdurchschnittliche Leseleistung “trotz” eher durchschnittlicher Intelligenz</li>
<li><em>Fall 80</em>: Sehr niedriger IQ bei gleichzeitig durchschnittlicher Lesekompetenz</li>
<li><em>Fall 99</em>: Sehr niedrige Werte in IQ, Lesekompetenz und Mathematik</li>
</ul>
<p>Die Entscheidung, ob Ausreißer oder auffällige Datenpunkte aus Analysen ausgeschlossen werden, ist schwierig und kann nicht pauschal beantwortet werden. Im vorliegenden Fall wäre z.B. zu überlegen, ob die Intelligenztestwerte der Fälle 80 und 99, die im Bereich von Lernbehinderung oder sogar geistiger Behinderung liegen, in einer Stichprobe von Schülern/innen aus Regelschulen als glaubwürdige Messungen interpretiert werden können oder als Hinweise auf mangelndes Commitment bei der Beantwortung hinweisen.</p>
</div>
</div>
<div id="mahalanobisdistanz" class="section level3">
<h3>Mahalanobisdistanz</h3>
<p>Die Mahalanobisdistanz (siehe z.B. <a href="https://hds.hebis.de/ubffm/Record/HEB366849158">Eid et al., 2017,</a> ab Seite 707) ist ein Werkzeug, welches zur Identifikation von multidimensionalen Ausreißern verwendet werden kann. Mit Hilfe der Mahalanobisdistanz wird die Entfernung vom zentralen Zentroiden bestimmt und mit Hilfe der Kovarianzmatrix gewichtet. Im Grunde kann man sagen, dass die Entfernung vom gemeinsamen Mittelwert über alle Variablen an der Variation in den Daten relativiert wird. Im <strong>eindimensionalen Fall</strong> ist die Mahalanobisdistanz nichts anderes als der quadrierte <span class="math inline">\(z\)</span>-Wert, denn wir bestimmen dann die Mahalanobisdistanz einer Person <span class="math inline">\(i\)</span> via
<span class="math display">\[MD_i=\frac{(X_i-\bar{X})^2}{\sigma_X^2}=\left(\frac{X_i-\bar{X}}{\sigma_X}\right)^2=z^2.\]</span>
Wir erkennen, dass wir hier den Personenwert relativ zur Streuung in den Daten betrachten. Nutzen wir nun mehrere Variablen und wollen multivariate Ausreißer interpretieren, so ist die Mahalanobisdistanz folgendermaßen definiert:
<span class="math display">\[MD_i=(\mathbf{X}_i-\bar{\mathbf{X}})&#39;\Sigma^{-1}(\mathbf{X}_i-\bar{\mathbf{X}}).\]</span>
Der Vektor der Mittelwertsdifferenzen <span class="math inline">\(\mathbf{X}_i-\bar{\mathbf{X}}\)</span> wird durch die Kovarianzmatrix der Daten <span class="math inline">\(\Sigma\)</span> gewichtet. Sind zwei Variablen <span class="math inline">\(X_1\)</span> und <span class="math inline">\(X_2\)</span> positiv korreliert, so treten große Werte auf beiden Variablen gemeinsam häufig auf, allerdings sind große <span class="math inline">\(X_1\)</span> und kleine <span class="math inline">\(X_2\)</span>-Werte unwahrscheinlich. Dies lässt sich anhand der Mahalanobisdistanz untersuchen. Außerdem gilt, dass bei multivariater Normalverteilung der Daten die Mahalanobisdistanz <span class="math inline">\(\chi^2(df=p)\)</span> verteilt ist, wobei <span class="math inline">\(p\)</span> die Anzahl an Variablen ist. Der Vorteil hiervon ist, dass wir eine eindimensionale Verteilung untersuchen können, um ein Gefühl für multivariate Daten zu erhalten. Bspw. kann dann ein Histogramm oder ein Q-Q-Plot verwendet werden, um die Daten auf Normalverteilung zu untersuchen, bzw. es kann bspw. der Kolmogorov-Smirnov Test durchgeführt werden, um zu prüfen, ob die Mahalanobisdistanz <span class="math inline">\(\chi^2(df=p)\)</span> verteilt ist. Der Befehl in <code>R</code> für die Mahalanobisdistanz ist <code>mahalanobis</code>. Wir nehmen einfach die Leseleistung und die Mathematikleistung via <code>Schulleistungen$...</code> als unsere zwei Variablen auf und fassen diese zusammen zu einer Datenmatrix <code>X</code> mit <code>cbind</code> (column-bind), welches die übergebenen Variablen als Spaltenvektoren zusammenfasst (siehe dazu auch <a href="/post/einleitung-und-wiederholung/#AppendixB">Appendix B in der Einführungssitzung</a>). <code>mahalanobis</code> braucht 3 Argumente: <code>x = X</code> die Daten, den gemeinsamen Mittelwert der Daten, den wir hier mit <code>colMeans</code> bestimmen (es wird jeweils der Mittelwert für die Spalten gebildet) sowie die Kovarianzmatrix der Daten mit <code>cov(X)</code> (<code>cor</code> gibt die Korrelationsmatrix aus; hier wird allerdings die Kovarianzmatrix gebraucht- anhand der Korrelationsmatrix lässt sich jedoch die Beziehung der Variablen besser einordnen), an welcher die Struktur relativiert werden soll:</p>
<pre class="r"><code>X &lt;- cbind(Schulleistungen$reading, Schulleistungen$math) # Datenmatrix mit Leseleistung in Spalte 1 und Matheleistung in Spalte 2
colMeans(X)  # Spaltenmittelwerte (1. Zahl = Mittelwert der Leseleistung, 2. Zahl = Mittelwert der Matheleistung)</code></pre>
<pre><code>## [1] 496.0660 561.4645</code></pre>
<pre class="r"><code>cov(X) # Kovarianzmatrix von Leseleistung und Matheleistung</code></pre>
<pre><code>##           [,1]      [,2]
## [1,] 11333.071  4653.679
## [2,]  4653.679 13639.296</code></pre>
<pre class="r"><code>cor(X) # zum Vergleich: die Korrelationsmatrix (die Variablen scheinen mäßig zu korrelieren, was unbedingt in die Ausreißerdiagnostik involviert werden muss)</code></pre>
<pre><code>##           [,1]      [,2]
## [1,] 1.0000000 0.3743059
## [2,] 0.3743059 1.0000000</code></pre>
<pre class="r"><code>MD &lt;- mahalanobis(x = X, center = colMeans(X), cov = cov(X))</code></pre>
<p>Zum bestimmen der kritischen Distanz nehmen wir die <span class="math inline">\(\chi^2\)</span> Verteilung heran. Wir bestimmen mit <code>qchisq</code> den kritischen Wert, wobei als <span class="math inline">\(p\)</span>-Wert hier meist ein <span class="math inline">\(\alpha\)</span>-Niveau von .01 oder .001 herangezogen wird, damit wir nicht fälschlicherweise zu viele Werte aussortieren. Hierbei übergeben wir dem Argument <code>p</code> das <span class="math inline">\(\alpha\)</span>-Niveau, <code>lower.tail = F</code> besagt, dass wir damit die obere Grenze meinen (also mit <code>p</code> gerade die Wahrscheinlichkeit meinen, einen extremeren Wert zu finden), <code>df = 2</code> stellt die Freiheitsgrade ein (hier = 2, da 2 Variablen):</p>
<pre class="r"><code>qchisq(p = .01, lower.tail = F, df = 2)    # alpha = 1%</code></pre>
<pre><code>## [1] 9.21034</code></pre>
<pre class="r"><code>qchisq(p = .001, lower.tail = F, df = 2)   # alpha = 0.1%</code></pre>
<pre><code>## [1] 13.81551</code></pre>
<p>Nun können wir die Mahalanobisdistanz untersuchen:</p>
<pre class="r"><code>MD</code></pre>
<pre><code>##   [1]  0.88733518  0.21566377  2.41458442  0.13177919  1.27927630  4.28965128
##   [7]  0.03451167  0.33661264  9.62502968  1.22925996  3.44404519  1.06648706
##  [13]  1.47842392  1.83106572  4.29879647 22.89143112  0.21555167  0.49834275
##  [19]  2.65724843  1.81949191  2.12850866  1.31066833  0.14334729  0.15748576
##  [25]  1.71430321  1.74615142  1.00646134  0.17988251  5.04513115  7.14678462
##  [31]  0.16464887  7.01861572  4.85177478  1.19043337  2.72462931  4.34611563
##  [37]  0.52793945  0.80019408  0.74740206  0.57368396  0.28016792  6.27035221
##  [43]  4.07657057  0.04345642  0.81076314  2.97056512  2.10323417  1.08103246
##  [49]  0.60950451  2.90704907  0.10986424  6.09359260  0.94728160  2.35683503
##  [55]  0.03620534  0.40241153  1.62654030  1.72697393  3.75760606  1.46629223
##  [61]  0.71323544  0.41142182  0.39965918  0.61656862  0.37536805  0.22094404
##  [67]  3.04527077  0.27381745  0.79358572  3.86151524  3.19370574  0.19468438
##  [73]  0.29354422  2.35326071  0.95634005  0.07240648  1.92700957  1.44365936
##  [79]  0.12899197  4.04487099  0.42931176  0.05394028  4.83779128  0.15390664
##  [85]  2.20766936  1.27108100  0.37456784  0.20895062  1.47329914  1.26292357
##  [91]  1.25364137  0.67270315  0.85951141  4.25348827  0.28156897  0.18421086
##  [97]  0.47650958  0.13256756  8.26520343  0.18224578</code></pre>
<p>Hier alle Werte durchzugehen ist etwas lästig. Natürlich können wir den Vergleich mit den kritischen Werten auch automatisieren und z.B. uns nur diejenigen Mahalanobisdistanzwerte ansehen, die größer als der kritische Wert zum <span class="math inline">\(\alpha\)</span>-Niveau von 1% sind. Wenn wir den <code>which</code> Befehl nutzen, so erhalten wir auch noch die Probandennummer der möglichen Ausreißer.</p>
<pre class="r"><code>MD[MD &gt; qchisq(p = .01, lower.tail = F, df = 2)]      # Mahalanobiswerte &gt; krit. Wert</code></pre>
<pre><code>## [1]  9.62503 22.89143</code></pre>
<pre class="r"><code>which(MD &gt; qchisq(p = .01, lower.tail = F, df = 2))   # Pbn-Nr.</code></pre>
<pre><code>## [1]  9 16</code></pre>
<p>Auf dem <span class="math inline">\(\alpha\)</span>-Niveau von 1% gäbe es 2 Ausreißer (Pbn-Nr = 9, 16), auf dem von 0.1% nur noch einen (Pbn-Nr = 16). Außerdem widerspricht die Verteilung der Mahalanobisdistanz nicht (zu sehr) der Annahme auf multivariate Normalverteilung:</p>
<pre class="r"><code>hist(MD, freq = F, breaks = 15)
xWerte &lt;- seq(from = min(MD), to = max(MD), by = 0.01)
lines(x = xWerte, y = dchisq(x = xWerte, df = 2), lwd = 3)</code></pre>
<p><img src="/post/2020-10-12-MSc1_Sitzung1_Regression_files/figure-html/unnamed-chunk-31-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>qqPlot(x = MD,distribution =  &quot;chisq&quot;, df = 2, pch = 16)</code></pre>
<p><img src="/post/2020-10-12-MSc1_Sitzung1_Regression_files/figure-html/unnamed-chunk-31-2.png" width="672" style="display: block; margin: auto;" /></p>
<pre><code>## [1] 16  9</code></pre>
<p>Der Q-Q-Plot hat außerdem als Output die Fallnummer (Pbn-Nr) der extremeren Werte - hier ebenfalls 9 und 16. Die Mahalanobisdistanz widerspricht nicht der multivariaten Normalverteilungsannahme.</p>
<p>Den gesamten <code>R</code>-Code, der in dieser Sitzung genutzt wird, können Sie <a href="https://raw.githubusercontent.com/jpirmer/MSc1_FEI/master/R-Scripts/1_Regression_RCode.R"><svg style="height:0.8em;top:.04em;position:relative;" viewBox="0 0 512 512"><path d="M216 0h80c13.3 0 24 10.7 24 24v168h87.7c17.8 0 26.7 21.5 14.1 34.1L269.7 378.3c-7.5 7.5-19.8 7.5-27.3 0L90.1 226.1c-12.6-12.6-3.7-34.1 14.1-34.1H192V24c0-13.3 10.7-24 24-24zm296 376v112c0 13.3-10.7 24-24 24H24c-13.3 0-24-10.7-24-24V376c0-13.3 10.7-24 24-24h146.7l49 49c20.1 20.1 52.5 20.1 72.6 0l49-49H488c13.3 0 24 10.7 24 24zm-124 88c0-11-9-20-20-20s-20 9-20 20 9 20 20 20 20-9 20-20zm64 0c0-11-9-20-20-20s-20 9-20 20 9 20 20 20 20-9 20-20z"/></svg> hier herunterladen</a>.</p>
</div>
</div>
<div id="appendix" class="section level2">
<h2>Appendix</h2>
<div id="AppendixA" class="section level3">
<h3>Appendix A</h3>
<p>Wir schauen uns nun ein weiteres Beispiel an und berechnen alle Koeffizienten zunächst mit <code>lm</code> und anschließend “zu Fuß”. Wir wollen folgendes Modell schätzen</p>
<p><span class="math display">\[y_{i,math} = b_0 +b_{reading}x_{i,reading} + b_{IQ}x_{i,IQ} + e_i\]</span>
oder in Matrixform:</p>
<p><span class="math display">\[\begin{align}
\begin{pmatrix} y_1\\y_2\\y_3\\y_4\\...\\y_{100}\end{pmatrix} = b_{0} *
\begin {pmatrix}1\\1\\1\\1\\...\\1\end{pmatrix} + b_{reading} *
\begin {pmatrix}x_{reading1}\\x_{reading2}\\x_{reading3}\\x_{reading4}\\...\\y_{reading100}\end{pmatrix} + b_{IQ} *
\begin {pmatrix}x_{IQ1}\\x_{IQ2}\\x_{IQ3}\\x_{IQ4}\\...\\x_{IQ100}\end{pmatrix} +
\begin {pmatrix}e_1\\e_2\\e_3\\e_4\\...\\e_{100}\end{pmatrix}
\end{align}\]</span></p>
<p>Mit <code>lm</code> kommen wir zu folgendem Ergebnis; das Modell nennen wir phantasievoll <code>our_next_model</code>:</p>
<pre class="r"><code>our_next_model &lt;- lm(math ~ reading + IQ, data = Schulleistungen)
our_next_model</code></pre>
<pre><code>## 
## Call:
## lm(formula = math ~ reading + IQ, data = Schulleistungen)
## 
## Coefficients:
## (Intercept)      reading           IQ  
##    58.17167     -0.03585      5.30982</code></pre>
<p>Nun schätzen wir diese Parameter zu Fuß:</p>
<pre class="r"><code># Vektor Y
y &lt;- Schulleistungen$math
head(y)</code></pre>
<pre><code>## [1] 451.9832 589.6540 509.3267 560.4300 659.4524 602.8577</code></pre>
<pre class="r"><code># Matrix X vorbereiten (Spalten mit beiden Prädiktoren + Spalte mit Einsen anfügen)
X &lt;- as.matrix(Schulleistungen[,c(&quot;reading&quot;, &quot;IQ&quot;)])       
X &lt;- cbind(rep(1,nrow(X)), X)                         
head(X)</code></pre>
<pre><code>##         reading        IQ
## [1,] 1 449.5884  81.77950
## [2,] 1 544.8495 106.75898
## [3,] 1 331.3466  99.14033
## [4,] 1 531.5384 111.91499
## [5,] 1 604.3759 116.12682
## [6,] 1 308.7457 106.14127</code></pre>
<p><span class="math display">\[\begin{align}y = \begin{pmatrix}451,98\\589,65\\509,33\\560,43\\...\\603,18\end{pmatrix}\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}X = \begin{pmatrix}1 &amp; 449,59 &amp; 81,78\\1 &amp; 544,85 &amp; 106,76\\1 &amp; 331,35 &amp; 99,14\\1 &amp; 531,54 &amp; 111,91\\ ... &amp; ... &amp; ... \\1 &amp; 487,22 &amp; 106,13\end{pmatrix}\end{align}\]</span></p>
<p><strong>Vorgehen bei der Berechnung der Regressionsgewichte:</strong></p>
<ol style="list-style-type: decimal">
<li>Berechnung der Kreuzproduktsumme (X’X)</li>
<li>Berechnung der Inversen der Kreuzproduktsumme (<span class="math inline">\((X&#39;X)^{-1}\)</span>)</li>
<li>Berechnung des Kreuzproduksummenvektors (X’y)</li>
<li>Berechnung des Einflussgewichtsvektors</li>
</ol>
<div id="berechnung-der-kreuzproduktsumme-xx" class="section level4">
<h4>1. Berechnung der Kreuzproduktsumme (<span class="math inline">\(X’X\)</span>)</h4>
<p>Die Kreuzproduktsumme (X’X) wird berechnet, indem die transponierte Matrix X (X’) mit der Matrix X multipliziert wird. Die transponierte Matrix X’ erhalten Sie durch den Befehl t(X).</p>
<p><span class="math display">\[\begin{align}
\dfrac{}{X&#39;\begin{pmatrix} 1 &amp; 1 &amp; 1 &amp; 1 &amp; ... &amp; 1\\449,58 &amp; 544,85 &amp; 331,35 &amp; 531,54 &amp; ... &amp; 487,22\\81,78 &amp; 106,76 &amp; 99,14 &amp; 111,91 &amp; ... &amp; 106,13\end{pmatrix}}
\dfrac{\begin{pmatrix}1 &amp; 449,59 &amp; 81,78\\1 &amp; 544,85 &amp; 106,76\\1 &amp; 331,35 &amp; 99,14\\1 &amp; 531,54 &amp; 111,91\\... &amp; ... &amp; ... \\1 &amp; 487,22 &amp; 106,13\end{pmatrix}X}
{\begin{pmatrix}100,00 &amp; 49606,60 &amp; 9813,43\\49606,61 &amp; 25730126,10 &amp; 4962448,08\\9813,43 &amp; 4962448,10 &amp; 987595,82\end{pmatrix}X&#39;X}\end{align}\]</span></p>
<pre class="r"><code># Berechnung der Kreuzproduktsumme X’X in R
XX &lt;- t(X) %*% X        # X&#39; erhalten Sie durch t(X)
XX</code></pre>
<pre><code>##                      reading          IQ
##           100.000    49606.6    9813.425
## reading 49606.605 25730126.1 4962448.077
## IQ       9813.425  4962448.1  987595.824</code></pre>
</div>
<div id="berechnung-der-inversen-der-kreuzproduktsumme-xx-1" class="section level4">
<h4>2. Berechnung der Inversen der Kreuzproduktsumme <span class="math inline">\((X&#39;X)^{-1}\)</span></h4>
<p>Die Inverse der Kreuzproduktsumme kann in R durch den solve-Befehl berechnet werden.</p>
<pre class="r"><code># Berechnung der Inversen (mit Regel nach Sarrus) in R
solve(XX)</code></pre>
<pre><code>##                             reading            IQ
##          0.4207610612 -1.568521e-04 -3.392822e-03
## reading -0.0001568521  1.316437e-06 -5.056210e-06
## IQ      -0.0033928220 -5.056210e-06  6.013228e-05</code></pre>
<p><span class="math display">\[\begin{align}(X&#39;X)^{-1}= \begin{pmatrix}0,42 &amp; -1,56e^{-04} &amp; -3,39^{-03}\\-0,00 &amp; 1,32e^{-06} &amp; -5,06e^{-06}\\-0,00 &amp; -5,06e^{-06} &amp; 6,01e^{-05}\end{pmatrix}\end{align}\]</span></p>
</div>
<div id="berechnung-des-kreuzproduksummenvektors-xy" class="section level4">
<h4>3. Berechnung des Kreuzproduksummenvektors (X’y)</h4>
<p>Der Kreuzproduktsummenvektor (X’y) wird durch die Multiplikation der transponierten X Matrix (X’) und des Vektors y berechnet.</p>
<p><span class="math display">\[\begin{align}
\dfrac{}{X&#39;\begin{pmatrix}1 &amp; 1 &amp; 1 &amp; 1 &amp; ... &amp; 1\\449,58 &amp; 544,85 &amp; 331,35 &amp; 531,54 &amp; ... &amp; 487,22\\81,78 &amp; 106,76 &amp; 99,14 &amp; 111,91 &amp; ... &amp; 106,13\end{pmatrix}}
\dfrac{\begin{pmatrix}451,98\\589,65\\509,33\\560,43\\...\\603,18\end{pmatrix}y}
{\begin{pmatrix}56146,45\\28313059,77\\5636931,00\end{pmatrix}X&#39;y}
\end{align}\]</span></p>
<pre class="r"><code>#Berechnung des Kreuzproduksummenvektors X`y in R
Xy &lt;- t(X) %*% y        
head(Xy)</code></pre>
<pre><code>##                [,1]
##            56146.45
## reading 28313059.77
## IQ       5636931.00</code></pre>
</div>
<div id="berechnung-des-einflussgewichtsvektors" class="section level4">
<h4>4. Berechnung des Einflussgewichtsvektors</h4>
<p>Die geschätzten Regressionsgewichte nach dem Kriterium der kleinsten Quadrate werden berechnet, indem die Inverse der Kreuzproduktsumme <span class="math inline">\((X&#39;X)^{-1}\)</span> mit dem Kreuzproduksummenvektor (X’y) multipliziert wird. Den Vektor mit den vorhergesagten Werte von y (<span class="math inline">\(\hat{y}\)</span>) können Sie durch die Multiplikation der X-Matrix mit den Regressionsgewichten (<span class="math inline">\(\hat{b}\)</span>) berechnen.</p>
<p><span class="math display">\[\begin{align}
\dfrac{}{(X&#39;X)^{-1}\begin{pmatrix}0,42 &amp; -1,56e^{-04} &amp; -3,39^{-03}\\-0,00 &amp; 1,32e^{-06} &amp; -5,06e^{-06}\\-0,00 &amp; -5,06e^{-06} &amp; 6,01e^{-05}\end{pmatrix}}
\dfrac{\begin{pmatrix}56146,45\\28313059,77\\5636931,00\end{pmatrix}X&#39;y}
{\begin{pmatrix}58,17\\-0,04\\5,31\end{pmatrix}\hat{b}}
\end{align}\]</span></p>
<pre class="r"><code># Berechnung des Einflussgewichtsvektors in R
b_hat &lt;- solve(XX) %*% Xy     # Vektor der geschätzten Regressionsgewichte
b_hat</code></pre>
<pre><code>##                [,1]
##         58.17167003
## reading -0.03584686
## IQ       5.30981976</code></pre>
<pre class="r"><code>y_hat &lt;- as.vector(X %*% b_hat) # Vorhersagewerte
head(y_hat)</code></pre>
<pre><code>## [1] 476.2897 605.5115 572.7112 633.3661 653.1192 610.6951</code></pre>
<p><span class="math display">\[\begin{align}
\hat{y}_{math} = \begin{pmatrix}476,29\\605,51\\572,71\\633,37\\...\\604,22\end{pmatrix}
\end{align}\]</span></p>
<p>Tatsächlich kommen wir zum selben Ergebnis wie mit <code>lm</code>. Dies liegt daran, dass <code>lm</code> im Grunde diese Matrixoperationen für uns durchführt!</p>
</div>
<div id="berechnung-der-standardisierten-regressionsgewichte" class="section level4">
<h4>Berechnung der standardisierten Regressionsgewichte</h4>
<p>Bisher wurden aber nur die <em>unstandardisierten Regressionsgewichte</em> berechnet. Diese haben den Vorteil, leichter interpretierbar zu sein. So wird das unstandardisierte Regressionsgewicht folgendermaßen interpretiert: wenn sich die unabhängige Variable um eine Einheit verändert, verändert sich die abhängige Variable um den unstandardisierten Koeffizienten. Der Nachteil dieser unstandardisierten Regressionsgewichte ist jedoch, dass die Regressionsgewichte nicht vergleichbar sind. Demzufolge kann anhand der Größe der Regressionsgewichte nicht gesagt werden, welcher Regressionskoeffizient eine stärkere Erklärungskraft hat.
Daher werden die Regressionsgewichte häufig standardisiert. Durch die Standardisierung sind die Regressionsgewichte nicht mehr von der ursprünglichen Skala abhängig und haben daher den Vorteil, dass sie miteinander verglichen werden können. Allerdings sind die <em>standardisierten Regressionsgewichte</em> nicht mehr so leicht zu interpretieren. Die Interpretation der standardisierten Regressionsgewichte lautet: wenn sich die unabhängige Variable um eine Standardabweichung erhöht (und unter Kontrolle weiterer unabhängiger Variablen), so beträgt die erwartete Veränderung in der abhängigen Variable <span class="math inline">\(\beta\)</span> Standardabweichungen (das standardisierte Interzept ist <strong>Null</strong>).
Die standardisierten Regressionsgewichte können über den standardisierten y Vektor und die standardisierte Matrix (dazu <code>scale</code>-Befehl in <code>R</code>) ermittelt werden.</p>
<pre class="r"><code>#Berechnung der standardisierten Regressionsgewichte
y_s &lt;- scale(y) # Standardisierung y
X_s &lt;- scale(X) # Standardisierung X
X_s[,1] &lt;- 1    # Einsenvektor ist nach Standardisierung zunächst NaN, muss wieder gefüllt werden
b_hat_s &lt;- solve(t(X_s)%*% X_s) %*% t(X_s)%*%y_s #Regressionsgewichte aus den standardisierten Variablen
round(b_hat_s, 3)</code></pre>
<pre><code>##           [,1]
##          0.000
## reading -0.033
## IQ       0.716</code></pre>
<p>Der <code>lm</code> Befehl mit dem zusatz <code>lm.beta</code> aus dem gleichnamigen Paket führt zu:</p>
<pre class="r"><code>lm.beta(our_next_model)</code></pre>
<pre><code>## 
## Call:
## lm(formula = math ~ reading + IQ, data = Schulleistungen)
## 
## Standardized Coefficients::
## (Intercept)     reading          IQ 
##   0.0000000  -0.0326760   0.7161496</code></pre>
</div>
<div id="berechnung-des-globalen-signifikanztests" class="section level4">
<h4>Berechnung des globalen Signifikanztests</h4>
<p><strong>Determinationskoeffizient <span class="math inline">\(R^2\)</span></strong></p>
<p>Der Determinationskoeffizient <span class="math inline">\(R^2\)</span> gibt an, wie viel Varianz in der abhängigen Variable durch die unabhängigen Variablen erklärt werden kann:</p>
<p><span class="math inline">\(R^2= \dfrac{Q_d}{Q_d + Q_e}\)</span></p>
<pre class="r"><code># Determinationskoeffizient R2
Q_d &lt;- sum((y_hat - mean(y))^2)    # Regressionsquadratsumme
Q_e &lt;- sum((y - y_hat)^2)          # Fehlerquadratsumme
R2 &lt;- Q_d / (Q_d + Q_e)            # Determinationskoeffizient R2</code></pre>
<p><span class="math inline">\(R^2= \dfrac{Q_d}{Q_d + Q_e} = \dfrac{6.5805169\times 10^{5}}{6.5805169\times 10^{5} + 6.9223863\times 10^{5}} = 0.49\)</span></p>
<p><strong><span class="math inline">\(F\)</span>-Wert</strong></p>
<p>Der F-Wert dient zur Überprüfung der Gesamtsignifikanz des Modells.</p>
<pre class="r"><code># F-Wert
n &lt;- length(y)                     # Fallzahl (n=100)
m &lt;- ncol(X)-1                     # Zahl der Prädiktoren (m=2)
F_omn &lt;- (R2/m) / ((1-R2)/(n-m-1))   # F-Wert
F_krit &lt;- qf(.95, df1=m, df2=n-m-1)  # kritischer F-Wert (alpha=5%)
p &lt;- 1-pf(F_omn, m, n-m-1)           # p-Wert</code></pre>
<p><span class="math inline">\(F_{omn} = \dfrac{\dfrac{R^2}{m}}{\dfrac{1-R^2}{n-m-1}} = \dfrac{\dfrac{0.49}{2}}{\dfrac{1-0.49}{100-2-1}} = 46.1\)</span></p>
<p><span class="math inline">\(df_1 = 2, df_1 = n-m-1 = 100-2-1 =97\)</span></p>
<p><span class="math inline">\(F_{krit}(\alpha=.05, df_1=2, df_2= 97)= 3.09\)</span></p>
<p><span class="math inline">\(p=0.00000000000000844\)</span></p>
</div>
</div>
<div id="AppendixB" class="section level3">
<h3>Appendix B</h3>
<p>Folgende Befehle führen zum gleichen Ergebnis wie:</p>
<pre class="r"><code>lm(reading ~ 1 + female + IQ, data = Schulleistungen)</code></pre>
<pre><code>## 
## Call:
## lm(formula = reading ~ 1 + female + IQ, data = Schulleistungen)
## 
## Coefficients:
## (Intercept)       female           IQ  
##      88.209       38.470        3.944</code></pre>
<p>Das Interzept kann explizit mit angegeben werden (falls Sie <code>0 +</code> schreiben, setzen Sie das Interzept auf 0, was sich entsprechend auf die Parameterschätzungen auswirken wird, falls das Interzept von 0 verschieden ist!):</p>
<pre class="r"><code>lm(reading ~ 0 + female + IQ, data = Schulleistungen)</code></pre>
<pre><code>## 
## Call:
## lm(formula = reading ~ 0 + female + IQ, data = Schulleistungen)
## 
## Coefficients:
## female      IQ  
## 45.187   4.785</code></pre>
<p>Dem Output ist zu entnehmen, dass die Parameterschätzungen sich drastisch geändert haben!</p>
<p>Lassen wir das Interzept in der Schreibweise weg, so wird es per Default mitgeschätzt.</p>
<pre class="r"><code>lm(reading ~ female + IQ, data = Schulleistungen)</code></pre>
<pre><code>## 
## Call:
## lm(formula = reading ~ female + IQ, data = Schulleistungen)
## 
## Coefficients:
## (Intercept)       female           IQ  
##      88.209       38.470        3.944</code></pre>
<p>Mit <code>formula</code> benutzen wir nicht die Position in der Funktion, sondern das Argument für die Formel:</p>
<pre class="r"><code>lm(formula = 1 + reading ~ female + IQ, data = Schulleistungen) </code></pre>
<pre><code>## 
## Call:
## lm(formula = 1 + reading ~ female + IQ, data = Schulleistungen)
## 
## Coefficients:
## (Intercept)       female           IQ  
##      89.209       38.470        3.944</code></pre>
<p>Wir können also auch einfach die Reihenfolge umdrehen, solange wir Argumente benutzen:</p>
<pre class="r"><code>lm(data = Schulleistungen, formula = 1 + reading ~ female + IQ) </code></pre>
<pre><code>## 
## Call:
## lm(formula = 1 + reading ~ female + IQ, data = Schulleistungen)
## 
## Coefficients:
## (Intercept)       female           IQ  
##      89.209       38.470        3.944</code></pre>
<p>Die Formel kann auch in Anführungszeichen geschrieben werden:</p>
<pre class="r"><code>lm(&quot;reading ~ 1 + female + IQ&quot;, data = Schulleistungen) </code></pre>
<pre><code>## 
## Call:
## lm(formula = &quot;reading ~ 1 + female + IQ&quot;, data = Schulleistungen)
## 
## Coefficients:
## (Intercept)       female           IQ  
##      88.209       38.470        3.944</code></pre>
<p>Wir können auf die Datensatzspezifizierung verzichten, indem wir die Variablen direkt ansprechen (es ändern sich entsprechend die Namen der Koeffizienten im Output):</p>
<pre class="r"><code>lm(Schulleistungen$reading ~ 1 + Schulleistungen$female + Schulleistungen$IQ) </code></pre>
<pre><code>## 
## Call:
## lm(formula = Schulleistungen$reading ~ 1 + Schulleistungen$female + 
##     Schulleistungen$IQ)
## 
## Coefficients:
##            (Intercept)  Schulleistungen$female      Schulleistungen$IQ  
##                 88.209                  38.470                   3.944</code></pre>
<p>Wir können auch neue Variablen definieren, um diese dann direkt anzusprechen (es ändern sich entsprechend die Namen der Koeffizienten):</p>
<pre class="r"><code>AV &lt;- Schulleistungen$reading
UV1 &lt;- Schulleistungen$female
UV2 &lt;- Schulleistungen$IQ

lm(AV ~ 1 + UV1 + UV2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = AV ~ 1 + UV1 + UV2)
## 
## Coefficients:
## (Intercept)          UV1          UV2  
##      88.209       38.470        3.944</code></pre>
<p>Selbstverständlich gibt es auch noch weitere Befehle, die zum selben Ergebnis kommen. Sie sehen, dass Sie in <code>R</code> in vielen Bereichen mit leicht unterschiedlichem Code zum selben Ergebnis gelangen!</p>
</div>
<div id="AppendixC" class="section level3">
<h3>Appendix C</h3>
<p>Im folgenden Block sehen wir den Code für ein Histogramm in <code>ggplot2</code>-Notation (das Paket muss natürlich installiert sein: <code>install.packages(ggplot2)</code>). Hier sind einige Zusatzeinstellungen gewählt, die das Histogramm optisch aufbereiten.</p>
<pre class="r"><code>library(ggplot2)
df_res &lt;- data.frame(res) # als Data.Frame für ggplot
ggplot(data = df_res, aes(x = res)) + 
     geom_histogram(aes(y =..density..),
                    bins = 15,                    # Wie viele Balken sollen gezeichnet werden?
                    colour = &quot;blue&quot;,              # Welche Farbe sollen die Linien der Balken haben?
                    fill = &quot;skyblue&quot;) +           # Wie sollen die Balken gefüllt sein?
     stat_function(fun = dnorm, args = list(mean = mean(res), sd = sd(res)), col = &quot;darkblue&quot;) + # Füge die Normalverteilungsdiche &quot;dnorm&quot; hinzu und nutze den empirischen Mittelwert und die empirische Standardabweichung &quot;args = list(mean = mean(res), sd = sd(res))&quot;, wähle dunkelblau als Linienfarbe
     labs(title = &quot;Histogramm der Residuen mit Normalverteilungsdichte&quot;, x = &quot;Residuen&quot;) # Füge eigenen Titel und Achsenbeschriftung hinzu</code></pre>
<p><img src="/post/2020-10-12-MSc1_Sitzung1_Regression_files/figure-html/unnamed-chunk-48-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Nutzen wir nur die Defaulteinstellung des Histogramms (bis auf <code>bins = 15</code> <em>- für die Vergleichbarkeit der beiden Grafiken</em>), sieht es so aus:</p>
<pre class="r"><code># hier nochmal nur das Nötigste:
ggplot(data = df_res, aes(x = res)) + 
     geom_histogram(aes(y =..density..),  bins = 15) +           
     stat_function(fun = dnorm, args = list(mean = mean(res), sd = sd(res)))</code></pre>
<p><img src="/post/2020-10-12-MSc1_Sitzung1_Regression_files/figure-html/unnamed-chunk-49-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Hier sind auch noch die weiteren Histogramme dieser Sitzung mit <code>ggplot</code> aufbereitet:</p>
<div id="hat-values" class="section level4">
<h4>Hat-Values</h4>
<pre class="r"><code>n &lt;- length(residuals(our_model))
h &lt;- hatvalues(our_model) # Hebelwerte
df_h &lt;- data.frame(h) # als Data.Frame für ggplot
ggplot(data = df_h, aes(x = h)) + 
     geom_histogram(aes(y =..density..),  bins = 15)+
  geom_vline(xintercept = 4/n, col = &quot;red&quot;) # Cut-off bei 4/n</code></pre>
<p><img src="/post/2020-10-12-MSc1_Sitzung1_Regression_files/figure-html/unnamed-chunk-50-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="cooks-distanz-1" class="section level4">
<h4>Cooks-Distanz:</h4>
<pre class="r"><code># Cooks Distanz
CD &lt;- cooks.distance(our_model) # Cooks Distanz
df_CD &lt;- data.frame(CD) # als Data.Frame für ggplot
ggplot(data = df_CD, aes(x = CD)) + 
     geom_histogram(aes(y =..density..),  bins = 15)+
  geom_vline(xintercept = 1, col = &quot;red&quot;) # Cut-Off bei 1</code></pre>
<p><img src="/post/2020-10-12-MSc1_Sitzung1_Regression_files/figure-html/unnamed-chunk-51-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="AppendixD" class="section level3">
<h3>Appendix D</h3>
<div id="multikollinearität-und-standardfehler" class="section level4">
<h4>Multikollinearität und Standardfehler</h4>
<p>Dies ist der Appendix A der Bachelor Sitzung zu Voraussetzungen der Regression von <a href="/authors/irmer">Julien Irmer</a>.</p>
<p>Im Folgenden stehen <span class="math inline">\(\beta\)</span>s für <em><strong>unstandardisierte</strong></em> Regressionskoeffizienten.</p>
<p>Für eine einfache Regressionsgleichung mit <span class="math display">\[Y_i=\beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \varepsilon_i\]</span>
kann die selbe Gleichung auch in Matrixnotation formuliert werden <span class="math display">\[Y = X\beta + \varepsilon.\]</span> Hier ist <span class="math inline">\(X\)</span> die Systemmatrix, welche die Zeilenvektoren <span class="math inline">\(X_i=(1, x_{i1}, x_{i2})\)</span> enthält. Die Standardfehler, welche die Streuung der Parameter <span class="math inline">\(\beta:=(\beta_0,\beta_1,\beta_2)\)</span> beschreiben, lassen sich wie folgt ermitteln. Wir bestimmen zunächst die Matrix <span class="math inline">\(I\)</span> wie folgt
<span class="math display">\[I:=(X&#39;X)^{-1}\hat{\sigma}^2_e,\]</span>
wobei <span class="math inline">\(\hat{\sigma}^2_e\)</span> die Residualvarianz unserer Regressionanalyse ist (also der nicht-erklärte Anteil an der Varianz von <span class="math inline">\(Y\)</span>). Aus der Matrix <span class="math inline">\(I\)</span> erhalten wir die Standardfehler sehr einfach: Sie stehen im Quadrat auf der Diagonalen. Das heißt, die Standardfehler sind <span class="math inline">\(SE(\beta)=\sqrt{\text{diag}(I)}\)</span> (Wir nehmen mit <span class="math inline">\(\text{diag}\)</span> die Diagonalelemente aus <span class="math inline">\(I\)</span> und ziehen aus diesen jeweils die Wurzel: der erste Eintrag ist der <span class="math inline">\(SE\)</span> von <span class="math inline">\(\beta_0\)</span>; also <span class="math inline">\(SE(\beta_0)=\sqrt{I_{11}}\)</span>; der zweite von <span class="math inline">\(\beta_1\)</span>;<span class="math inline">\(SE(\beta_1)=\sqrt{I_{22}}\)</span>; usw.). Was hat das nun mit der Kollinearität zu tun? Wir wissen, dass in <span class="math inline">\(X&#39;X\)</span> die Information über die Kovariation im Datensatz steckt (<em>dafür muss nur noch durch die Stichprobengröße geteilt werden und das Vektorprodukt der Mittelwerte abgezogen werden; damit wir eine Zentrierung um den Mittelwert sowie eine Normierung an der Stichprobengröße vorgenommen</em>). Beispielsweise lässt sich die empirische Kovarianzmatrix <span class="math inline">\(S\)</span> zweier Variablen <span class="math inline">\(z_1\)</span> und <span class="math inline">\(z_2\)</span> sehr einfach bestimmen mit <span class="math inline">\(Z:=(z_1, z_2)\)</span>:
<span class="math display">\[ S=\frac{1}{n}Z&#39;Z - \begin{pmatrix}\overline{z}_1\\\overline{z}_2 \end{pmatrix}\begin{pmatrix}\overline{z}_1&amp;\overline{z}_2 \end{pmatrix}.\]</span>
Weitere Informationen hierzu (Kovarianzmatrix und Standardfehler) sind im Appendix B (sowie auch in einigen Kapiteln von <a href="https://hds.hebis.de/ubffm/Record/HEB366849158">Eid et al. (2017)</a> Unterpunkt 5.2-5.4 bzw. ab Seite 1058) nachzulesen.</p>
<p>Insgesamt bedeutet dies, dass die Standardfehler von der Inversen der Kovarianzmatrix unserer Daten sowie von der Residualvarianz abhängen. Sie sind also groß, wenn die Residualvarianz groß ist (damit ist die Vorhersage von <span class="math inline">\(Y\)</span> schlecht) oder wenn die Inverse der Kovarianzmatrix groß ist (also wenn die Variablen stark redundant sind und somit hoch miteinander korrelieren). Nehmen wir dazu der Einfachheit halber an, dass <span class="math inline">\(\hat{\sigma}_e^2=1\)</span> (es geht hier nur um eine numerische Präsentation der Effekte, nicht um ein sinnvolles Modell) sowie <span class="math inline">\(n = 100\)</span> (Stichprobengröße). Zusätzlich gehen wir von zentrierten Variablen (Mittelwert von 0) aus. Dann lässt sich aus <span class="math inline">\(X&#39;X\)</span> durch Division durch <span class="math inline">\(100\)</span> die Kovarianzmatrix der Variablen bestimmen. Wir gucken uns drei Fälle an mit</p>
<p><span class="math display">\[\begin{align*}
\text{Fall 1: } X&#39;X&amp;=\begin{pmatrix}100&amp;0&amp;0\\0&amp;100&amp;0\\0&amp;0&amp;100\end{pmatrix},\\
\text{Fall 2: } X&#39;X&amp;=\begin{pmatrix}100&amp;0&amp;0\\0&amp;100&amp;99\\0&amp;99&amp;100\end{pmatrix} \quad  \text{und}\\ 
\text{Fall 3: } X&#39;X&amp;=\begin{pmatrix}100&amp;0&amp;0\\0&amp;100&amp;100\\0&amp;100&amp;100\end{pmatrix}
\end{align*}\]</span></p>
<p>Hierbei ist zu beachten, dass <span class="math inline">\(X\)</span> die Systemmatrix ist, welche auch die <span class="math inline">\(1\)</span> des Interzepts enthält. Natürlich ist eine Variable von einer Konstanten unabhängig, weswegen die erste Zeile und Spalte von <span class="math inline">\(X&#39;X\)</span> jeweils der Vektor <span class="math inline">\((100, 0, 0)\)</span> ist. Die zugehörigen Korrelationsmatrizen können durch Divison durch 100 berechnet werden (<em>da wir zentrierte Variablen haben, die Stichprobengröße gleich 100 ist und die Varianzen der Variablen gerade 100 sind!</em>). Wir betrachten nur die Minormatrizen, aus welchen die 1. Zeile und die 1. Spalte gestrichen wurden. Diese teilen wir durch 100 und erhalten die Korrelationsmatrix der Variablen:</p>
<p><span class="math display">\[\begin{align*}
\text{Fall 1: }\Sigma_1&amp;=\begin{pmatrix}1&amp;0\\0&amp;1\end{pmatrix}.\\
\text{Fall 2: }\Sigma_2&amp;=\begin{pmatrix}1&amp;.99\\.99&amp;1\end{pmatrix} \quad  \text{und}\\
\text{Fall 3: } \Sigma_3&amp;=\begin{pmatrix}1&amp;1\\1&amp;1\end{pmatrix}. 
\end{align*}\]</span></p>
<p>Im <em>Fall 1</em> sind die zwei Variablen unkorreliert. Die Inverse ist leicht zu bilden.</p>
<pre class="r"><code>XX_1 &lt;- matrix(c(100,0,0,
               0,100,0,
               0,0,100),3,3)
XX_1 # Die Matrix X&#39;X im Fall 1</code></pre>
<pre><code>##      [,1] [,2] [,3]
## [1,]  100    0    0
## [2,]    0  100    0
## [3,]    0    0  100</code></pre>
<pre class="r"><code>I_1 &lt;- solve(XX_1)*1 # I (*1 wegen Residualvarianz = 1)
I_1</code></pre>
<pre><code>##      [,1] [,2] [,3]
## [1,] 0.01 0.00 0.00
## [2,] 0.00 0.01 0.00
## [3,] 0.00 0.00 0.01</code></pre>
<pre class="r"><code>sqrt(diag(I_1)) # Wurzel aus den Diagonalelementen der Inverse = SE, wenn sigma_e^2=1</code></pre>
<pre><code>## [1] 0.1 0.1 0.1</code></pre>
<p>Die Standardfehler sind nicht sehr groß: alle liegen bei <span class="math inline">\(0.1\)</span>.</p>
<p>Im <em>Fall 2</em> sind die zwei Variablen fast perfekt (zu <span class="math inline">\(.99\)</span>) korreliert - es liegt hohe Multikollinearität vor. Die Inverse ist noch zu bilden. Die Standardfehler sind deutlich erhöht im Vergleich zu <em>Fall 1</em>.</p>
<pre class="r"><code>XX_2 &lt;- matrix(c(100,0,0,
               0,100,99,
               0,99,100),3,3)
XX_2 # Die Matrix X&#39;X im Fall 2</code></pre>
<pre><code>##      [,1] [,2] [,3]
## [1,]  100    0    0
## [2,]    0  100   99
## [3,]    0   99  100</code></pre>
<pre class="r"><code>I_2 &lt;- solve(XX_2)*1 # I (*1 wegen Residualvarianz = 1)
I_2</code></pre>
<pre><code>##      [,1]       [,2]       [,3]
## [1,] 0.01  0.0000000  0.0000000
## [2,] 0.00  0.5025126 -0.4974874
## [3,] 0.00 -0.4974874  0.5025126</code></pre>
<pre class="r"><code>sqrt(diag(I_2)) # SEs im Fall 2</code></pre>
<pre><code>## [1] 0.1000000 0.7088812 0.7088812</code></pre>
<pre class="r"><code>sqrt(diag(I_1)) # SEs im Fall 1</code></pre>
<pre><code>## [1] 0.1 0.1 0.1</code></pre>
<p>Die Standardfehler des <em>Fall 2</em> sind sehr groß im Vergleich zu <em>Fall 1</em> (mehr als sieben Mal so groß - was de facto schon gigantisch ist!); nur der Standardfehler des Interzepts bleibt gleich. Die Determinante von <span class="math inline">\(X&#39;X\)</span> in <em>Fall 2</em> liegt deutlich näher an <span class="math inline">\(0\)</span> im Vergleich zu <em>Fall 1</em>; hier: <span class="math inline">\(10^6\)</span>.</p>
<pre class="r"><code>det(XX_2) # Determinante Fall 2</code></pre>
<pre><code>## [1] 19900</code></pre>
<pre class="r"><code>det(XX_1) # Determinante Fall 1</code></pre>
<pre><code>## [1] 1e+06</code></pre>
<p>Im <em>Fall 3</em> sind die zwei Variablen perfekt korreliert - es liegt perfekte Multikollinearität vor. Die Inverse kann <strong>nicht</strong> gebildet werden (da <span class="math inline">\(\text{det}(X&#39;X) = 0\)</span>). Die Standardfehler können nicht berechnet werden. Eine Fehlermeldung wird ausgegeben.</p>
<pre class="r"><code>XX_3 &lt;- matrix(c(100,0,0,
               0,100,100,
               0,100,100),3,3)
XX_3 # Die Matrix X&#39;X im Fall 3</code></pre>
<pre><code>##      [,1] [,2] [,3]
## [1,]  100    0    0
## [2,]    0  100  100
## [3,]    0  100  100</code></pre>
<pre class="r"><code>det(XX_3) # Determinante on X&#39;X im Fall 3</code></pre>
<pre><code>## [1] 0</code></pre>
<pre class="r"><code>I_3 &lt;- solve(XX_3)*1 # I (*1 wegen Residualvarianz = 1)
I_3
sqrt(diag(I_3)) # Wurzel aus den Diagonalelementen der Inverse = SE, wenn sigma_e^2=1

# hier wird eine Fehlermeldung ausgegeben, wodurch der Code nicht ausführbar ist und I_3 nicht gebildet werden kann:

#    Error in solve.default(XX_3) : 
#    Lapack routine dgesv: system is exactly singular: U[2,2] = 0</code></pre>
<p>Der VIF bzw. die Toleranz quantifizieren die Korrelation zwischen den beiden Variablen. Der VIF wäre in diesen Analysen im <em>1. Fall</em> für beide Variablen 1, im <em>2. Fall</em> für beide Variabeln 50.25 und im <em>3. Fall</em> nicht berechenbar (bzw. <span class="math inline">\(\infty\)</span>). Entsprechend wäre die Toleranz im <em>1. Fall</em> 1 und 1, im <em>2. Fall</em> 0.02 und 0.02 sowie im <em>3. Fall</em> 0 und 0.</p>
<p>Dieser Exkurs zeigt, wie sich die Multikollinearität auf die Standardfehler und damit auf die Präzision der Parameterschätzung auswirkt. Inhaltlich bedeutet dies, dass die Prädiktoren redundant sind und nicht klar ist, welchem Prädiktor die Effekte zugeschrieben werden können.</p>
<p><em>Die Matrix <span class="math inline">\(I\)</span> ist im Zusammenhang mit der Maximum-Likelihood-Schätzung die Inverse der Fischer-Information und enthält die Informationen der Kovariationen der Parameterschätzer (diese Informationen enthält sie hier im Übrigen auch!).</em></p>
</div>
</div>
</div>
<div id="literatur" class="section level2">
<h2>Literatur</h2>
<p><a href="https://hds.hebis.de/ubffm/Record/HEB369761391">Agresti, A, &amp; Finlay, B. (2013).</a> <em>Statistical methods for the social sciences.</em> (Pearson new international edition, 4th edition). Harlow, Essex : Pearson Education Limited.</p>
<p><a href="https://hds.hebis.de/ubffm/Record/HEB366849158">Eid, M., Gollwitzer, M., &amp; Schmitt, M. (2017).</a> <em>Statistik und Forschungsmethoden</em> (5. Auflage, 1. Auflage: 2010). Weinheim: Beltz.</p>
<p><a href="https://hds.hebis.de/ubffm/Record/HEB371183324">Pituch, K. A. &amp; Stevens, J. P. (2016).</a> <em>Applied Multivariate Statistics for the Social Sciences</em> (6th ed.). New York: Taylor &amp; Francis.</p>
<ul>
<li><small> <em>Blau hinterlegte Autorenangaben führen Sie direkt zur universitätsinternen Ressource.</em></li>
</ul>
</div>
