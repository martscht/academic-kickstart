---
title: 'Grundkonzepte Bayesianischer Analysen'
date: '2023-04-21'
lastmod: '2023-04-26'
slug: bayes-intro
categories:
  - extras
tags:
  - Bayes
  - Verteilungen
subtitle: 'An einem fiktiven klinischen Beispiel mit viel zu kleinem $n$'
summary: ''
authors: [schultze]
featured: no
header: 
  image: "/header/bayes_intro.jpg"
  caption: "[Courtesy of pxhere](https://pxhere.com/en/photo/769748)"
projects: []
---



<p>In vielen Bereichen der Psychologie haben wir ein Problem. Also, eigentlich mehrere, aber eins beschäftigt uns außerordentlich häufig, auch während des Studiums: unsere Studien arbeiten oft mit sehr kleinen Stichproben. Insbesondere in klinischen Untersuchungen liegt das oft einfach daran, dass es sehr aufwendig ist Probanden zu erheben. Wenn wir psychotherapeutische Interventionen untersuchen bedeutet oft jedes einzelne zusätzliche <span class="math inline">\(n\)</span>, dass wir dutzende Stunden Arbeit aufwenden müssen. Auf der anderen Seite steht das Problem, dass wir bei jeder Verringerung des <span class="math inline">\(n\)</span> unsere Fähigkeit einschränken, aus unserer Stichprobe auch zulässige Rückschlüsse auf die Population ziehen zu können. In diesem Abschnitt wird es - wie der Titel hoffentlich klar gemacht hat - um eine Einführung in Bayes gehen. Wie die beiden Dinge zusammenhängen, sollte idealerweise nach ungefähr der Hälfte dieses Beitrags klar geworden sein. Danach gibt es ein paar technische Spielereien.</p>
<div id="ein-einfaches-beispiel" class="section level2">
<h2>Ein einfaches Beispiel</h2>
<p>Nehmen wir an, dass Sie in einer Suchtklinik arbeiten - oder vielleicht ein Praktikum machen. Das bisherige System, nach welchem Patient:innen Ausgang außerhalb des Klinikgeländes gewährt wird bezieht sich vor allem auf die Zeit, die die Person schon in der Klinik ist. Als Sie anfangen, finden Sie das System irgendwie suboptimal und Sie denken sich: “Das sollte doch eigentlich vom Therapiefortschritt abhängen…”. Obwohl Sie vielleicht Recht haben, ist auch für die Patient:innen Planbarkeit wichtig: ein Termin beim Bürgeramt, zum Beispiel, muss etliche Wochen vorab vereinbart werden. Also denken Sie sich etwas Neues aus, das alle super glücklich machen sollte. Sie können dieses System aber nicht an 42 Leuten testen (wie Ihre Poweranalyse Ihnen rät), sondern probieren es zunächst mit den zehn Patient:innen, die im Rahmen des Praktikums in Ihre Obhut übergeben wurden.</p>
<p>Ihr neues System führt dazu, dass 7 von 10 Patient:innen aus dem Ausgang zurück kommen ohne Drogen genommen zu haben. Angesichts der Tatsache, dass es bei den Kolleg:innen, die sich an das alte System halten immer knapp die Hälfte ist, verbuchen Sie das als Erfolg. Übertragen wir das Ganze mal in <code>R</code>:</p>
<pre class="r"><code># Beobachtungen
obs &lt;- c(0, 1, 1, 0, 1, 1, 1, 0, 1, 1)

# N
length(obs)</code></pre>
<pre><code>## [1] 10</code></pre>
<pre class="r"><code># Erfolgsquote
mean(obs)</code></pre>
<pre><code>## [1] 0.7</code></pre>
<p>Jede <code>1</code> stellt eine Person dar, die erfolgreich aus dem Ausgang zurückkam, ohne Drogen genommen zu haben. Jede <code>0</code> ein Scheitern, dass Sie an Ihrer Berufswahl zweifeln lässt.</p>
<p>Etwas formaler ausgedrückt: wir wollen jetzt prüfen, ob die Erfolgsquote Ihres Systems sich von der Quote Ihrer Kolleg:innen unterscheidet. Die Nullhypothese ist also, dass wir vermuten, dass auch Ihr System eine Erfolgsquote von 50% produziert: <span class="math inline">\(H_0: \pi = .5\)</span>. <span class="math inline">\(\pi\)</span> stellt hierbei die Wahrscheinlichkeit des “erfolgreichen” Ausgangs in der Population aller Personen dar, die jemals an diesem System teilnehmen könnten.</p>
</div>
<div id="frequentistische-ansätze" class="section level2">
<h2>Frequentistische Ansätze</h2>
<p>Gucken wir uns zunächst die Möglichkeiten an, zu prüfen, ob Ihr System besser ist als das Ihrer Kolleg:innen. Ein klassischer Ansatz (den Sie <a href="/post/gruppenvergleiche-unabhaengig/#fragestellung-c">hier in Fragestellung C</a> nachlesen können) ist der <span class="math inline">\(\chi^2\)</span>-Test. In unserem Fall haben wir zwar nicht vier sondern nur zwei Felder, aber das macht das Ganze einfach nur einfacher:</p>
<pre class="r"><code># Häufigkeitstabelle der Erfolge
tab &lt;- table(obs)

# Tabelle in den Chi2-Test
chisq.test(tab)</code></pre>
<pre><code>## 
##  Chi-squared test for given probabilities
## 
## data:  tab
## X-squared = 1.6, df = 1, p-value = 0.2059</code></pre>
<p>Was hier geprüft wird ist die gleichmäßige Besetzung der Zellen. Unter der Nullhypothese <span class="math inline">\(H_0 : \pi = .5\)</span> müssten wir also fünf Erfolge und fünf Misserfolge beobachten:</p>
<pre class="r"><code>chisq.test(tab)$expected</code></pre>
<pre><code>## 0 1 
## 5 5</code></pre>
<p>In diesem Fall erhalten wir ein nicht bedeutsames Ergebnis - wir behalten die Nullhypothese also bei.</p>
<p>Den Aufmerksamen unter Ihnen ist vielleicht aufgefallen, dass der <span class="math inline">\(\chi^2\)</span>-Test, den wir hier nutzen, mit ein paar Annahmen einhergeht. Das liegt daran, dass wir <a href="/post/gruppenvergleiche-unabhaengig/#Chi-Sq">die Diskrepanz zwischen Erwartung und Beobachtung zu einer Zahl verrechnen</a> und die erzeugte Zahl dann mit einer bekannten Verteilung (der <span class="math inline">\(\chi^2\)</span>-Verteilung) abgleichen, um festzustellen wie wahrscheinlich unser Ergebnis wäre, wenn die Nullhypothese wahr wäre. Die Übertragung funktioniert nur unter bestimmten Annahmen, vor allem aber funktioniert Sie <em>immer</em> besser, je größer <span class="math inline">\(n\)</span> ist. Das gilt nicht nur für den <span class="math inline">\(\chi^2\)</span>-Test, sondern für alle parametrischen Tests - also Tests, die sich darauf verlassen, eine Prüfgröße zu erstellen und diese mit einer bekannten Verteilung abzugleichen.</p>
<p>Diese Tatsache führt dazu, dass insbesondere in klinischen Studien häufig gefordert wird, stattdessen mit Tests zu arbeiten, die sich nicht auf asymptotische Eigenschaften verlassen - sogenannte non-parametrische Tests. Für unser Beispiel gibt es da zum Glück eine recht einfache Möglichkeit!</p>
<p>Wenn Ihre Statistik I Vorlesung noch nicht allzu lange her ist, erinnern Sie sich vielleicht, dass die Anzahl von Erfolgen <span class="math inline">\(x\)</span> aus <span class="math inline">\(n\)</span> unabhängigen Versuchen <a href="/post/verteilungen/#Binomial">binomialverteilt</a> ist. Das ist, im Gegensatz zu dem was ich gerade über die <span class="math inline">\(\chi^2\)</span>-Tests gesagt habe, keine Annahme, sondern einfach eine Realität der Welt in der wir leben. Wir können also mit einer Gleichung direkt bestimmen, wie wahrscheinlich es ist, dass sieben Ihrer zehn Patient:innen aus dem Ausgang zurückkommen ohne rückfällig geworden zu sein, wenn ihr Ausgangsprinzip genauso gut funktioniert, wie das Ihrer Kolleg:innen (<span class="math inline">\(H_0: \pi = .5\)</span>).</p>
<p><span class="math display">\[
  P(X = x | n, \pi) = {n \choose x} \cdot \pi^x \cdot (1 - \pi)^{n-x}
\]</span></p>
<p>Für diesen Fall also:</p>
<p><span class="math display">\[
  P(X = 7 | 10, .5) = {10 \choose 7} \cdot .5^7 \cdot (1 - .5)^{10-7} = .117
\]</span></p>
<pre class="r"><code># Wahrscheinlichkeit händisch bestimmen
choose(10, 7) * .5^7 * (1 - .5)^(10 - 7)</code></pre>
<pre><code>## [1] 0.1171875</code></pre>
<p>Uns interessiert aber nicht, wie wahrscheinlich es ist, dass Sie <em>genau</em> sieben Erfolge haben. In der Inferenzstatistik interessiert uns typischerweise, wie wahrscheinlich es ist dieses oder ein extremeres (im Fall der ungerichteten Nullhypothese) Ergebnis zu finden. Dafür können wir einfach die Funktion zur Binomialverteilung nutzen:</p>
<pre class="r"><code># Gerichtet
pbinom(6, 10, .5, lower.tail = FALSE)</code></pre>
<pre><code>## [1] 0.171875</code></pre>
<pre class="r"><code># Ungerichtet
pbinom(6, 10, .5, lower.tail = FALSE) + pbinom(3, 10, .5)</code></pre>
<pre><code>## [1] 0.34375</code></pre>
<p>Wir setzen hier 6 und nicht 7 in die Funktion ein, weil uns <code>pbinom</code> die <em>Überschreitungswahrscheinlichkeit</em> ausgibt. Wir brauchen also die Wahrscheinlichkeit dafür einen Wert von 6 zu überschreiten (weil wir die 7 ja einschließen wollen). Der Test, den wir gerade durchgeführt haben, nennt man <em>Binomialtest</em> und auch für diesen gibt es eine eigene Funktion in <code>R</code>, die dem gleichen Schema folgt, wie z.B. die <code>t.test</code>-Funktion:</p>
<pre class="r"><code>binom.test(7, 10, .5)</code></pre>
<pre><code>## 
##  Exact binomial test
## 
## data:  7 and 10
## number of successes = 7, number of trials = 10, p-value = 0.3438
## alternative hypothesis: true probability of success is not equal to 0.5
## 95 percent confidence interval:
##  0.3475471 0.9332605
## sample estimates:
## probability of success 
##                    0.7</code></pre>
<p>Auch hier also keine statistische Bedeutsamkeit.</p>
<details>
<summary>
Power in unseren frequentistischen Möglichkeiten
</summary>
<p>Wir haben jetzt also zwei Ansätze gesehen die einfache Frage zu prüfen, ob Ihr Ausgangssystem zu anderen Erfolgsquoten führt, als das Ihrer Kolleg:innen. Dabei hatten wir zunächst den klassischen, parametrischen Weg gewählt und einen <span class="math inline">\(\chi^2\)</span>-Test durchgeführt. Allerdings fällt es uns bei kleinen Stichproben häufig schwer uns auf die asymptotischen Eigenschaften von Tests zu verlassen. Bei anderen parametrischen Tests - wie z.B. <span class="math inline">\(t\)</span>-Tests oder ANOVAs - kann es bei kleinen Stichproben auch quasi unmöglich werden überhaupt zu prüfen, ob die Annahmen haltbar sind, die diese Verfahren voraussetzen.</p>
<p>Also sind wir auf einen non-parametrischen Test ausgewichen (den Binomialtest). Das Problem dabei ist, dass wir die Power unsere Inferenzstatistik sogar noch verringern. In parametrischen Tests “gewinnen” wir ein wenig Power dadurch, dass wir Annahmen machen. Wenn wir weniger Annahmen machen, brauchen wir mehr Daten, um zur gleichen Sicherheit zu kommen. Mehr zur Poweranalyse in frequentistischen Analysen finden Sie <a href="/post/simulation">hier</a>.</p>
</details>
</div>
<div id="neyman-pearson-nullhypothesentests" class="section level2">
<h2>Neyman-Pearson Nullhypothesentests</h2>
<p>In unseren frequentistischen Berechnungen haben wir unsere Analysen stets mit der Betrachtung des <span class="math inline">\(p\)</span>-Werts abgeschlossen und dabei den einfachen Nullhypothesentest zugrundegelegt. In beiden Fällen ist das Ergebnis (vermutlich wegen geringer Power) nicht statistisch bedeutsam. Die logische Frage, die sich an diesen - deprimierenden - Ausgang unserer Studie anschließt ist: “Was haben wir an Erkenntnis gewonnen?”.</p>
<p>Im klassischen Neyman-Pearson Nullhypothesentest haben wir an dieser Stelle keine belastbare Erkenntnis generiert. Wir behalten unsere Nullhypothese bei, weil wir keinen echten Anlass haben, von dieser Abstand zu nehmen. In der nächsten Untersuchung (die vielleicht die nächste Praktikantin an der gleiche Klinik durchführen wird) werden wir wieder mit der Nullhypothese anfangen, dass Ihr Plan genauso gut ist, wie der Ihrer Kolleg:innen (es wird also wieder <span class="math inline">\(H_0 : \pi = .5\)</span> getestet werden) und die Ergebnisse der hier untersuchten 10 Patient:innen werden in Vergessenheit geraten.</p>
<p>Um uns noch einmal kurz zu verdeutlichen, warum das ist, gucken wir uns noch einmal an, was ein <span class="math inline">\(p\)</span>-Wert eigentlich ist. Hervorragende Einleitungen in die Grundidee des klassischen, frequentistischen Testens gibt es viele, sodass wir uns hier nur auf die, für uns in diesem spezifischen Fall relevanten Komponenten beschränken.</p>
<p>Der <span class="math inline">\(p\)</span>-Wert, den wir erzeugt haben ist die Wahrscheinlichkeit dieses - oder ein extremeres - Ergebnis zu finden, wenn die Nullhypothese wahr wäre. Also, wenn die Grundrate mit der Leute nach Ihrem Schema rückfällig werden, tatsächlich <span class="math inline">\(\pi = .5\)</span> wäre, könnte es per Zufall passieren, dass sie nur 3 Rückfälle finden, wenn Sie nur 10 Personen untersuchen. Etwas formaler könnte man notieren <span class="math inline">\(p(X = x|H_0)\)</span>: die Wahrscheinlichkeit des empirischen Ergebnisses, gegeben die Nullhypothese. Wenn dieses Ergebnis unwahrscheinlich genug wäre (also <span class="math inline">\(p &lt; .05\)</span>), hätten wir abgeleitet, dass wir die Nullhypothese verwerfen und uns stattdessen für die Alternativhypothese entscheiden sollten. Das Problem dabei ist, dass wir nichts Neues wissen, wenn <span class="math inline">\(p &gt; .05\)</span>, weil dieses Ergebnis eine Aussage über die Daten, nicht über die Nullyhypothese ist: <span class="math inline">\(p(X = x|H_0) \neq p(H_0|X = x)\)</span>. Darüber hinaus ist <span class="math inline">\(P(X = x|H_0) \neq 1-P(x|H_1)\)</span> - das Ergebnis ist also auch keine Aussage über die Alternativhypothese.</p>
<details>
<summary>
Ein kurzer Einschub zur klassischen Wissenschaftstheorie
</summary>
<p>Das klassische Nullhypothesentesten ist konzeptuell sehr nah an Karl Poppers <em>naivem Falsifikationismus</em>. Dieses Konzept haben Sie im Verlauf des Studiums bestimmt schon etliche Male gehört: wir haben eine Theorie, leiten aus dieser eine Hypothese (hier die <span class="math inline">\(H_0\)</span>) ab und prüfen diese. Wenn wir die Hypothese verwerfen, verwerfen wir die generierende Theorie. Wenn wir die Hypothese nicht verwerfen (korroborieren), behalten wir die Theorie bei, generieren eine neue Hypothese und bleiben in dieser Schleife, bis wir die Theorie irgendwann verwerfen werden. Das Neyman-Pearson Nullhypothesentesten (mit <span class="math inline">\(H_0\)</span> <em>und</em> <span class="math inline">\(H_1\)</span>) ist konzeptuell eng mit Poppers <em>raffiniertem Falsifikationismus</em> verwandt, in dem Theorien unter Konkurrenzdruck stehen und wir uns beim Test einer Hypothese dann zwischen Theorien entscheiden können.</p>
<p>In den 60er und 70er Jahren wurde diese Ansicht des wissenschaftlichen Erkenntnisgewinns stark kritisiert. Vor allem Thomas Kuhn und Imre Lakatos (aber auch Popper selbst) integrierten in wissenschaftstheoretische Konzept, dass wissenschaftlicher Fortschritt durch schrittweise Anpassung von Theorien, das einpflegen neuer Erkenntnisse und die subjektiven Ansichten von Forscher:innen hinsichtlich der Nützlichkeit von Theorien geleitet wird. Nur in äußerst seltenen Fällen wird eine Theorie in Gänze über Bord geworfen, wenn eine aus ihr abgeleitete Hypothese falsifiziert wird. Viel häufiger kommt es vor, dass neue (empirische) Erkenntnisse genutzt werden, um unsere Theorien zu verfeinern. Leider wird dieses Vorgehen in unserer Art Hypothesen zu prüfen nicht direkt reflektiert. Stattdessen entscheiden wir bei jeder Studie erneut über die Ablehnung der Nullhypothese und schreiten nur dann mit unserem Wissen voran, wenn dies erfolgreich ist.</p>
<p>Wenn Sie eine etwas detaillierte Betrachtung des Nullhypothesentestens und bayesianischer Alternativen aus wissenschaftstheoretischer Sicht interessiert, ist das Buch von Dienes (2008) sehr empfehlenswert.</p>
</details>
</div>
<div id="likelihood" class="section level2">
<h2>Likelihood</h2>
<p>Leider bekommen wir von unseren Tests also eine Aussage über Daten, gegeben der Hypothesen und keine Aussagen über unsere Hypothesen. Naiv verstehen wir aber meistens die Daten als “fix” und unsere Hypothesen als das, was mit Unsicherheit behaftet ist. Also interessiert uns eher wie wahrscheinlich unsere Hypothese ist, gegeben der Daten - <span class="math inline">\(p(H|X = x)\)</span>.</p>
<p>Gucken wir uns noch einmal den Binomialtest an, den wir durchgeführt haben, um zu bestimmen wie wahrscheinlich es ist sieben “Erfolge” zu verbuchen, wenn wir zehn Patient:innen in den Ausgang schicken:</p>
<p><span class="math display">\[
  P(X = x | n, \pi) = {n \choose x} \cdot \pi^x \cdot (1 - \pi)^{n-x}
\]</span>
In <code>R</code> dann für unseren spezifischen Fall:</p>
<pre class="r"><code>choose(10, 7) * .5^7 * (1 - .5)^(10 - 7)</code></pre>
<pre><code>## [1] 0.1171875</code></pre>
<p>Wenn die Grundrate in der Population also <span class="math inline">\(\pi = .5\)</span> <em>wäre</em>, bestünde eine Wahrscheinlichkeit von 0.117, dass wir in unserem Fall sieben Erfolge verbuchen. Aber ist dieses <span class="math inline">\(\pi\)</span> eine gute Erklärung, bzw. eine gute Annahme, gegeben unserer Daten? Dafür könnten wir mit roher Gewalt einfach alle möglichen Werte von <span class="math inline">\(\pi\)</span> ausprobieren, um zu sehen, unter welcher Grundrate das Ergebnis am wahrscheinlichsten ist:</p>
<pre class="r"><code>pi &lt;- c(.5, .6, .7, .8, .9, 1)
L &lt;- choose(10, 7) * pi^7 * (1 - pi)^(10 - 7)
d &lt;- data.frame(pi, L)
d</code></pre>
<pre><code>##    pi          L
## 1 0.5 0.11718750
## 2 0.6 0.21499085
## 3 0.7 0.26682793
## 4 0.8 0.20132659
## 5 0.9 0.05739563
## 6 1.0 0.00000000</code></pre>
<p>Wir haben den Prozess also umgedreht: statt <span class="math inline">\(\pi\)</span> als gegeben anzunehmen, haben wir die Daten (<span class="math inline">\(n\)</span> und <span class="math inline">\(x\)</span>) festgesetzt und dann die Annahmen variiert. Ich habe das Ergebnis davon oben schon mit <code>L</code> bezeichnet, weil es statistisch als <em>Likelihood</em> bezeichnet wird:</p>
<p><span class="math display">\[
  L(\pi = .5 | n = 10, X = 7) = {10 \choose 7} \cdot .5^7 \cdot (1 - .5)^{10-7} = .117
\]</span>
Wenig überraschend ist, dass <span class="math inline">\(\pi = .7\)</span> in diesem fall die höchste Likelihood erreicht - schließlich ist .7 auch die von uns in diesem Fall beobachtete relative Häufigkeit. Wenn wir zwei Grundraten vergleichen
wollen, können wir einfach deren Verhältnis - die sogenannte <em>Likelihood Ratio</em> - bestimmen:</p>
<pre class="r"><code>L_H0 &lt;- dbinom(7, 10, .5)
L_H1 &lt;- dbinom(7, 10, .7)

L_H1/L_H0</code></pre>
<pre><code>## [1] 2.276932</code></pre>
<p>Eine Grundrate von .7 ist - gegeben der von uns beobachteten Daten - also 2.28-mal so <em>likely</em> wie eine Grundrate von .5. Der Begriff Likelihood Ratio ist Ihnen bestimmt schon einmal begegnet: genau das gleiche Prinzip nutzen wir, um verschiedene Modelle z.B. in <a href="/post/multi-level-modeling/#test-des-zufallseffekts">hierarchischer Regression</a> oder <a href="/post/cfa/#modellvergleiche">Faktorenanalyse</a> zu vergleichen. In unserem Fall hier sind die “Modelle” nur sehr simpel: in einem ist die Grundrate .5 in dem anderen ist sie .7.</p>
<p>In der <code>R</code>-Syntax benutze ich hier jetzt <code>dbinom</code> statt die komplette Binomialverteilung händisch aufzuschreiben - es passiert aber genaus das Gleiche. Wir können die <em>Dichteverteilung</em> sogar nutzen, um uns anzugucken, wie die Likelihood so über verschiedene Werte von <span class="math inline">\(\pi\)</span> aussieht:</p>
<pre class="r"><code>likeli_plot &lt;- ggplot(d, aes(x = pi, y = L)) + 
  xlim(0, 1) +
  geom_function(fun = dbinom, args = list(x = 7, size = 10)) +
  labs(x = expression(pi), y = &#39;Likelihood&#39;)

likeli_plot + geom_vline(xintercept = .5, lty = 2) +
  annotate(&#39;text&#39;, x = .48, y = .02, label = &#39;H[0]&#39;, parse = TRUE) +
  geom_vline(xintercept = .7, lty = 2) +
  annotate(&#39;text&#39;, x = .68, y = .02, label = &#39;H[1]&#39;, parse = TRUE)</code></pre>
<p><img src="/post/2023-04-21-bayes-intro_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
</div>
<div id="basic-bayes" class="section level2">
<h2>Basic Bayes</h2>
<p>Für einen Beitrag, der eigentlich “Grundideen Bayesianischer Analysen” betitelt ist, war das ganz schön viel Vorgeplänkel bis ich jetzt endlich bei Bayes angekommen bin. Aber all das war nötig, um ein Verständnis von dem zu bekommen, was jetzt passiert (vermute ich… mir ging es zumindest so).</p>
<p>Die Grundidee der Bayesianischen Analysen lässt sich eigentlich in folgender Gleichung zusammenfassen:</p>
<p><img src="/post/bayes-formel.png" style="width:50.0%" /></p>
<p>Was uns in diesem Ansatz interessiert - das Outcome von Forschung, wie sie hier verstanden wird - ist das Verhältnis der Wahrscheinlichkeiten von konkurrierenden Theorien (<span class="math inline">\(H_0\)</span> und <span class="math inline">\(H_1\)</span>) gegeben der Daten, die wir gesehen haben (<span class="math inline">\(X\)</span>). Diese Information wird <em>Posterior</em> genannt - weil es unseren Wissensstand <em>nach</em> der Studie darstellt.</p>
<p>Die Inferenzstatistik, die wir in den beiden klassischen Varianten oben betrieben haben betrachtet ausschließlich die Daten - wir nehmen an, dass wir keinerlei Vorwissen über unsere Hypothesen und deren Beziehung zu den Daten haben. Jedes Mal, wenn wir unseren Arbeitsalltag damit erschweren, dass wir irgendwie wieder zehn Personen erheben, um festzustellen, ob Ihr Ausgangsplan besser ist als die altmodischen Varianten, tun wir so, als ob es das allererste Mal wäre, dass wir auf diese Idee kommen. Und wir tun so, als hätten wir niemals zuvor Ergebnisse zu diesem Thema gesehen. Etwas vereinfacht ausgedrückt, basiert unsere Entscheidung ausschließlich auf der Likelihood-Ratio zwischen <span class="math inline">\(H_0\)</span> und <span class="math inline">\(H_1\)</span>.</p>
<p>Warum dann überhaupt so Forschung betreiben und die Likelihood-Ratio betrachten? Der immense Vorteil dieses Verhältnisses ist, dass es direkt aus den Daten gewonnen werden kann. Es ist also quasi objektiv bestimmbar. Um aber eine bedingte Wahrscheinlich drehen zu können brauchen wir - nach Satz von Bayes - die Grundwahrscheinlichkeit der Hypothesen. Wir brauchen also irgendeine Annahme darüber wie wahrscheinlich unsere Hypothesen wahr sind. Diese Vorannahme wird <em>Prior</em> genannt: die Informationen, die wir <em>vor</em> unserer Erhebung kennen (z.B. als unseren vorherigen, gescheiterten <span class="math inline">\(n=10\)</span> Studien).</p>
<p>Der Posterior ist dann die Mischung aus Prior (unseren Vorannahmen) und den, in dieser Studie neu gewonnenen Daten. Wir “updaten” unsere Annahmen bzw. Theorien also anhand der Daten (die in Form der Likelihood Ratio eingehen).</p>
<div id="uninformative-prior" class="section level3">
<h3>Uninformative Prior</h3>
<p>Wenn das Vorwissen, das wir über den Gegenstand unserer Untersuchung haben quasi Null ist, können wir Prior setzen, die keine Information enthalten. In solchen Fällen sollen alle Möglichkeiten (also in unserem Fall alle Werte der Grundrate <span class="math inline">\(\pi\)</span>) gleich wahrscheinlich sein - dass immer alle Personen rückfällig werden ist genauso wahrscheinlich, wie der totale Erfolg des Programms und alles dazwischen.</p>
<p><img src="/post/2023-04-21-bayes-intro_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>Sie fragen Sich vielleicht, warum Sie nur zwei Linien sehen, wenn wir doch drei Komponenten (Prior, Likelihood und Posterior) haben. Das liegt daran, dass unser Prior keinerlei Information beinhält - die jedes <span class="math inline">\(\pi\)</span> ist gleich wahrscheinlich, also gilt für das Verhältnis der Wahrscheinlichkeiten aller Kombinationen aus <span class="math inline">\(H_0\)</span> und <span class="math inline">\(H_1\)</span>: <span class="math inline">\(\frac{P(H_1)}{P(H_0)} = 1\)</span>, sodass dieser Teil in der Gleichung einfach irrelevant wird. So geht unser Posterior also ausschließlich auf unsere Daten zurück und entspricht genau der Likelihood-Verteilung.</p>
</div>
<div id="schwache-prior" class="section level3">
<h3>Schwache Prior</h3>
<p>Jetzt haben wir also einen sehr umständlichen Weg besprochen, exakt das Gleiche zu bekommen, wie vorher auch. Interessant wird das Ganze erst, wenn man über Prior Informationen in das System hineingibt, die Vorinformationen darstellen. Zum Beispiel könnten wir davon ausgehen, dass die Extreme unwahrscheinlicher Sind, als Werte in der Mitte - z.B. weil wir von unseren Kolleg:innen wissen, dass es keine Ausgangskonzepte gibt, die bewirken, dass niemand rückfällig wird oder alle rückfällig werden.</p>
<p><img src="/post/2023-04-21-bayes-intro_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>Wenn wir dermaßen schwache Annahmen in unsere Analysen einbauen, weicht die Posterior Verteilung nur leicht von unserer Likelihood Verteilung ab. Weil wir als Vorannahme hatten, dass <span class="math inline">\(\pi = .5\)</span> der wahrscheinlichste Wert ist, ist auch im Posterior nicht mehr die beobachtete relative Häufigkeit von .7 die ermittelte Grundrate in der Population.</p>
</div>
<div id="starke-prior" class="section level3">
<h3>Starke Prior</h3>
<p>Noch deutlicher wird dieser Effekt, wenn wir z.B. Informationen aus vorherigen Untersuchungen zum gleichen System einspeisen, dass Sie jetzt untersuchen wollen. Vielleicht stehen Sie im Kontakt zu anderen Kliniken, an denen schon mal ähnliche System erprobt wurden - immer wieder mit sehr kleinen Stichproben. All diese Informationen wollen Sie aber berücksichtigen, weil Ihre Studie mit <span class="math inline">\(n=10\)</span> nicht die einzige Quelle der Weisheit ist. Wenn es stichhaltige Informationen aus anderen Studien gibt, können wir diese als starke Prior einbauen:</p>
<p><img src="/post/2023-04-21-bayes-intro_files/figure-html/unnamed-chunk-15-1.png" width="672" />
In diesem Fall stellt der Posterior wirklich eine Mischung aus Ihren vorherigen Kenntnissen und den Informationen aus Ihrer eigenen Datenerhebung dar. Aus vorherigen Studien hatten Sie abgeleitet, dass die Erfolgsquote dieses Ausgangssystems (wie bei allen Anderen) bei ungefähr <span class="math inline">\(\pi = .5\)</span> liegen sollte. Nachdem Sie das Ganze mit zehn Personen ausprobiert haben und dabei festgestellt haben, dass sieben von denen nicht rückfällig geworden sind, erscheint es Ihnen eher naheliegend, dass der “wahre” Effekt vermutlich irgendwo dazwischen liegt.</p>
<p>In weiteren Studien - z.B. wenn Sie den Standort wechseln, oder einer Freundin in einer anderen Klinik empfehlen, das gleiche System mal auszuprobieren - können Sie diesen Posterior wiederum als Prior nutzen. So entsteht auch in der statistischen Auswertung <em>kumulativer</em> Erkenntnisgewinn.</p>
<p>Wie genau diese Posterior-Verteilungen zustandekommen werden wir uns im <a href="/post/bayes-conjugate">nächsten Bayes-Beitrag</a> noch genauer angucken. Das wird dann ein Beitrag mit ein paar mehr tatsächlien Umsetzungen in <code>R</code> … versprochen.</p>
</div>
</div>
<div id="inferenzstatistische-schlüsse-mit-bayes" class="section level2">
<h2>Inferenzstatistische Schlüsse mit Bayes</h2>
<p>Im letzten Abschnitt hat in allen drei Variante eins gefehlt: eine eindeutige Enstscheidung, ob Ihr Ausgangssystem jetzt besser ist, als das Ihrer Kolleg:innen. Was Ihnen in solchen Analysen eher sehr selten begegnen wird sind <span class="math inline">\(p\)</span>-Werte. Diese haben die spezifische Bedeutung, die wir oben schon besprochen haben und sind deswegen in Bayesianischer Analyse eher fehl am Platz. Stattdessen wird in diesen Analysen vor allem mit zwei Mitteln gearbeitet: dem Credible Interval und dem Bayes Factor.</p>
<div id="credible-interval" class="section level3">
<h3>Credible Interval</h3>
<p>In frequentistischer Statistik können wir Konfidenzintervalle um unsere Punktschätzer generieren, die von Parameter, Standardfehler und der angestrebten Sicherheit abhängen. Z.B. hatten wir oben im Binomialtest von <code>R</code> folgendes Konfidenzintervall ausgegeben bekommen:</p>
<pre class="r"><code>binom.test(7, 10, .5)</code></pre>
<pre><code>## 
##  Exact binomial test
## 
## data:  7 and 10
## number of successes = 7, number of trials = 10, p-value = 0.3438
## alternative hypothesis: true probability of success is not equal to 0.5
## 95 percent confidence interval:
##  0.3475471 0.9332605
## sample estimates:
## probability of success 
##                    0.7</code></pre>
<p>Wenn wir unsere Studie unendlich häufig unter diesen Bedingungen durchführen würden, würde dieses Intervall in 95% aller Fälle den “wahren Wert” Ihres Ausgangssystems enthalten. Wenn wir uns nochmal die Likelihoodverteilung vor Augen führen, können wir an dieser erkennen, dass wir dabei einfach die extremen 5% der Verteilung abtrennen und die mittleren 95% betrachten:</p>
<pre class="r"><code>ki &lt;- binom.test(7, 10, .5)$conf.int
likeli_plot + 
  geom_vline(xintercept = ki[1], lty = 3) + 
  geom_vline(xintercept = ki[2], lty = 3)</code></pre>
<p><img src="/post/2023-04-21-bayes-intro_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>Mit unserer Bayes-Analyse können wir etwas Ahnliches bestimmen, das <em>Credible Interval</em>. Dieses Intervall entspricht dem Intervall in das der unbeobachtete Wert in der Population mit z.B. 95%iger Wahrscheinlichkeit fällt. Dieses Intervall bestimmen wir naheliegenderweise nicht aus der Likelihood-Verteilung, sondern aus unserer Posterior-Verteilung. Für diese interessieren uns dann ebenfalls die mittleren 95%. Für den Fall mit uninformativen Priors, ist das Credible Interval numerisch identisch zum Konfidenzintervall, das wir anhand des Binomial-Tests erzeugt haben. Am Beispiel mit starken Priors sieht das Interval hingegen so aus:</p>
<p><img src="/post/2023-04-21-bayes-intro_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>Das 95%-Credible Interval ist in diesem Fall <span class="math inline">\([0.38; 0.78]\)</span>. Wie diese Werte genau entstehen, bzw. wie wir sie mit <code>R</code> ermitteln können besprechen wir im <a href="/post/bayes-conjugate">zweiten Bayes-Beitrag</a>, hier geht es erst einmal um das Prinzip: Credible Intervals sind die mittleren z.B. 95% der Posterior Verteilung.</p>
</div>
<div id="bayes-factor" class="section level3">
<h3>Bayes-Factor</h3>
<p>Der Bayes-Factor ist eine Kennzahl, die uns einen etwas direkteren Einblick in das Verhältnis zweier konkurrierender Theorien bzw. Hypothesen gibt. Dabei stellt</p>
<hr />
</div>
</div>
<div id="literatur" class="section level2">
<h2>Literatur</h2>
<p>Dienes, Z. (2008). Understanding psychology as a science: an introduction to scientific and statistical
inference. Palgrave Macmillan.</p>
</div>
