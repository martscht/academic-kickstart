---
title: Korrelation
author: 
date: '2021-01-04'
slug: korrelation
categories:
  - BSc2
tags:
  - Korrelation
subtitle: ''
summary: ''
authors: [winkler, schroeder]
lastmod: '2021-01-04T13:13:57+01:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

```{r setup, cache = FALSE, echo = FALSE}
knitr::opts_chunk$set(error = TRUE)
```


<details><summary>Kernfragen dieser Lehreinheit</summary>

* Wie können Kreuztabellen in R erstellt werden? Welche Varianten gibt es, relative Häufigkeitstabellen zu erstellen?
* Wie kann ein gemeinsames Balkendiagramm für zwei Variablen erstellt werden?
* Welche zwei Varianten gibt es, Varianzen und Kovarianzen zu bestimmen?
* Wie kann die Produkt-Moment-Korrelation, die Rang-Korrelation nach Spearman und Kendalls $\tau$ bestimmt werden?
* Wie wird bei der Berechnung von Korrelationen mit fehlenden Werten umgegangen?
</details>

***

## Datensatz laden

Zu Beginn laden wir wie gewohnt den Datensatz (Sie können den Datensatz [hier <i class="fas fa-download"></i> herunterladen](/post/fb20.rda)) und erstellen, falls nötig, Labels und Skalenwerte. 

```{r, echo = FALSE}
setwd('C:/Users/Nutzer/Documents/Lehre/Statistik')
```

```{r, eval = FALSE}
setwd(...) #auf Pfad mit Datensatz
```

```{r}
load("fb20.rda")

#Labels
fb20$fach <- factor(fb20$fach, levels = 1:5, labels = c ('Allgemeine', 'Biologische', 'Entwicklung', 'Klinische', 'Diag./Meth.'))
fb20$ziel <- factor(fb20$ziel, levels = 1:4, labels = c ('Wirtschaft', 'Therapie', 'Forschung', 'Andere'))

#Rekodierung
fb20$mdbf4_r <- -1 * (fb20$mdbf4 - 6)
fb20$mdbf11_r <- -1 * (fb20$mdbf11 - 6)
fb20$mdbf5_r <- -1 * (fb20$mdbf5 - 6)
fb20$mdbf7_r <- -1 * (fb20$mdbf7 - 6)
fb20$mdbf3_r <- -1 * (fb20$mdbf3 - 6)
fb20$mdbf9_r <- -1 * (fb20$mdbf5 - 9)

#Skalenwerte
gut <- fb20[,c('mdbf1','mdbf4_r','mdbf8','mdbf11_r')]
fb20$gs <- rowMeans(gut)
wach <- fb20[,c('mdbf2','mdbf5_r','mdbf7_r','mdbf10')]
fb20$wm <- rowMeans(wach)
ruhig <- fb20[,c('mdbf3_r','mdbf6','mdbf9_r','mdbf12')]
fb20$ru <- rowMeans(ruhig)

```

****

## Häufigkeitstabellen

Die Erstellung von *Häufigkeitstabellen* zur Darstellung univariater Häufigkeiten haben Sie schon kennengelernt. Dies funktioniert mit einfachen Befehlen für die Häufigkeiten und die zugehörigen relativen Prozentzahlen.


```{r}
tab<-table(fb20$fach)                 #Absolut
tab

prop.table(tab)                       #Relativ
```

Die Erweiterung für den bivariaten Fall ist dabei nicht schwierig und wird als *Kreuztabelle* bezeichnet. Sie liefert die Häufigkeit von Kombinationen von Ausprägungen in mehreren Variablen. In den Zeilen wird eine Variable abgetragen und in den Spalten die zweite. Im Unterschied zum univariaten Fall muss im `table()` Befehl nur die zweite interessierende Variable zusätzlich genannt werden. Tabellen können beliebig viele Dimensionen haben, werden dann aber sehr unübersichtlich.

```{r}
tab<-table(fb20$fach,fb20$ziel)       #Kreuztabelle
tab
```

In eine Kreuztabelle können Randsummen mit dem `addmargins()` Befehl hinzugefügt werden. Randsummen erzeugen in der letzten Spalte bzw. Zeile die univariaten Häufigkeitstabellen der Variablen.

```{r}
addmargins(tab)                       #Randsummen hinzufügen
```

Auch für die Kreuztabelle ist die Möglichkeit der Darstellung der Häufigkeiten in Relation zur Gesamtzahl der Beobachtungen gegeben. 

```{r}
prop.table(tab)                       #Relative Häufigkeiten
```

`r tab[4, 2]` von insgesamt `r sum(tab)` (`r round(tab[4,2]/sum(tab)*100,2)`%)  wollen therapeutisch arbeiten *und* interessieren sich bisher am meisten für die klinische Psychologie.


`prob.table()` kann allerdings nicht nur an der Gesamtzahl relativiert werden, sondern auch an der jeweiligen Zeilen- oder Spaltensumme. Dafür gibt man im Argument `margin` für Zeilen `1` oder für Spalten `2` an.

```{r}
prop.table(tab, margin = 1)           #relativiert an Zeilen
```

Von `r sum(tab[4, ])` Personen, die sich am meisten für klinische Psychologie interessieren, wollen `r round(tab[4,2]/sum(tab[4,])*100,2)`% (nämlich `r tab[4, 2]` Personen) später therapeutisch arbeiten.

```{r}
prop.table(tab, margin = 2)           #relativiert an Spalten
```

Von `r sum(tab[, 2])` Personen, die später therapeutisch arbeiten wollen, interessieren sich `r round(tab[4,2]/sum(tab[,2])*100,2)`% (nämlich `r tab[4, 2]` Personen) für die klinische Psychologie.


`addmargins()`und `prop.table()` können beliebig kombiniert werden.
`prop.table(addmargins(tab))` behandelt die Randsummen als eigene Kategorie (inhaltlich meist unsinnig!).
`addmargins(prop.table(tab))` liefert die Randsummen der relativen Häufigkeiten.

```{r}
addmargins(prop.table(tab))
```

****

## Balkendiagramme

Grafisch kann eine solche Kreuztabelle durch gruppierte Balkendiagramme dargestellt werden. Das Argument `beside` sorgt für die Anordnung der Balken (bei `TRUE` nebeneinander, bei `FALSE` übereinander). Das Argument `legend` nimmt einen Vektor für die Beschriftung entgegen. Die Reihen des Datensatzes bilden dabei stets eigene Balken, während die Spalten die Gruppierungsvariable bilden. Deshalb müssen als Legende die Namen der Reihen `rownames()` unserer Tabelle `tab` ausgewählt werden.

```{r}
barplot (tab,
         beside = TRUE,
         col = c('mintcream','olivedrab','peachpuff','steelblue','maroon'),
         legend = rownames(tab))
```

****

## Varianz, Kovarianz und Korrelation

In der Vorlesungen haben Sie gelernt, dass es für *Kovarianzen* und *Varianzen* empirische und geschätzte Werte gibt. R berechnet standardmäßig für die Varianz und Kovarianz die *Populationsschätzer*, verwendet also folgende Formeln für Varianz

$$\hat{\sigma}^2_{X} = \frac{\sum_{m=1}^n (y_m - \bar{y})^2}{n-1}$$

und Kovarianz.

$$\hat{\sigma}_{XY} = \frac{\sum_{m=1}^n (x_m - \bar{x}) \cdot (y_m - \bar{y})}{n-1}$$

Die Funktionen für die Varianz ist dabei `var()`. Im Folgenden wird diese für die Variablen `gs` (Gut vs. Schlecht) und `gewis` (Gewissenhaftigkeit) aus dem Datensatz bestimmt. Als Argumente müssen jeweils die Variablennamen verwendet werden.

```{r}
var(fb20$lz)                            #Varianz Lebenszufriedenheit
```

Wie bereits in vergangenen Sitzungen gesehen führen fehlende Werte zu der Ausgabe `NA`. Um dies zu beheben, wird im univariaten Fall `na.rm = TRUE` zum Ausschluss verwendet. 

```{r}
var(fb20$gs, na.rm = TRUE)               #Varianz Gut vs. Schlecht

var(fb20$gewis, na.rm = TRUE)            #Varianz Gewissenhaftigkeit
```


Die Funktion `cov()` wird für die Kovarianz verwendet und benötigt als Argumente die Variablen. 

```{r}
cov(fb20$gs, fb20$gewis)                #Kovarianz Gut vs. Schlecht und Gewissenhaftigkeit
```

Natürlich haben auch bei der Kovarianzberechnung fehlende Werte einen Einfluss. Zur Bewältigung des Problems gibt es das Argument `use`. Bei Zusammenhangsmaßen gibt es in R mehrere Möglichkeiten für den Umgang mit fehlenden Werten, die sich nur unterscheiden, wenn mehr als zwei Variablen korreliert werden:

* *Paarweiser Fallausschluss*: Personen, die auf (mindestens) einer von zwei Variablen `NA` haben, werden von der Berechnung ausgeschlossen.
* *Listenweiser Fallausschluss*: Personen, die auf (mindestens) einer von allen Variablen `NA` haben, werden von der Berechnung ausgeschlossen.
* *na.or.complete*: Zeilen, die einen fehlenden Wert (NA) enthalten, werden bei den Berechnungen ignoriert. Das entspricht der Angabe von na.rm = TRUE.

Am besten lässt sich der Unterschied in einer *Kovarianzmatrix* veranschaulichen. Hier werden alle Varianzen und Kovarianzen von einer Menge an Variablen berechnet und in einer Tabelle darstellt. Dafür muss ein Datensatz erstellt werden, der nur die interessierenden Variablen enthält. Zu unseren beiden Variablen nehmen wir als drittes noch die Lebenszufriedenheit (`lz`) auf.

```{r}
drei <- fb20[, c('gs','lz','gewis')]         #Datensatzreduktion
cov(drei)                                    #Kovarianzmatrix
```

Da die fehlenden Werte nicht entfernt wurden, gibt R kein Ergebnis aus.
Nun folgt die Gegenüberstellung der beiden betrachteten Möglichkeiten zum Ausschluss. 

```{r}
cov(drei, use = 'pairwise')             #Paarweiser Fallausschluss

cov(drei, use = 'complete')             #Listenweiser Fallausschluss
```

Wie wir sehen unterscheiden sich die Werte voneinander, da beim listenweisen Fallausschluss noch mehr Personen von Beginn an von der Berechnung ausgeschlossen werden.


Der Zusammenhang zwischen zwei Variablen kann in einem *Scatterplot* bzw. *Streupunktdiagramm* dargestellt werden. Dafür kann man die `plot()` Funktion nutzen. Als Argumente können dabei `x` für die Variable auf der x-Achse, `y` für die Variable auf der y-Achse, `xlim`, `ylim` für eventuelle Begrenzungen der Achsen und `pch` für die Punktart angegeben werden.

```{r}
plot(x = fb20$gs, y = fb20$gewis)
```

Wie in der Vorlesung besprochen sind für verschiedene Skalenniveaus Zusammenhangsmaße verfügbar, die im Gegensatz zur Kovarianz auch eine Vergleichbarkeit zwischen zwei Zusammenhangswerten sicherstellen. Für zwei metrisch skalierte Variablen gibt es dabei die *Produkt-Moment-Korrelation*. In der Funktion `cor()` werden dabei die Argumente `x` und `y` für die beiden betrachteten Variablen benötigt. `use` beschreibt weiterhin den Umgang mit fehlenden Werten.

```{r}
cor(x = fb20$gs, y = fb20$gewis, use = 'pairwise')
```

Bei einer positiven Korrelation gilt „je mehr Variable x... desto mehr Variable y" bzw. umgekehrt, bei einer negativen Korrelation „je mehr Variable x... desto weniger Variable y" bzw. umgekehrt. Korrelationen sind immer ungerichtet, das heißt, sie enthalten keine Information darüber, welche Variable eine andere bedingt - beide Variablen sind gleichberechtigt. Korrelationen und Regressionen liefern keine Hinweise auf Kausalitäten. Sie sagen beide etwas über den Zusammenhang zweier Variablen aus.

In R können wir uns auch eine *Korrelationsmatrix* ausgeben lassen. Dies geschieht äquivalent zu der Kovarianzmatrix mit dem Datensatz als Argument in der `cor()` Funktion. In der Diagonale stehen die Korrelationen der Variable mit sich selbst - also 1 - und in den restlichen Feldern die Korrelationen der Variablen untereinander.

```{r}
cor(drei, use = 'pairwise')
```


Die Stärke des korrelativer Zusammenhangs wird mit dem Korrelationskoeffizienten ausgedrückt, der zwischen -1 und +1 liegt. 
Die default Einstellung bei `cor()`ist die *Produkt-Moment-Korrelation*, also die Pearson-Korrelation.

```{r}
cor(fb20$extra, fb20$lz, use = "na.or.complete",method = c("pearson"))
```


Achtung! Die Pearson-Korrelation hat gewisse Voraussetzungen, die vor der Durchführung überprüft werden sollten!

**Voraussetzungen Pearson-Korrelation:**  

1. Skalenniveau: intervallskalierte Daten $\rightarrow$ ok (Ratingskalen werden meist als intervallskaliert aufgefasst)  
2. Linearität: Zusammenhang muss linear sein $\rightarrow$ Grafische Überprüfung (Scatterplot)  
3. Normalverteilung $\rightarrow$ QQ-Plot, Histogramm oder Shapiro-Wilk-Test  

**zu 3. Normalverteilung**

$\rightarrow$ QQ-Plot, Histogramm & Shapiro-Wilk-Test

```{r}
#QQ
qqnorm(fb20$extra)
qqline(fb20$extra)

qqnorm(fb20$lz)
qqline(fb20$lz)

#Histogramm
par(mfrow = c(2,1))   #damit beide Histogrammme in einer Grafik angezeigt werden, siehe Sitzung 12)
hist(fb20$extra, prob = T, ylim = c(0, 1))
curve(dnorm(x, mean = mean(fb20$extra, na.rm = T), sd = sd(fb20$extra, na.rm = T)), col = "blue", add = T)  
hist(fb20$lz, prob = T, ylim = c(0,1))
curve(dnorm(x, mean = mean(fb20$lz, na.rm = T), sd = sd(fb20$lz, na.rm = T)), col = "blue", add = T)
par(mfrow = c(1,1))

#Shapiro
shapiro.test(fb20$extra)
shapiro.test(fb20$lz)
```

$p < \alpha$ $\rightarrow$ H1: Normalverteilung kann nicht angenommen werden. Somit ist diese Voraussetzung verletzt. Daher muss die Rangkorrelation nach Spearman genutzt werden. Dies geschieht über `method = "spearman"`.


**Rangkorrelation in R**

```{r}
r1 <- cor(fb20$extra,fb20$lz,
          method = "spearman",     #Pearson ist default
          use = "complete.obs") 

r1

r1^2                               #R^2, Anteil an erklärter Varianz
```


**Interpretation des deskriptiven Zusammenhangs:**  
Es handelt sich um eine positive Korrelation von _r_ = .23. Der Effekt ist nach Cohens (1988) Konvention als schwach zu bewerten. Je höher die Ausprägung in Extraversion, desto höher ist die Ausprägung in der Lebenszufriedenheit. 

**Exkurs: Cohens (1988) Konvention zur Interpretation von $|r|$:**

* ~ .10: schwacher Effekt  
* ~ .30: mittlerer Effekt  
* ~ .50: starker Effekt 

Als letzte Variante gibt es noch Kendalls $\tau$ als Möglichkeit der "method" bei Korrelationen. Diese kann man mit ``kendall`` ansprechen.

```{r}

cor(fb20$extra, fb20$lz, use = 'pairwise', method = 'kendall')
```

**Signifikanztestung des Korrelationskoeffizienten:**
Nachedem der Korrelationskoeffizient berechnet wurde, muss dieser noch auf Signifikanz geprüft werden.

* H0: $\rho = 0$ $\rightarrow$ es gibt keinen Zusammenhang zwischen Extraversion und Lebenszufriedenheit
* H1: $\rho \neq 0$ $\rightarrow$  es gibt einen Zusammenhang zwischen Extraversion und Lebenszufriedenheit

```{r}
cor.test(fb20$extra, fb20$lz, 
         alternative = "two.sided", 
         method = "spearman",       #Da Voraussetzungen für Pearson verletzt
         use = "complete.obs")
```

$p < \alpha$ $\rightarrow$ H1. Die Korrelation ist mit einer Irrtumswahrscheinlichkeit von 5% signifikant von 0 verschieden. --> Bei der Rangkorrelation kann der exakte p-Wert jedoch nicht berechnet werden. Wenn die Voraussetzungen für die Pearson-Korrelation erfüllt wären, würde das Ganze so aussehen:

```{r}
cor.test(fb20$extra, fb20$lz, 
         alternative = "two.sided", 
         method = "pearson",       
         use = "complete.obs")
```

**Ergebnisinterpretation:**
Es wurde untersucht, ob Extraversion und Lebenszufriedenheit miteinander zusammenhängen. Der Pearson-Korrelationskoeffizient beträgt .22 und ist statistisch signifikant (_t_ = 2.1388, _p_ = .04). Folglich wird die Nullhypothese verworfen: Extraversion und Lebenszufriedenheit weisen einen signifikanten Zusammenhang auf.


## Wie können Zusammenhangsmaße für ordinalskalierte Daten berechnet werden?


Hierfür müssen wir das Paket `rococo` installieren, das verschiedene Konkordanz-basierte Zusammenhangsmaße enthält. Die Installation muss dem Laden des Paketes logischerweise vorausgestellt sein. Wenn R einmal geschlossen wird, müssen alle Zusatzpakete neu geladen, jedoch nicht neu installiert werden.

```{r, eval = FALSE}
install.packages('rococo')          #installieren
```

```{r}
library(rococo)                     #laden
```

Wir erhalten hier als Message den Hinweis, unter welcher Version das Paket erstellt wurde.
Übersichte über Pakete kann man mit `??`erhalten.

```{r, eval = FALSE}
??rococo
```

Dank dem neuen Paket können wir nun den Koeffizienten $\hat{\gamma}$ zum Zusammenhang zwischen Entspannung (`mdbf12`) und Gut Fühlen (`mdbf8`) berechnen. Die beiden Variablen wurden ursprünglich auf einer Skala von 1 (*überhaupt nicht*) bis 5 (*sehr*) (also auf Ordinalskalenniveau) erfasst. 

```{r}
rococo(fb20$mdbf12, fb20$mdbf8)
```

Die Funktion heißt hier zufälligerweise genau gleich wie das Paket. Wenn man nur Informationen über die Funktion statt das Paket sucht, geht das anhand von `?`.

```{r, eval = FALSE}
?rococo
```

****
